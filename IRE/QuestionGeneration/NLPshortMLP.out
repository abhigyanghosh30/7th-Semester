What can use the same technique to generate bigrams by first generating a random bigram that starts with <s>?	We can use the same technique to generate bigrams by first generating a random bigram that starts with <s> (according to its bigram probability).	such AAC devices	4.213375631547105
Who did include <s> and </s> in your counts just like any Write a program to compute?	Include <s> and </s> in your counts just like any Write a program to compute unsmoothed unigrams and bigrams.	unsmoothed unigrams and bigrams	4.149544016238435
What can such AAC devices use to generate bigrams by first generating a random bigram that starts with <s>?	We can use the same technique to generate bigrams by first generating a random bigram that starts with <s> (according to its bigram probability).	the same technique	4.133688422376218
Who did include <s> and </s> in your counts just like to compute unsmoothed unigrams and bigrams?	Include <s> and </s> in your counts just like any Write a program to compute unsmoothed unigrams and bigrams.	just like any Write a program	4.101964380219369
What 'll first need to augment each sentence with a special symbol <s> at the beginning of the sentence?	We 'll first need to augment each sentence with a special symbol <s> at the beginning of the sentence, to give us the bigram context of the first word.	such AAC devices	3.9247308821616382
What can such AAC devices use the same technique to generate by first generating a random bigram that starts with <s>?	We can use the same technique to generate bigrams by first generating a random bigram that starts with <s> (according to its bigram probability).	bigrams	3.895561804568821
What did include <s> and </s> in just like any Write a program to compute unsmoothed unigrams and bigrams?	Include <s> and </s> in your counts just like any Write a program to compute unsmoothed unigrams and bigrams.	in your counts	3.829367095918589
What 'll such AAC devices first need to augment with a special symbol <s> at the beginning of the sentence?	We 'll first need to augment each sentence with a special symbol <s> at the beginning of the sentence, to give us the bigram context of the first word.	each sentence	3.6768324218766315
Do include <s> and </s> in average's counts just like any other token?	Include <s> and </s> in your counts just like any other token.		3.6223980624138976
When </s> in your counts just like any Write a program to compute unsmoothed unigrams and bigrams?	Include <s> and </s> in your counts just like any Write a program to compute unsmoothed unigrams and bigrams.	include <s> and	3.590923931402063
What did Noam Chomsky argue that ``finite-state Markov processes'' were incapable of being a complete cognitive model of human grammatical knowledge in?	In a series of extremely influential papers starting with Chomsky and including Chomsky and Miller and Chomsky, Noam Chomsky argued that ``finite-state Markov processes'', while a possibly useful engineering heuristic, were incapable of being a complete cognitive model of human grammatical knowledge.	in a series of extremely influential papers starting with Chomsky	3.4627355127988153
Who Corpus-Based Research into Language in Oostdijk?	In Oostdijk, N. and de Haan, P. , Corpus-Based Research into Language, 189198.	N. and de Haan	3.291468493343918
What can we use to generate bigrams by first generating a random bigram that starts with <s>?	We can use the same technique to generate bigrams by first generating a random bigram that starts with <s> (according to its bigram probability).	the same technique	3.2667270196070985
What does Equation 3. 4 suggest?	Equation 3. 4 suggests that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities.	that such AAC devices could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities	3.258082216713535
How many letters as V or C did Markov classify?	Markov classified 20,000 letters as V or C and computed the bigram and trigram probability that a given letter would be a vowel given the previous one or two letters.	20,000 letters as V or C	3.1879614289901075
What is to build approximate language models using techniques like Bloom filters (Talbot and Osborne 2007?	Another option is to build approximate language models using techniques like Bloom filters (Talbot and Osborne 2007, Church et al.	another option	3.1871052781677736
Who can use the same technique to generate bigrams by first generating a random bigram that starts with <s>?	We can use the same technique to generate bigrams by first generating a random bigram that starts with <s> (according to its bigram probability).	we	3.168188979075868
Can such AAC devices use the same technique to generate bigrams by first generating a random bigram that starts with <s>?	We can use the same technique to generate bigrams by first generating a random bigram that starts with <s> (according to its bigram probability).		3.1609154231935648
What can we use the same technique to generate by first generating a random bigram that starts with <s>?	We can use the same technique to generate bigrams by first generating a random bigram that starts with <s> (according to its bigram probability).	bigrams	3.150096766418434
What 'll use data from the now-defunct Berkeley Restaurant Project?	We 'll use data from the now-defunct Berkeley Restaurant Project, a dialogue system from the last century that answered questions about a database of restaurants in Berkeley, California (Jurafsky et al. , 1994).	such AAC devices	3.138339237883391
What 'll such AAC devices see to?	We 'll see how to use n-gram models to estimate the probability of the last word of an n-gram given the previous words, and also to assign probabilities to entire sequences.	how to use n-gram models to estimate the probability of the last word of an n-gram given the previous words, and also to assign probabilities to entire sequences	3.1347859092711636
How many different discounts d1, d2, and d3 does modified Kneser-Ney use?	Rather than use a single fixed discount d, modified Kneser-Ney uses three different discounts d1, d2, and d3 + for n-grams with counts of 1,2 and three or more, respectively.	three different discounts d1, d2, and d3	3.1335095043955064
How many times in a corpus of a million words like the Brown corpus does Chinese occur for example?	For example, suppose the word Chinese occurs 400 times in a corpus of a million words like the Brown corpus.	400 times in a corpus of a million words like the Brown corpus	3.1173458437892227
What was used in the IBM TANGORA speech recognition system in the 1970s?	A trigram model was used in the IBM TANGORA speech recognition system in the 1970s, but the idea was not written up until later.	a trigram model	3.0981815454710158
Did include <s> and </s> in your counts just like any Write a program to compute unsmoothed unigrams and bigrams?	Include <s> and </s> in your counts just like any Write a program to compute unsmoothed unigrams and bigrams.		3.0527903551430304
Who R. D. in Luce?	In Luce, R. D. R. , and Galanter, E. (Eds.	R. , and Galanter	3.036817047253957
What is one that assigns a higher probability to the A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY test data?	A better n-gram model is one that assigns a higher probability to the A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY test data, and perplexity is a normalized version of the probability of the test set.	a better n-gram model	3.0357151829635405
Who R. , and Galanter in Luce?	In Luce, R. D. R. , and Galanter, E. (Eds.	R. D.	3.029215194572451
What showed the advantages of Modified Interpolated KneserNey?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.	Chen	3.027638338545021
Who Nauk (Bulletin de l'Academie Imperiale des Sciences de Mikolov?	Izvistia Imperatorskoi Akademii Nauk (Bulletin de l'Academie Imperiale des Sciences de Mikolov, T. .	Izvistia Imperatorskoi Akademii	3.0143770934974796
Who classified 20,000 letters as V or C?	Markov classified 20,000 letters as V or C and computed the bigram and trigram probability that a given letter would be a vowel given the previous one or two letters.	Markov	3.0141145552167554
What 'll such AAC devices first need to augment each sentence with?	We 'll first need to augment each sentence with a special symbol <s> at the beginning of the sentence, to give us the bigram context of the first word.	with a special symbol <s> at the beginning of the sentence	2.9861526302447197
How many times in the training set, and each of the other digits occurred 1 time each do Let's say that 0 occur?	Let's say that 0 occur 91 times in the training set, and each of the other digits occurred 1 time each.	91 times in the training set, and each of the other digits occurred 1 time each	2.9801738026748907
Where did L. N. workshop in Practice in Gelsema, E. S. and Kanal?	In Gelsema, E. S. and Kanal, L. N. , Proceedings, Workshop on Pattern Recognition in Practice, 381397.	on Pattern Recognition	2.9615810145867707
What did the resulting parameter set maximizes the likelihood of the training set T given the model M in?	In MLE, the resulting parameter set maximizes the likelihood of the training set T given the model M (i. e. , P (T M) ).	in MLE	2.947602825585953
What did Noam Chomsky argue in a series of extremely influential papers starting with Chomsky?	In a series of extremely influential papers starting with Chomsky and including Chomsky and Miller and Chomsky, Noam Chomsky argued that ``finite-state Markov processes'', while a possibly useful engineering heuristic, were incapable of being a complete cognitive model of human grammatical knowledge.	that ``finite-state Markov processes'' were incapable of being a complete cognitive model of human grammatical knowledge	2.9407605456740242
Who has the now-defunct Berkeley Restaurant Project's roots in a method called absolute discounting?	Kneser-Ney has its roots in a method called absolute discounting.	kneser-Ney	2.9406919150440496
What shows the add-one smoothed probabilities for the bigrams in Fig?	Figure 3. 6 shows the add-one smoothed probabilities for the bigrams in Fig.	figure 3. 6	2.9303715734537223
What set maximizes the likelihood of the training set T given the model M in MLE?	In MLE, the resulting parameter set maximizes the likelihood of the training set T given the model M (i. e. , P (T M) ).	the resulting parameter	2.9244612908780767
What 'll we first need to augment with a special symbol <s> at the beginning of the sentence?	We 'll first need to augment each sentence with a special symbol <s> at the beginning of the sentence, to give us the bigram context of the first word.	each sentence	2.9211160631308632
Who did Izvistia Imperatorskoi Akademii nauk?	Izvistia Imperatorskoi Akademii Nauk (Bulletin de l'Academie Imperiale des Sciences de Mikolov, T. .	(Bulletin de l'Academie Imperiale des Sciences de Mikolov	2.915467311735194
Who did the resulting parameter set the likelihood of the training set T given the model M in MLE?	In MLE, the resulting parameter set maximizes the likelihood of the training set T given the model M (i. e. , P (T M) ).	maximizes	2.8996693078937956
What 'll introduce both feedforward language models (Bengio et al?	We 'll introduce both feedforward language models (Bengio et al.	Chen	2.8910207375919224
What is the geometric mean of the inverse test set probability computed by the model?	The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.	the perplexity of a test set according to a language model	2.8907322873693695
Who 'll first need to augment each sentence with a special symbol <s> at the beginning of the sentence?	We 'll first need to augment each sentence with a special symbol <s> at the beginning of the sentence, to give us the bigram context of the first word.	we	2.881907548178472
What does Equation 3. 4 suggest that such AAC devices could estimate by multiplying together a number of conditional probabilities?	Equation 3. 4 suggests that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities.	the joint probability of an entire sequence of words	2.880196875498889
Where are problems with the addone algorithm summarized?	Problems with the addone algorithm are summarized in Gale and Church.	in Gale and Church	2.878906044191789
What does Chinese occur for example?	For example, suppose the word Chinese occurs 400 times in a corpus of a million words like the Brown corpus.	400 times in a corpus of a million words like the Brown corpus	2.8779045875663636
What has Modified Interpolated KneserNey since become for language modeling?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.	the standard baseline	2.875105230273404
Who used what are now called Markov chains to predict whether an upcoming letter in Pushkin's Eugene Onegin would be a vowel or a consonant?	The underlying mathematics of the n-gram was first proposed by Markov, who used what are now called Markov chains (bigrams and trigrams) to predict whether an upcoming letter in Pushkin's Eugene Onegin would be a vowel or a consonant.	Markov	2.8702503624651907
Who Workshop on Pattern Recognition in Practice in Gelsema, E. S. and Kanal?	In Gelsema, E. S. and Kanal, L. N. , Proceedings, Workshop on Pattern Recognition in Practice, 381397.	L. N.	2.8691565557625975
What led many linguists and computational linguists to ignore work in statistical modeling for decades?	These arguments led many linguists and computational linguists to ignore work in statistical modeling for decades.	these arguments	2.8674023274617215
What has Modified Interpolated KneserNey since become the standard baseline for?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.	for language modeling	2.861903204640235
What toolkits like KenLM (Heafield 2011?	Finally, efficient language model toolkits like KenLM (Heafield 2011, Heafield et al.	efficient language model	2.8418558381604666
What is the smoothed bigram probabilities in Fig?	The result is the smoothed bigram probabilities in Fig.	the result	2.8403220474515334
When is the percentage of OOV words that appear in the test set called?	The percentage of OOV words that appear in the test set is called the OOV rate.	the OOV rate	2.837149913057014
What did the resulting parameter set maximizes set T given the model M in MLE?	In MLE, the resulting parameter set maximizes the likelihood of the training set T given the model M (i. e. , P (T M) ).	the likelihood of the training	2.8232403833032516
What would an upcoming letter in Pushkin's Eugene Onegin be?	The underlying mathematics of the n-gram was first proposed by Markov, who used what are now called Markov chains (bigrams and trigrams) to predict whether an upcoming letter in Pushkin's Eugene Onegin would be a vowel or a consonant.	a vowel or a consonant	2.81967804853961
What could Chen encode the most likely horse with for example?	For example, we could encode the most likely horse with the code 0, and the remaining horses as 10, then 110,1110,111100,111101,111110, and 111111.	with the code 0, and the remaining horses as 10, then 110,1110,111100,111101,111110, and 111111	2.8159384782566925
What are Markov models that estimate words from a fixed window of previous words?	n-grams are Markov models that estimate words from a fixed window of previous words.	n-grams	2.8153149571075016
What are recommended for any reader with further Two commonly used toolkits for building language models are SRILM and KenLM (Heafield 2011?	These papers are recommended for any reader with further Two commonly used toolkits for building language models are SRILM (Stolcke, 2002) and KenLM (Heafield 2011, Heafield et al.	these papers	2.809560831515986
What did originally distribute as in Jelinek, F. and Mercer?	Originally distributed as IBM technical report in Jelinek, F. and Mercer, R. .	as IBM technical report	2.809457508313027
What did the resulting parameter set maximizes the likelihood of the training set in MLE?	In MLE, the resulting parameter set maximizes the likelihood of the training set T given the model M (i. e. , P (T M) ).	T given the model M	2.808206389114852
What 'll introduce more sophisticated language models like the RNN LMs of Chapter 9 in later chapters?	In later chapters we 'll introduce more sophisticated language models like the RNN LMs of Chapter 9.	such AAC devices	2.807399175322102
Do include <s> and </s> in your counts just like any other token?	Include <s> and </s> in your counts just like any other token.		2.8073704271173066
What is one intuitive way to think about entropy as a lower bound on the number of bits it would take to encode?	One intuitive way to think about entropy is as a lower bound on the number of bits it would take to encode a certain decision or piece of information in the optimal Consider an example from the standard information theory textbook Cover and Thomas.	a certain decision or piece of information in the optimal Consider an example from the standard information theory textbook Cover and Thomas	2.8010676779154715
What did Chen and Goodman produce a highly influential series of papers with?	Starting in the late 1990s, Chen and Goodman produced a highly influential series of papers with a comparison of different language models (Chen and Goodman 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).	with a comparison of different language models	2.79847939347378
What are also important for augmentative and alternative communication systems (Trnka et al?	Probabilities are also important for augmentative and alternative communication systems (Trnka et al.	probabilities	2.7941224753582103
What came from Jelinek and colleagues at the IBM Thomas J. Watson Research Center at CMU?	The resurgence of n-gram models came from Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and Baker at CMU, who was influenced by the work of Baum and colleagues.	the resurgence of n-gram models	2.791708518340437
Who derives from Laplace's 1812 law of succession?	Add-one smoothing derives from Laplace's 1812 law of succession and was first applied as an engineering solution to the zero-frequency problem by Jeffreys based on an earlier Add-K suggestion by Johnson.	add-one smoothing	2.791161264583621
Who am Sam corpus Calculate the probability of the sentence i want chinese food?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.	devset	2.7892322851870266
Who applied n-grams to compute approximations to English word sequences?	Shannon applied n-grams to compute approximations to English word sequences.	Shannon	2.7781236043963804
What are beginning to look a lot like Shakespeare?	The trigram and 4-gram sentences are beginning to look a lot like Shakespeare.	the trigram and 4-gram sentences	2.7669710934398077
What is the equation for interpolated absolute discounting applied to bigrams: PAbsoluteDiscounting = P The first term?	The equation for interpolated absolute discounting applied to bigrams: PAbsoluteDiscounting (wi wi1) = P The first term is the discounted bigram, and the second term is the unigram with an interpolation weight.	the discounted bigram	2.7661191445626065
What did N. and de Haan base into Language in Oostdijk?	In Oostdijk, N. and de Haan, P. , Corpus-Based Research into Language, 189198.	Research	2.7598139322849695
What does PCONTINUATION answer ``How likely is w to appear as a novel continuation?''?	, we 'd like to create a unigram model that we might call PCONTINUATION, which answers the question ``How likely is w to appear as a novel continuation?''	the question	2.7597126650761292
What did N. and de Haan base Research into in Oostdijk?	In Oostdijk, N. and de Haan, P. , Corpus-Based Research into Language, 189198.	into Language	2.7547919378943253
What does Chinese occur 400 times in a corpus of a million words like the Brown corpus for?	For example, suppose the word Chinese occurs 400 times in a corpus of a million words like the Brown corpus.	for example	2.746154325845975
Who R. D. R. , and Galanter in?	In Luce, R. D. R. , and Galanter, E. (Eds.	in Luce	2.742717982310488
What does a DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allow to use some m?	It A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allows us to use some m, which is a model of p (i. e. , an approximation to p).	Chen	2.741165988534151
What was influenced by the work of Baum and colleagues?	The resurgence of n-gram models came from Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and Baker at CMU, who was influenced by the work of Baum and colleagues.	CMU	2.7387748192961547
Who Readings in Speech Recognition?	and Lee, K. - F. , Readings in Speech Recognition, 450506.	Lee	2.7288010232680517
When were Markov models commonly used in engineering?	Based on Shannon's work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s.	by the 1950s	2.7285638891445054
What answers the question ``How likely is w to appear as a novel continuation?''?	, we 'd like to create a unigram model that we might call PCONTINUATION, which answers the question ``How likely is w to appear as a novel continuation?''	PCONTINUATION	2.7278897853336153
What is the inverse probability of the test set?	The perplexity (sometimes called PP for short) of a language model on a test set is the inverse probability of the test set, normalized by the number of words.	the perplexity of a language model on a test set	2.7226817218491863
What performed a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters?	They performed a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters.	Chen	2.721631360157131
What is the perplexity of a test set according to a language model?	The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.	the geometric mean of the inverse test set probability computed by the model	2.718085281055751
What did N. and de Haan base Research into Language in?	In Oostdijk, N. and de Haan, P. , Corpus-Based Research into Language, 189198.	in Oostdijk	2.717060513547236
What 'll such AAC devices introduce more sophisticated language models like the RNN LMs of Chapter 9 in?	In later chapters we 'll introduce more sophisticated language models like the RNN LMs of Chapter 9.	in later chapters	2.704094986180925
What mixes a discounted probability with a lower-order continuation probability?	The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.	the interpolated Kneser-Ney smoothing algorithm	2.693008707492364
Who gives up the idea of trying to make the language model a true probability distribution?	Stupid backoff gives up the idea of trying to make the language model a true probability distribution.	stupid backoff	2.687938677004185
What shows the bigram probabilities after normalization (dividing each cell in Fig?	Figure 3. 2 shows the bigram probabilities after normalization (dividing each cell in Fig.	figure 3. 2	2.6852936766409967
What did Chen and Goodman produce a highly influential series of with a comparison of different language models?	Starting in the late 1990s, Chen and Goodman produced a highly influential series of papers with a comparison of different language models (Chen and Goodman 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).	of papers	2.681027824462507
What did L. N. workshop on in Practice in Gelsema, E. S. and Kanal?	In Gelsema, E. S. and Kanal, L. N. , Proceedings, Workshop on Pattern Recognition in Practice, 381397.	on Pattern Recognition	2.6775779742487305
What explains otherwise mysterious properties of perplexity (why the inverse probability, ?	The perplexity measure actually arises from the information-theoretic concept of cross-entropy, which explains otherwise mysterious properties of perplexity (why the inverse probability, for example? )	cross-entropy	2.6722845426002158
Who were the IBM Thomas J. Watson Research Center influenced by?	The resurgence of n-gram models came from Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and Baker at CMU, who was influenced by the work of Baum and colleagues.	by Shannon, and Baker	2.6708669315052225
Who has since become the standard baseline for language modeling?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.	Modified Interpolated KneserNey	2.667506974507734
'll such AAC devices first need to augment each sentence with a special symbol <s> at the beginning of the sentence?	We 'll first need to augment each sentence with a special symbol <s> at the beginning of the sentence, to give us the bigram context of the first word.		2.6666078302588128
What does have an effect on metrics like perplexity?	The exact choice of <UNK> model does have an effect on metrics like perplexity.	the exact choice of <UNK> model	2.6617919439844506
What were Markov models commonly used in by the 1950s?	Based on Shannon's work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s.	in engineering	2.660787552916629
What occurs 400 times in a corpus of a million words like the Brown corpus for example?	For example, suppose the word Chinese occurs 400 times in a corpus of a million words like the Brown corpus.	Chinese	2.6595320888022504
What did Shannon apply n-grams to compute approximations to?	Shannon applied n-grams to compute approximations to English word sequences.	to English word sequences	2.659017366308785
What did Shannon apply n-grams to compute to English word sequences?	Shannon applied n-grams to compute approximations to English word sequences.	approximations	2.6587424248046463
When just like any other token?	Include <s> and </s> in your counts just like any other token.	include <s> and </s> in average's counts	2.654012799873674
What can achieve low perplexity by assigning the unknown word a high probability?	A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability.	a language model	2.6522164790494265
What did Shannon apply to compute approximations to English word sequences?	Shannon applied n-grams to compute approximations to English word sequences.	n-grams	2.6519294993583054
What does SRILM offer a wider range of?	SRILM offers a wider range of options and types of discounting, while KenLM is optimized for speed and memory size, making it possible to build web-scale language models.	of options and types of discounting	2.650736378833325
What augments absolute discounting with a more sophisticated way to handle the lower-order unigram distribution?	Kneser-Ney discounting (Kneser and Ney, 1995) augments absolute discounting with a more sophisticated way to handle the lower-order unigram distribution.	kneser-Ney discounting	2.649939219579178
What is a better n-gram model?	A better n-gram model is one that assigns a higher probability to the A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY test data, and perplexity is a normalized version of the probability of the test set.	one that assigns a higher probability to the A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY test data	2.6429346790135764
What produced a highly influential series of papers with a comparison of different language models?	Starting in the late 1990s, Chen and Goodman produced a highly influential series of papers with a comparison of different language models (Chen and Goodman 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).	Chen and Goodman	2.6427926660125896
Who did the resurgence of n-gram models come from at CMU?	The resurgence of n-gram models came from Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and Baker at CMU, who was influenced by the work of Baum and colleagues.	from Jelinek and colleagues at the IBM Thomas J. Watson Research Center	2.641002468207303
What is the number of possible next words that can follow any word?	The branching factor of a language is the number of possible next words that can follow any word.	the branching factor of a language	2.633759965734713
Who does kneser-Ney discounting augment with a more sophisticated way to handle the lower-order unigram distribution?	Kneser-Ney discounting (Kneser and Ney, 1995) augments absolute discounting with a more sophisticated way to handle the lower-order unigram distribution.	absolute discounting	2.627888431479611
What is extrinsic evaluation?	Extrinsic evaluation is the only way to know if a particular improvement in a component is really going to help the task at hand.	the only way to know if a particular improvement in a component is really going to help the task at hand	2.6261758804420277
Who has probability S = count et al?	in referring to it as S: S (w wi1) otherwise The backoff terminates in the unigram, which has probability S = count et al.	the unigram	2.6250897590513884
What did originally distribute as IBM technical report in?	Originally distributed as IBM technical report in Jelinek, F. and Mercer, R. .	in Jelinek, F. and Mercer	2.624490142833152
Who am devset corpus Calculate the probability of the sentence i want chinese food?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.	Sam	2.6167894760979373
What look at an n-gram grammar trained on a completely different corpus: the Wall Street Journal newspaper?	To get an idea of the dependence of a grammar on its training set, let's look at an n-gram grammar trained on a completely different corpus: the Wall Street Journal newspaper.	let 's	2.6161988725234666
Who am devset Sam corpus the probability of the sentence i want chinese food?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.	Calculate	2.6147287505149053
What did Chen and Goodman produce with a comparison of different language models?	Starting in the late 1990s, Chen and Goodman produced a highly influential series of papers with a comparison of different language models (Chen and Goodman 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).	a highly influential series of papers	2.614487114757409
What are probabilities also important for (Trnka et al?	Probabilities are also important for augmentative and alternative communication systems (Trnka et al.	for augmentative and alternative communication systems	2.6142826652924778
What would sum to one without an end-symbol?	Without an end-symbol, the sentence probabilities for all sentences of a given length would sum to one.	the sentence probabilities for all sentences of a given length	2.6110753747960285
What does a DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allow Chen to use?	It A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allows us to use some m, which is a model of p (i. e. , an approximation to p).	some m	2.610752329525326
What does absolute discounting formalize by subtracting a fixed discount d from each count?	Absolute discounting formalizes this intuition by subtracting a fixed discount d from each count.	this intuition	2.6094055989390523
What would define an infinite set of probability distributions?	This model would define an infinite set of probability distributions, with one distribution per sentence length.	this model	2.6074970638841357
What am devset Sam corpus Calculate i want chinese food?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.	the probability of the sentence	2.5979692322890613
What is convenient to describe how a smoothing algorithm affects the numerator?	Instead of changing both the numerator and denominator, it is convenient to describe how a smoothing algorithm affects the numerator, by defining an adjusted count c.	what	2.5941372717452778
What did L. N. workshop on Pattern Recognition in in Gelsema, E. S. and Kanal?	In Gelsema, E. S. and Kanal, L. N. , Proceedings, Workshop on Pattern Recognition in Practice, 381397.	in Practice	2.591160041342741
When did Chen and Goodman start?	Starting in the late 1990s, Chen and Goodman produced a highly influential series of papers with a comparison of different language models (Chen and Goodman 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).	in the late 1990s	2.58624547680255
Who backing-off for Mgram language modeling?	Improved backing-off for Mgram language modeling.	Improved	2.585772613039842
Who were influenced by Shannon, and Baker?	The resurgence of n-gram models came from Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and Baker at CMU, who was influenced by the work of Baum and colleagues.	the IBM Thomas J. Watson Research Center	2.5824831925537532
What are probabilities also important for augmentative and alternative communication systems?	Probabilities are also important for augmentative and alternative communication systems (Trnka et al.	(Trnka et al	2.5821615921748413
What distributed as IBM technical report in Jelinek, F. and Mercer?	Originally distributed as IBM technical report in Jelinek, F. and Mercer, R. .	originally	2.5813334366019034
What are summarized in Gale and Church?	Problems with the addone algorithm are summarized in Gale and Church.	problems with the addone algorithm	2.580108587583613
What am devset Sam corpus Calculate the probability of the sentence i want?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.	chinese food	2.579316361859873
What does the exact choice of <UNK> model have?	The exact choice of <UNK> model does have an effect on metrics like perplexity.	an effect on metrics like perplexity	2.57911749048289
What can achieve low perplexity by choosing a small vocabulary?	A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability.	a language model	2.577015520869845
What project words into a continuous space in which words with similar contexts have similar representations?	Neural language models instead project words into a continuous space in which words with similar contexts have similar representations.	neural language models instead	2.5726938026167967
Who makes use of the probability of a word being a novel continuation?	Kneser-Ney smoothing makes use of the probability of a word being a novel continuation.	kneser-Ney smoothing	2.571965463390647
What offers a wider range of options and types of discounting?	SRILM offers a wider range of options and types of discounting, while KenLM is optimized for speed and memory size, making it possible to build web-scale language models.	SRILM	2.5706183300909116
What is Heafield et al?	Finally, efficient language model toolkits like KenLM (Heafield 2011, Heafield et al.	KenLM (Heafield 2011	2.5703316662748446
Who is J. K.?	IEEE Transactions on Pattern Analysis and Machine Baker, J. K. .	Machine Baker	2.5687766300607433
Whose 1812 law of succession does add-one smoothing derive from?	Add-one smoothing derives from Laplace's 1812 law of succession and was first applied as an engineering solution to the zero-frequency problem by Jeffreys based on an earlier Add-K suggestion by Johnson.	from Laplace's 1812 law of succession	2.5685650773792204
What Add-one smoothed bigram probabilities for eight of the words corpus of 9332 sentences?	Figure 3. 6 Add-one smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP corpus of 9332 sentences.	figure 3. 6	2.566084201530862
What are problems with the addone algorithm summarized in?	Problems with the addone algorithm are summarized in Gale and Church.	in Gale and Church	2.565247872552795
What is then = log = 3 bits N- GRAM L ANGUAGE M ODELS Until now Chen have been computing the entropy of a single variable?	The entropy of the choice of horses is then = log = 3 bits N- GRAM L ANGUAGE M ODELS Until now we have been computing the entropy of a single variable.	the entropy of the choice of horses	2.564873630800733
What is a held-out corpus an additional training corpus like?	A held-out corpus is an additional training corpus that we use to set hyperparameters like these values, by choosing the values that maximize the likelihood of the held-out corpus.	that these zeros-- things that do not ever occur in the training set but do occur in the test set-- use to set hyperparameters like these values	2.562848485758335
What is KenLM (Heafield 2011?	Finally, efficient language model toolkits like KenLM (Heafield 2011, Heafield et al.	Heafield et al	2.5623731371169214
What the more information gives such AAC devices about the word sequence the perplexity (since as Eq?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.	the n-gram	2.561550833940915
If the probability of any word in the test set is 0 what is 0?	Second, if the probability of any word in the test set is 0, the entire probability of the test set is 0.	the entire probability of the test set	2.5581977572665417
What did Noam Chomsky argue in a series of extremely influential papers including Chomsky and Miller and Chomsky?	In a series of extremely influential papers starting with Chomsky and including Chomsky and Miller and Chomsky, Noam Chomsky argued that ``finite-state Markov processes'', while a possibly useful engineering heuristic, were incapable of being a complete cognitive model of human grammatical knowledge.	that ``finite-state Markov processes'' were incapable of being a complete cognitive model of human grammatical knowledge	2.551867882604157
What did Markov classify?	Markov classified 20,000 letters as V or C and computed the bigram and trigram probability that a given letter would be a vowel given the previous one or two letters.	20,000 letters as V or C	2.5509405178810036
Who is I. H. C.?	In NAACL-HLT 07,173 Witten, I. H. C. .	Witten	2.5475588523037556
What 'll such AAC devices introduce in later chapters?	In later chapters we 'll introduce more sophisticated language models like the RNN LMs of Chapter 9.	more sophisticated language models like the RNN LMs of Chapter 9	2.547245352346129
What consists of 91 zeros and 1 each of the other digits 1-9?	3. 12 Given a training set of 100 numbers consists of 91 zeros and 1 each of the other digits 1-9.	3. 12 Given a training set of 100 numbers	2.5463147259214205
What is to turn the problem back into a closed vocabulary one by choosing a fixed vocabulary in advance: 1?	The first one is to turn the problem back into a closed vocabulary one by choosing a fixed vocabulary in advance: 1.	the first one	2.545523484230996
What is one that measures the quality of a model independent of any application?	An intrinsic evaluation metric is one that measures the quality of a model independent of any application.	an intrinsic evaluation metric	2.540025455468476
What does figure 3. 5 show?	Figure 3. 5 shows the add-one smoothed counts for the Figure 3. 5 Add-one smoothed bigram counts for eight of the words (out of V = 1446) in the Berkeley Restaurant Project corpus of 9332 sentences.	the add-one smoothed counts for the Figure 3. 5 Add-one smoothed bigram counts for eight of the words corpus of 9332 sentences	2.536692618315884
Who can such AAC devices use for the first trigram We always represent and compute language model probabilities in log format as log probabilities for example?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.	two pseudo-words	2.536248730574014
Who did Chen show the advantages of?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.	of Modified Interpolated KneserNey	2.5356148895176602
What discussed the problem of words whose bigram probability is zero?	The previous section discussed the problem of words whose bigram probability is zero.	the previous section	2.5328052941143593
What can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus?	We can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus.	such AAC devices	2.5326971291860265
What were commonly used in engineering by the 1950s?	Based on Shannon's work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s.	Markov models	2.53127864905258
What is the perplexity of a language model on a test set?	The perplexity (sometimes called PP for short) of a language model on a test set is the inverse probability of the test set, normalized by the number of words.	the inverse probability of the test set	2.5306018634753897
What does figure 3. 2 show after normalization (dividing each cell in Fig?	Figure 3. 2 shows the bigram probabilities after normalization (dividing each cell in Fig.	the bigram probabilities	2.5280542473821632
What can Chen compute by computing a very long sample of the output's average log probability?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.	the entropy of some stochastic process	2.52341640282618
What solve a major problem with n-gram language models: n-grams have no way to generalize from training to test set?	These solve a major problem with n-gram language models: the number of parameters increases exponentially as the n-gram order increases, and n-grams have no way to generalize from training to test set.	these	2.5212866703448613
What would be nice to have a metric that can be used to quickly evaluate potential improvements in a language model?	Instead, it would be nice to have a metric that can be used to quickly evaluate potential improvements in a language model.	speech recognition	2.5211762961202595
What did L. N. workshop on Pattern Recognition in Practice in?	In Gelsema, E. S. and Kanal, L. N. , Proceedings, Workshop on Pattern Recognition in Practice, 381397.	in Gelsema, E. S. and Kanal	2.5206779009090243
What counts and language models from the common crawl?	N-gram counts and language models from the common crawl.	N-gram	2.5188390271085774
Whose work were Markov models based on?	Based on Shannon's work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s.	on Shannon's work	2.518085118331018
If a higher-order n-gram has a zero count, what simply backoff to a lower order n-gram?	If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed weight.	Chen	2.5164319961251578
What do such AAC devices not know?	We do not know any way to compute the exact probability of a word given a long sequence of preceding words, 1).	any way to compute the exact probability of a word given a long sequence of preceding words)	2.509505082002368
What 'll follow Brants et al?	This algorithm does not produce a probability distribution, so we 'll follow Brants et al.	Chen	2.5086233049017226
What suggests that such AAC devices could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities?	Equation 3. 4 suggests that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities.	Equation 3. 4	2.508013466833307
What is generally represented in memory as a 64-bit hash number?	Rather than store each word as a string, it is generally represented in memory as a 64-bit hash number, with the words themselves stored on disk.	P.	2.5077684994955436
What is the perplexity of a test set according to a language model the geometric mean of?	The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.	of the inverse test set probability computed by the model	2.5063149960377573
What does KenLM make possible to build web-scale language models?	SRILM offers a wider range of options and types of discounting, while KenLM is optimized for speed and memory size, making it possible to build web-scale language models.	memory size	2.5049611383418395
What can be computed by Eq?	These adjusted counts can be computed by Eq.	these adjusted counts	2.5030457265079447
What does N-gram count from the common crawl?	N-gram counts and language models from the common crawl.	and language models	2.501650207679017
What is that the probabilities often encode specific facts about a given training corpus?	One implication of this is that the probabilities often encode specific facts about a given training corpus.	one implication of this	2.5003926939362717
What does figure 3. 2 show the bigram probabilities after?	Figure 3. 2 shows the bigram probabilities after normalization (dividing each cell in Fig.	after normalization (dividing each cell in Fig	2.4996533605132214
What rely on lower-order n-gram counts through backoff or interpolation?	Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.	commonly used smoothing algorithms for n-grams	2.4988329521829384
What only give an approximation to the correct distributions and entropies of natural language?	Thus, our statistical models only give an approximation to the correct distributions and entropies of natural language.	Chen's statistical models	2.4977460804981084
What is the branching factor of a language the number of that can follow any word?	The branching factor of a language is the number of possible next words that can follow any word.	of possible next words	2.4965191478451456
What do Let's say that 0 occur?	Let's say that 0 occur 91 times in the training set, and each of the other digits occurred 1 time each.	91 times in the training set, and each of the other digits occurred 1 time each	2.494886587766973
What does Equation 3. 4 suggest that such AAC devices could estimate the joint probability of by multiplying together a number of conditional probabilities?	Equation 3. 4 suggests that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities.	of an entire sequence of words	2.491880623726821
What is to move a bit less of the probability mass from the seen to the unseen events?	One alternative to add-one smoothing is to move a bit less of the probability mass from the seen to the unseen events.	one alternative to add-one smoothing	2.490265738274322
What can use two pseudo-words for the first trigram We always represent and compute language model probabilities in log format as log probabilities for example?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.	such AAC devices	2.487105732422574
What present ways to modify the MLE estimates slightly to get better probability estimates in Section 3. 4?	We present ways to modify the MLE estimates slightly to get better probability estimates in Section 3. 4.	such AAC devices	2.4864183789391507
What can a language model achieve by assigning the unknown word a high probability?	A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability.	low perplexity	2.4834245732534663
What does Lee readings in?	and Lee, K. - F. , Readings in Speech Recognition, 450506.	in Speech Recognition	2.4814428632714765
What was add-one smoothing first applied as by Jeffreys based on an earlier Add-K suggestion by Johnson?	Add-one smoothing derives from Laplace's 1812 law of succession and was first applied as an engineering solution to the zero-frequency problem by Jeffreys based on an earlier Add-K suggestion by Johnson.	as an engineering solution to the zero-frequency problem	2.476189718442069
What does N-gram count and language models from?	N-gram counts and language models from the common crawl.	from the common crawl	2.474048747928518
What are what kinds of linguistic phenomena captured in?	What kinds of linguistic phenomena are captured in these bigram statistics?	in these bigram statistics	2.4738846926591336
What does most of what Chen will use entropy for involve?	But most of what we will use entropy for involves sequences.	sequences	2.4719865703381227
What were based on Shannon's work?	Based on Shannon's work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s.	Markov models	2.4719338643819833
Who is Church et al?	Another option is to build approximate language models using techniques like Bloom filters (Talbot and Osborne 2007, Church et al.	Osborne 2007	2.4698309309383077
What have chosen the sample words to cohere with each other in fact?	In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse.	such AAC devices	2.469715176940679
What is the same as the probability distribution at time t + 1 in other words?	In other words, the probability distribution for words at time t is the same as the probability distribution at time t + 1.	the probability distribution for words at time t	2.4686650581869247
What does the more information the n-gram give such AAC devices about the perplexity (since as Eq?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.	about the word sequence	2.4685924562172756
What did Chen perform?	They performed a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters.	a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters	2.467913164141573
What is the unigram with an interpolation weight?	The equation for interpolated absolute discounting applied to bigrams: PAbsoluteDiscounting (wi wi1) = P The first term is the discounted bigram, and the second term is the unigram with an interpolation weight.	the second term	2.461060275347561
What involves quite detailed computation for estimating the Good-Turing smoothing and the P and values?	The combined Good-Turing backoff algorithm involves quite detailed computation for estimating the Good-Turing smoothing and the P and values.	the combined Good-Turing backoff algorithm	2.459348655275941
Who is Machine Baker?	IEEE Transactions on Pattern Analysis and Machine Baker, J. K. .	J. K.	2.4590353476477853
What does figure 3. 6 add smoothed bigram probabilities for eight of the words corpus of?	Figure 3. 6 Add-one smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP corpus of 9332 sentences.	of 9332 sentences	2.4545516186645115
What does the more information the n-gram give such AAC devices about the word sequence?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.	the perplexity (since as Eq	2.452550527258081
What was (Bulletin de l'Academie Imperiale des Sciences de Mikolov?	Izvistia Imperatorskoi Akademii Nauk (Bulletin de l'Academie Imperiale des Sciences de Mikolov, T. .	T.	2.4490904486137
What was Proceedings of the IEEE?	Proceedings of the IEEE, 64,532557.	64	2.4490818614308334
What next chose a random bigram starting with w (again?	We next chose a random bigram starting with w (again, drawn according to its To give an intuition for the increasing power of higher-order n-grams, Fig.	such AAC devices	2.4483334559862637
What have such AAC devices chosen to cohere with each other in fact?	In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse.	the sample words	2.442721564601444
What is the perplexity of a language model on a test set the inverse probability of?	The perplexity (sometimes called PP for short) of a language model on a test set is the inverse probability of the test set, normalized by the number of words.	of the test set	2.442508685623469
What occurs because too much probability mass is moved to all the zeros?	The sharp change in counts and probabilities occurs because too much probability mass is moved to all the zeros.	the sharp change in counts and probabilities	2.442291199286276
What does efficient language model toolkits like?	Finally, efficient language model toolkits like KenLM (Heafield 2011, Heafield et al.	like KenLM (Heafield 2011	2.441702645508648
What did the resurgence of n-gram models come from Jelinek and colleagues at the IBM Thomas J. Watson Research Center at?	The resurgence of n-gram models came from Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and Baker at CMU, who was influenced by the work of Baum and colleagues.	at CMU	2.4413891811397312
What provide a more sophisticated way to estimate the probability of n-grams?	Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams.	smoothing algorithms	2.4409305639853596
What allows Chen to use some m?	It A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allows us to use some m, which is a model of p (i. e. , an approximation to p).	a DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY	2.440163964712776
What might 3. 3 have for any n-gram that occurred a sufficient number of times?	For any n-gram that occurred a sufficient number of times, we might have a good estimate of its probability.	a good estimate of words's probability	2.4398629418217035
What are the class of probabilistic models that assume such AAC devices can predict the probability of some future unit without looking too far into the past?	Markov models are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past.	Markov models	2.43929628400764
What makes memory size possible to build web-scale language models?	SRILM offers a wider range of options and types of discounting, while KenLM is optimized for speed and memory size, making it possible to build web-scale language models.	KenLM	2.435217660916637
If a higher-order n-gram has a zero count, what do Chen simply backoff to?	If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed weight.	to a lower order n-gram	2.4342593142893914
What does SRILM offer?	SRILM offers a wider range of options and types of discounting, while KenLM is optimized for speed and memory size, making it possible to build web-scale language models.	a wider range of options and types of discounting	2.43423112478722
What does figure 3. 6 add smoothed?	Figure 3. 6 Add-one smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP corpus of 9332 sentences.	bigram probabilities for eight of the words corpus of 9332 sentences	2.431704353648099
What is useful when Chen do not know the actual probability distribution p that generated some data?	The cross-entropy is useful when we do not know the actual probability distribution p that generated some data.	the cross-entropy	2.431459104673543
What have such AAC devices chosen the sample words to cohere with each other in?	In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse.	in fact	2.430925740122252
What can such AAC devices then measure the quality of by its performance on some unseen data called the test set or test corpus?	We can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus.	of an n-gram model	2.4284019590483044
What can be used to suggest likely words for the menu?	Word prediction can be used to suggest likely words for the menu.	word prediction	2.4281419623385685
What can such AAC devices use two pseudo-words for the first trigram We always represent and compute language model probabilities in log format as log probabilities for?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.	for example	2.4246588503604714
What started in the late 1990s?	Starting in the late 1990s, Chen and Goodman produced a highly influential series of papers with a comparison of different language models (Chen and Goodman 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).	Chen and Goodman	2.4235842054554606
What have such AAC devices chosen the sample words to cohere with in fact?	In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse.	with each other	2.42330252224046
What trigram language models with Golomb coding?	Compressing trigram language models with Golomb coding.	compressing	2.4225616355701955
What is based on the inverse probability of the test set by definition?	By definition, perplexity is based on the inverse probability of the test set.	perplexity	2.4194106446152217
Can we use the same technique to generate bigrams by first generating a random bigram that starts with <s>?	We can use the same technique to generate bigrams by first generating a random bigram that starts with <s> (according to its bigram probability).		2.419068963475902
What perplexity is a normalized version of the probability of the test set?	A better n-gram model is one that assigns a higher probability to the A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY test data, and perplexity is a normalized version of the probability of the test set.	a better n-gram model	2.4182688123067377
What do Chen's statistical models only give to the correct distributions and entropies of natural language?	Thus, our statistical models only give an approximation to the correct distributions and entropies of natural language.	an approximation	2.4178102400220993
What do such AAC devices introduce the simplest model that assigns probabilities to sentences and sequences of words in?	In this chapter we introduce the simplest model that assigns probabilities to sentences and sequences of words, the n-gram.	in this chapter	2.4165725958964823
What was a trigram model used in?	A trigram model was used in the IBM TANGORA speech recognition system in the 1970s, but the idea was not written up until later.	in the IBM TANGORA speech recognition system in the 1970s	2.4148331497924
What changed from 609 to 238!	C (want to) changed from 609 to 238!	C	2.4144238520811108
What are models that assign probabilities to sequences of words called?	Models that assign probabilities to sequences of words are called language models or LMs.	language models or LMs	2.413995011522448
What actually arises from the information-theoretic concept of cross-entropy for example? )	The perplexity measure actually arises from the information-theoretic concept of cross-entropy, which explains otherwise mysterious properties of perplexity (why the inverse probability, for example? )	the perplexity measure	2.4131697593483694
What were Markov models based on?	Based on Shannon's work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s.	on Shannon's work	2.412745779278802
What can such AAC devices compare by seeing which gives the more accurate transcription for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	the performance of two language models	2.4119498580142467
What saw above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code?	We saw above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code, so the average was 3.	Chen	2.411456598225618
What does the interpolated Kneser-Ney smoothing algorithm mix?	The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.	a discounted probability with a lower-order continuation probability	2.4101048717933815
What is the only way to know if a particular improvement in a component is really going to help the task at hand?	Extrinsic evaluation is the only way to know if a particular improvement in a component is really going to help the task at hand.	extrinsic evaluation	2.409986061775766
What is the entire sequence of words in some test set?	3. 16 is the entire sequence of words in some test set.	3. 16	2.4093737927927834
What is Osborne 2007?	Another option is to build approximate language models using techniques like Bloom filters (Talbot and Osborne 2007, Church et al.	Church et al	2.4069888805945396
What computed a bigram grammar from 22 million words of AP newswire?	They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words.	these zeros-- things that do not ever occur in the training set but do occur in the test set--	2.4066564398139216
What 'll Chen follow?	This algorithm does not produce a probability distribution, so we 'll follow Brants et al.	Brants et al	2.4051763739008227
What need the end-symbol to make the bigram grammar a true probability distribution?	2 We need the end-symbol to make the bigram grammar a true probability distribution.	such AAC devices	2.4049388507811695
What need a test set for an intrinsic evaluation of a language model?	For an intrinsic evaluation of a language model we need a test set.	such AAC devices	2.404207602819598
What is the better model given two probabilistic models?	Given two probabilistic models, the better model is the one that has a tighter fit to the test data or that better predicts the details of the test data, and hence will assign a higher probability to the test data.	the one that has a tighter fit to the test data or that better predicts the details of the test data, and hence will assign a higher probability to the test data	2.403172758055969
What is normalized by the number of words?	The perplexity (sometimes called PP for short) of a language model on a test set is the inverse probability of the test set, normalized by the number of words.	the test set	2.3993596634915098
What do commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through?	Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.	through backoff or interpolation	2.3987145403463526
What can Chen compute by taking a very long sample of the output?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.	the entropy of some stochastic process	2.3986320641991647
What does the more information the n-gram give about the word sequence the perplexity (since as Eq?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.	such AAC devices	2.3975925243129823
What call the initial test set the development test set or, devset in such cases?	In such cases, we call the initial test set the development test set or, devset.	such AAC devices	2.397303429116922
What can such AAC devices use two pseudo-words for We always represent and compute language model probabilities in log format as log probabilities for example?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.	for the first trigram	2.396965951387044
Who was the underlying mathematics of the n-gram first proposed by?	The underlying mathematics of the n-gram was first proposed by Markov, who used what are now called Markov chains (bigrams and trigrams) to predict whether an upcoming letter in Pushkin's Eugene Onegin would be a vowel or a consonant.	by Markov	2.3957653491418784
When would the sentence probabilities for all sentences of a given length sum to one?	Without an end-symbol, the sentence probabilities for all sentences of a given length would sum to one.	without an end-symbol	2.393241439521007
What can use a clever idea from Church and Gale?	To see this, we can use a clever idea from Church and Gale.	these zeros-- things that do not ever occur in the training set but do occur in the test set--	2.3893944627832266
What did Chen introduce perplexity in Section 3. 2. 1 as?	We introduced perplexity in Section 3. 2. 1 as a way to evaluate n-gram models on a test set.	as a way to evaluate n-gram models on a test set	2.3889660741432666
What is hand-corrected The longer the context on which such AAC devices train the model?	Output is hand-corrected The longer the context on which we train the model, the more coherent the sentences.	output	2.3855297042968324
What is dependent only on Pi1 for example in a bigram?	For example, in a bigram, Pi is dependent only on Pi1.	Pi	2.385473168322439
What do such AAC devices need for an intrinsic evaluation of a language model?	For an intrinsic evaluation of a language model we need a test set.	a test set	2.3848153757998825
What is commonly used as a quick check on an algorithm?	Nonetheless, because perplexity often correlates with such improvements, it is commonly used as a quick check on an algorithm.	perplexity	2.381934152182637
What can such AAC devices then measure by its performance on some unseen data called the test set or test corpus?	We can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus.	the quality of an n-gram model	2.380999165199105
What might have a good estimate of words's probability for any n-gram that occurred a sufficient number of times?	For any n-gram that occurred a sufficient number of times, we might have a good estimate of its probability.	3. 3	2.3807022266233266
What do Chen's statistical models only give an approximation to?	Thus, our statistical models only give an approximation to the correct distributions and entropies of natural language.	to the correct distributions and entropies of natural language	2.3805153361825666
What was N. and de Haan?	In Oostdijk, N. and de Haan, P. , Corpus-Based Research into Language, 189198.	P.	2.3791820158905015
What introduced perplexity in Section 3. 2. 1 as a way to evaluate n-gram models on a test set?	We introduced perplexity in Section 3. 2. 1 as a way to evaluate n-gram models on a test set.	Chen	2.3775484067191965
What do neural language models instead project words into in which words with similar contexts have similar representations?	Neural language models instead project words into a continuous space in which words with similar contexts have similar representations.	into a continuous space	2.3771026623408846
What is that a long-enough sequence of words will contain in it many other shorter sequences?	The intuition of the Shannon-McMillan-Breiman theorem is that a long-enough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence according to their probabilities.	the intuition of the Shannon-McMillan-Breiman theorem	2.374699167920784
What shows that they look a little too much like Shakespeare?	Indeed, a careful investigation of the 4-gram sentences shows that they look a little too much like Shakespeare.	a careful investigation of the 4-gram sentences	2.3746168423697545
What did such AAC devices next choose starting with w (again?	We next chose a random bigram starting with w (again, drawn according to its To give an intuition for the increasing power of higher-order n-grams, Fig.	a random bigram	2.3735704430800757
What is P. generally represented in as a 64-bit hash number?	Rather than store each word as a string, it is generally represented in memory as a 64-bit hash number, with the words themselves stored on disk.	in memory	2.370125286167991
What do commonly used smoothing algorithms for n-grams rely on through backoff or interpolation?	Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.	on lower-order n-gram counts	2.3688564332887783
What can a language model achieve by choosing a small vocabulary?	A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability.	low perplexity	2.3688288408833533
What 'll such AAC devices need to introduce for this reason?	For this reason, we 'll need to introduce cleverer ways of estimating the probability of a word w given a history h, or the probability of an entire word sequence W.	cleverer ways of estimating the probability of a word w given a history h, or the probability of an entire word sequence W	2.368237169664069
What is Pi dependent only on Pi1 for in a bigram?	For example, in a bigram, Pi is dependent only on Pi1.	for example	2.3666279804020887
What does the perplexity measure actually arise from for example? )	The perplexity measure actually arises from the information-theoretic concept of cross-entropy, which explains otherwise mysterious properties of perplexity (why the inverse probability, for example? )	from the information-theoretic concept of cross-entropy	2.365787453975557
What was CMU influenced by?	The resurgence of n-gram models came from Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and Baker at CMU, who was influenced by the work of Baum and colleagues.	by the work of Baum and colleagues	2.3657357057755157
What recursively back off to the Katz probability for the shorter-history- gram?	Otherwise, we recursively back off to the Katz probability for the shorter-history- gram.	these zeros-- things that do not ever occur in the training set but do occur in the test set--	2.3647303701997058
Who was first applied as an engineering solution to the zero-frequency problem by Jeffreys based on an earlier Add-K suggestion by Johnson?	Add-one smoothing derives from Laplace's 1812 law of succession and was first applied as an engineering solution to the zero-frequency problem by Jeffreys based on an earlier Add-K suggestion by Johnson.	add-one smoothing	2.363196603866759
What is the probability distribution for words at time t in other words?	In other words, the probability distribution for words at time t is the same as the probability distribution at time t + 1.	the same as the probability distribution at time t + 1	2.3618077732119316
What does modified Kneser-Ney use?	Rather than use a single fixed discount d, modified Kneser-Ney uses three different discounts d1, d2, and d3 + for n-grams with counts of 1,2 and three or more, respectively.	three different discounts d1, d2, and d3	2.3615314907328204
What is a better n-gram model perplexity a normalized version of?	A better n-gram model is one that assigns a higher probability to the A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY test data, and perplexity is a normalized version of the probability of the test set.	of the probability of the test set	2.3592096379817655
What can cause the perplexity to be artificially low?	Any kind of knowledge of the test set can cause the perplexity to be artificially low.	any kind of knowledge of the test set	2.3587396944313603
What did such AAC devices then compute on a test set of 1. 5 million words with Eq?	We then computed the perplexity of each of these models on a test set of 1. 5 million words with Eq.	the perplexity of each of these models	2.3582585708859987
What was linguistic, and psychological work on modeling word sequences?	Based on Shannon's work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s.	engineering	2.35665893718357
What is the second term with an interpolation weight?	The equation for interpolated absolute discounting applied to bigrams: PAbsoluteDiscounting (wi wi1) = P The first term is the discounted bigram, and the second term is the unigram with an interpolation weight.	the unigram	2.355214053202114
What does the perplexity measure actually arise from the information-theoretic concept of cross-entropy for? )	The perplexity measure actually arises from the information-theoretic concept of cross-entropy, which explains otherwise mysterious properties of perplexity (why the inverse probability, for example? )	for example	2.354814326068292
What will assign Kong a higher probability than glasses?	A standard unigram model will assign Kong a higher probability than glasses.	a standard unigram model	2.3538331747092016
What did Chen introduce perplexity in as a way to evaluate n-gram models on a test set?	We introduced perplexity in Section 3. 2. 1 as a way to evaluate n-gram models on a test set.	in Section 3. 2. 1	2.3526095987431734
What solve a major problem with n-gram language models: the number of parameters increases exponentially as the n-gram order increases?	These solve a major problem with n-gram language models: the number of parameters increases exponentially as the n-gram order increases, and n-grams have no way to generalize from training to test set.	these	2.3517636956037493
What might 3. 3 have a good estimate of for any n-gram that occurred a sufficient number of times?	For any n-gram that occurred a sufficient number of times, we might have a good estimate of its probability.	of words's probability	2.3509622051288868
What is Pi dependent only on Pi1 for example in?	For example, in a bigram, Pi is dependent only on Pi1.	in a bigram	2.3504100817943785
What say that 0 occur 91 times in the training set, and each of the other digits occurred 1 time each?	Let's say that 0 occur 91 times in the training set, and each of the other digits occurred 1 time each.	Let 's	2.3493461109396097
What showed that caches and class-based models provided only minor additional improvement?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.	Chen	2.3464799715558606
What can compute the entropy of some stochastic process by taking a very long sample of the output?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.	Chen	2.345769090068933
What can generalize the bigram to the trigram?	We can generalize the bigram (which looks one word into the past) to the trigram (which looks two words into the past) and thus to the n-gram (which looks n 1 words into the past).	such AAC devices	2.3448980414711174
What 'll such AAC devices use?	We 'll use data from the now-defunct Berkeley Restaurant Project, a dialogue system from the last century that answered questions about a database of restaurants in Berkeley, California (Jurafsky et al. , 1994).	data from the now-defunct Berkeley Restaurant Project	2.3441984130717497
Who is Goodman 2006?	Starting in the late 1990s, Chen and Goodman produced a highly influential series of papers with a comparison of different language models (Chen and Goodman 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).	Goodman 1999	2.3420302041030805
What do not use raw probability as our metric for evaluating language models, but a variant called perplexity in practice?	In practice we do not use raw probability as our metric for evaluating language models, but a variant called perplexity.	such AAC devices	2.341474976917094
Who is K. - F.?	and Lee, K. - F. , Readings in Speech Recognition, 450506.	Lee	2.3409392394075685
What are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq?	We are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq.	Chen	2.338003296163711
What can such AAC devices compare by running the speech recognizer twice for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	the performance of two language models	2.3363085352527877
What did the resulting parameter set maximizes in MLE?	In MLE, the resulting parameter set maximizes the likelihood of the training set T given the model M (i. e. , P (T M) ).	the likelihood of the training set T given the model M	2.3359114537355476
What am devset Sam corpus Calculate?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.	the probability of the sentence i want chinese food	2.334695474267262
What does KenLM make memory size possible to build?	SRILM offers a wider range of options and types of discounting, while KenLM is optimized for speed and memory size, making it possible to build web-scale language models.	web-scale language models	2.3337482174151436
What do compressing trigram with Golomb coding?	Compressing trigram language models with Golomb coding.	language models	2.3335796787501057
What do such AAC devices call the initial test set the development test set or, devset in?	In such cases, we call the initial test set the development test set or, devset.	in such cases	2.329309573956955
Who was add-one smoothing first applied as an engineering solution to the zero-frequency problem by by Johnson?	Add-one smoothing derives from Laplace's 1812 law of succession and was first applied as an engineering solution to the zero-frequency problem by Jeffreys based on an earlier Add-K suggestion by Johnson.	by Jeffreys based on an earlier Add-K suggestion	2.328777140669133
What would such AAC devices want to pick at the minimum?	At the minimum, we would want to pick N- GRAM L ANGUAGE M ODELS the smallest test set that gives us enough statistical power to measure a statistically significant difference between two potential models.	N- GRAM L ANGUAGE M ODELS the smallest test set that gives us enough statistical power to measure a statistically significant difference between two potential models	2.3282224111819727
Who does a careful investigation of the 4-gram sentences show that they look a little too much like?	Indeed, a careful investigation of the 4-gram sentences shows that they look a little too much like Shakespeare.	like Shakespeare	2.32774254732925
What continue generating words until we randomly generate the sentence-final token </s>?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.	such AAC devices	2.326515789588475
What are both the simple interpolation and conditional interpolation s learned from?	Both the simple interpolation and conditional interpolation s are learned from a held-out corpus.	from a held-out corpus	2.3259602372021457
What saw there in Eq?	We are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq.	Chen	2.321895810872142
What might expect some overlap between our n-grams for the two genres?	Shakespeare and the Wall Street Journal are both English, so we might expect some overlap between our n-grams for the two genres.	sentences and sequences of words, the n-gram	2.3191705609345528
What shows the add-one smoothed counts for the Figure 3. 5 Add-one smoothed bigram counts for eight of the words corpus of 9332 sentences?	Figure 3. 5 shows the add-one smoothed counts for the Figure 3. 5 Add-one smoothed bigram counts for eight of the words (out of V = 1446) in the Berkeley Restaurant Project corpus of 9332 sentences.	figure 3. 5	2.3191209336407432
What 'll we need to introduce for this reason?	For this reason, we 'll need to introduce cleverer ways of estimating the probability of a word w given a history h, or the probability of an entire word sequence W.	cleverer ways of estimating the probability of a word w given a history h, or the probability of an entire word sequence W	2.318993600987038
What is perplexity based on the inverse probability of the test set by?	By definition, perplexity is based on the inverse probability of the test set.	by definition	2.314755361104429
What do 3. 3 shows random sentences generated from unigram, bigram, trigram, and 4-gram To devset swallowed confess hear?	3. 3 shows random sentences generated from unigram, bigram, trigram, and 4-gram To him swallowed confess hear both.	both	2.3095137896960893
What can be dependent on events that were arbitrarily distant and time dependent?	But natural language is not stationary, since as we show in Chapter 12, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent.	the probability of upcoming words	2.3025537234066618
What can be estimated by counting in a corpus?	n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).	n-gram probabilities	2.29912944340625
What may even simple extensions of the example sentence have on the web?	Even simple extensions of the example sentence may have counts of zero on the web (such as ``Walden Pond's water is so transparent that the''; well, used to have counts of zero).	counts of zero	2.298012728274436
What can such AAC devices generalize the bigram to?	We can generalize the bigram (which looks one word into the past) to the trigram (which looks two words into the past) and thus to the n-gram (which looks n 1 words into the past).	to the trigram	2.2979793767443093
What can such AAC devices generalize to the trigram?	We can generalize the bigram (which looks one word into the past) to the trigram (which looks two words into the past) and thus to the n-gram (which looks n 1 words into the past).	the bigram	2.2973734724014707
What do such AAC devices need a test set for?	For an intrinsic evaluation of a language model we need a test set.	for an intrinsic evaluation of a language model	2.29692168842649
What can compute the entropy of some stochastic process by computing a very long sample of the output's average log probability?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.	Chen	2.294589615620198
Who require discounting to create a probability distribution?	Both backoff and interpolation require discounting to create a probability distribution.	both backoff and interpolation	2.294377909228429
What do the probabilities of an n-gram model come from the corpus with?	As with many of the statistical models in our field, the probabilities of an n-gram model come from the corpus it is trained on, the training set or training corpus.	with many of the statistical models in such AAC devices's field	2.2914828375484406
What is to have a variable that ranges over sequences of words?	One way to do this is to have a variable that ranges over sequences of words.	one way to do this	2.2879286254064275
What is P. generally represented in memory as?	Rather than store each word as a string, it is generally represented in memory as a 64-bit hash number, with the words themselves stored on disk.	as a 64-bit hash number	2.2862940003010337
What did Chen show?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.	the advantages of Modified Interpolated KneserNey	2.2853581583113964
What can such AAC devices then measure the quality of an n-gram model by on some unseen data called the test set or test corpus?	We can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus.	by its performance	2.2841325492723126
What need a training corpus of legal documents?	To build a language model for translating legal documents, we need a training corpus of legal documents.	3. 3	2.278084570231797
What is the second term the unigram with?	The equation for interpolated absolute discounting applied to bigrams: PAbsoluteDiscounting (wi wi1) = P The first term is the discounted bigram, and the second term is the unigram with an interpolation weight.	with an interpolation weight	2.273959525873092
What is perplexity based on by definition?	By definition, perplexity is based on the inverse probability of the test set.	on the inverse probability of the test set	2.271736332469184
What would such AAC devices have to get is so transparent and divide by the sum of the counts of all possible five word sequences?	We would have to get the count of its water is so transparent and divide by the sum of the counts of all possible five word sequences.	the count of an entire sentence's water	2.268409916311001
What are the trigram and 4-gram sentences beginning to look?	The trigram and 4-gram sentences are beginning to look a lot like Shakespeare.	a lot like Shakespeare	2.266796998447736
What are learned from a held-out corpus?	Both the simple interpolation and conditional interpolation s are learned from a held-out corpus.	both the simple interpolation and conditional interpolation s	2.2644989584494577
What does stupid backoff give up?	Stupid backoff gives up the idea of trying to make the language model a true probability distribution.	the idea of trying to make the language model a true probability distribution	2.2641905866799745
Who are both English?	Shakespeare and the Wall Street Journal are both English, so we might expect some overlap between our n-grams for the two genres.	Shakespeare and the Wall Street Journal	2.264094808736047
What is an intuitive way to estimate probabilities called?	An intuitive way to estimate probabilities is called maximum likelihood estimation or MLE.	maximum likelihood estimation or MLE	2.264028252247922
When is KenLM optimized?	SRILM offers a wider range of options and types of discounting, while KenLM is optimized for speed and memory size, making it possible to build web-scale language models.	for speed and memory size	2.2626492207627305
What is one of the most commonly used and best performing n-gram smoothing methods?	One of the most commonly used and best performing n-gram smoothing methods is the interpolated Kneser-Ney algorithm (Kneser and Ney 1995, Chen and Goodman 1998).	the interpolated Kneser-Ney algorithm	2.257972156499089
What was L. N.?	In Gelsema, E. S. and Kanal, L. N. , Proceedings, Workshop on Pattern Recognition in Practice, 381397.	Proceedings	2.25714429887557
What can Chen compute the entropy of by taking a very long sample of the output?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.	of some stochastic process	2.2568614468489856
What do such AAC devices not use as our metric for evaluating language models, but a variant called perplexity in practice?	In practice we do not use raw probability as our metric for evaluating language models, but a variant called perplexity.	raw probability	2.2473499777712296
What choose a random value between 0 and 1?	We choose a random value between 0 and 1 and print the word whose interval includes this chosen value.	such AAC devices	2.246678554040642
What do such AAC devices continue generating until we randomly generate the sentence-final token </s>?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.	words	2.2465936600477976
What did C change from 609 to!	C (want to) changed from 609 to 238!	to 238	2.2452499168670124
What would the sentence probabilities for all sentences of a given length sum to without an end-symbol?	Without an end-symbol, the sentence probabilities for all sentences of a given length would sum to one.	to one	2.24422901198413
What does kneser-Ney discounting augment absolute discounting with?	Kneser-Ney discounting (Kneser and Ney, 1995) augments absolute discounting with a more sophisticated way to handle the lower-order unigram distribution.	with a more sophisticated way to handle the lower-order unigram distribution	2.2416818960652933
What do not know any way to compute the exact probability of a word given a long sequence of preceding words)?	We do not know any way to compute the exact probability of a word given a long sequence of preceding words, 1).	such AAC devices	2.2409954669790584
What is said to be stationary if the probabilities it assigns to a sequence are invariant with respect to shifts in the time index?	A stochastic process is said to be stationary if the probabilities it assigns to a sequence are invariant with respect to shifts in the time index.	a stochastic process	2.2409497983764766
What's important not to let the test sentences into the training set?	Since our evaluation metric is based on test set probability, it's important not to let the test sentences into the training set.	such end-to-end evaluation	2.240906472180864
What is the intuition of the Shannon-McMillan-Breiman theorem that a long-enough sequence of words will contain in it?	The intuition of the Shannon-McMillan-Breiman theorem is that a long-enough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence according to their probabilities.	many other shorter sequences	2.240369916527085
What was first proposed by Markov?	The underlying mathematics of the n-gram was first proposed by Markov, who used what are now called Markov chains (bigrams and trigrams) to predict whether an upcoming letter in Pushkin's Eugene Onegin would be a vowel or a consonant.	the underlying mathematics of the n-gram	2.2396809718405803
Who were mapped to lower-case?	All characters were mapped to lower-case and punctuation marks were treated as words.	all characters	2.239486412691714
What can the test set only contain words from in such a closed vocabulary system?	In such a closed vocabulary system the test set can only contain words from this lexicon, and there will be no unknown words.	from this lexicon	2.2386881386183806
What are Shakespeare and the Wall Street Journal?	Shakespeare and the Wall Street Journal are both English, so we might expect some overlap between our n-grams for the two genres.	both English	2.235246675770102
What is called Laplace smoothing?	This algorithm is called Laplace smoothing.	this algorithm	2.2333596403303884
What did C change from to 238!	C (want to) changed from 609 to 238!	from 609	2.233012541232167
What can Chen compute the entropy of by computing a very long sample of the output's average log probability?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.	of some stochastic process	2.2300927760925906
What is equivalent to multiplying in linear space?	E VALUATING L ANGUAGE M ODELS Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them.	E VALUATING L ANGUAGE M ODELS Adding in log space	2.221307217006295
What should a model's improvement in perplexity always be confirmed by before concluding the evaluation of the model?	But a model's improvement in perplexity should always be confirmed by an end-to-end evaluation of a real task before concluding the evaluation of the model.	by an end-to-end evaluation of a real task	2.220575940304668
What does figure 3. 6 show?	Figure 3. 6 shows the add-one smoothed probabilities for the bigrams in Fig.	the add-one smoothed probabilities for the bigrams in Fig	2.2202915193210644
What was then hand-corrected for capitalization to improve readability?	Output was then hand-corrected for capitalization to improve readability.	output	2.2193955322604997
What did Chen perform a number of comparing different discounting algorithms, cache models, class-based models, and other language model parameters?	They performed a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters.	of carefully controlled experiments	2.2158519527838587
What are some perfectly acceptable English word sequences bound to be missing from?	But because any corpus is limited, some perfectly acceptable English word sequences are bound to be missing from it.	from a very large corpus	2.210954004330201
What see in Section 3. 7 that perplexity is also closely related to the informationtheoretic notion of entropy?	We see in Section 3. 7 that perplexity is also closely related to the informationtheoretic notion of entropy.	such AAC devices	2.209770024786979
What terminates in the unigram?	in referring to it as S: S (w wi1) otherwise The backoff terminates in the unigram, which has probability S = count et al.	the backoff	2.2089258680854407
What assigns a higher probability to the test set?	The answer is simple: whichever model assigns a higher probability to the test set-- meaning it more accurately predicts the test set-- is a better model.	model	2.207560902518374
What does add-one smoothing derive from?	Add-one smoothing derives from Laplace's 1812 law of succession and was first applied as an engineering solution to the zero-frequency problem by Jeffreys based on an earlier Add-K suggestion by Johnson.	from Laplace's 1812 law of succession	2.2063914927170476
What does kneser-Ney smoothing make use of?	Kneser-Ney smoothing makes use of the probability of a word being a novel continuation.	of the probability of a word being a novel continuation	2.2045320785320413
When does the length of the observed word sequence go?	Cross-entropy is defined in the limit, as the length of the observed word sequence goes to infinity.	to infinity	2.2043871861110804
What is the discounted bigram?	The equation for interpolated absolute discounting applied to bigrams: PAbsoluteDiscounting (wi wi1) = P The first term is the discounted bigram, and the second term is the unigram with an interpolation weight.	the equation for interpolated absolute discounting applied to bigrams: PAbsoluteDiscounting = P The first term	2.2020125403754824
What does cross-entropy explain otherwise mysterious properties of?	The perplexity measure actually arises from the information-theoretic concept of cross-entropy, which explains otherwise mysterious properties of perplexity (why the inverse probability, for example? )	of perplexity (why the inverse probability,	2.2014028639796224
Who was add-one smoothing first applied as an engineering solution to the zero-frequency problem by Jeffreys based on an earlier Add-K suggestion by?	Add-one smoothing derives from Laplace's 1812 law of succession and was first applied as an engineering solution to the zero-frequency problem by Jeffreys based on an earlier Add-K suggestion by Johnson.	by Johnson	2.1970352724415556
What do such AAC devices need?	2 We need the end-symbol to make the bigram grammar a true probability distribution.	the end-symbol to make the bigram grammar a true probability distribution	2.1968587266238204
What have some local word-to-word coherence?	The bigram sentences have some local word-to-word coherence (especially if we consider that punctuation counts as a word).	the bigram sentences	2.194349778742073
What is called maximum likelihood estimation or MLE?	An intuitive way to estimate probabilities is called maximum likelihood estimation or MLE.	an intuitive way to estimate probabilities	2.1939269324808177
What is Entropy a measure of?	Entropy is a measure of information.	of information	2.1935138550817537
What can compare the performance of two language models by running the speech recognizer twice for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	such AAC devices	2.1899079580088006
What do both backoff and interpolation require discounting to create?	Both backoff and interpolation require discounting to create a probability distribution.	a probability distribution	2.186952438026984
What does 3. 12 Given a training set of 100 numbers consist of?	3. 12 Given a training set of 100 numbers consists of 91 zeros and 1 each of the other digits 1-9.	of 91 zeros and 1 each of the other digits 1-9	2.1863119784811333
What was drawn according to again's To give an intuition for the increasing power of higher-order n-grams, Fig?	We next chose a random bigram starting with w (again, drawn according to its To give an intuition for the increasing power of higher-order n-grams, Fig.	w	2.1857459763738727
What will Chen be computing for a grammar for example?	For a grammar, for example, we will be computing the entropy of some sequence of words W = w0, w1, w2, . . . , wn.	the entropy of some sequence of words W = w0 w2, . . .	2.185231827469148
Who is Goodman 1999?	Starting in the late 1990s, Chen and Goodman produced a highly influential series of papers with a comparison of different language models (Chen and Goodman 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).	Goodman 2006	2.184890816852407
What is perplexity commonly used as a quick check on?	Nonetheless, because perplexity often correlates with such improvements, it is commonly used as a quick check on an algorithm.	on an algorithm	2.1840914124926436
What did Chen see there in?	We are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq.	in Eq	2.18240411812059
What only ``back off'' to a lower-order n-gram if we have zero evidence for a higher-order n-gram in other words?	In other words, we only ``back off'' to a lower-order n-gram if we have zero evidence for a higher-order n-gram.	these zeros-- things that do not ever occur in the training set but do occur in the test set--	2.1819844529397763
What are stored in reverse tries?	Probabilities are generally quantized using only 4-8 bits (instead of 8-byte floats), and n-grams are stored in reverse tries.	n-grams	2.176135942880008
What do compressing trigram language models with?	Compressing trigram language models with Golomb coding.	with Golomb coding	2.1760413571052126
What answers the question ``How likely''?	In other words, instead of P, which answers the question ``How likely is w?''	p	2.17586846102533
What can any kind of knowledge of the test set cause?	Any kind of knowledge of the test set can cause the perplexity to be artificially low.	the perplexity to be artificially low	2.1756266370147834
Who was Proceedings?	In Gelsema, E. S. and Kanal, L. N. , Proceedings, Workshop on Pattern Recognition in Practice, 381397.	L. N.	2.1748771179615733
What does kneser-Ney smoothing make?	Kneser-Ney smoothing makes use of the probability of a word being a novel continuation.	use of the probability of a word being a novel continuation	2.1747971164910385
What is Pi dependent only on for example in a bigram?	For example, in a bigram, Pi is dependent only on Pi1.	only on Pi1	2.1731187097705877
What is a model of p?	It A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allows us to use some m, which is a model of p (i. e. , an approximation to p).	the m	2.1685771886566605
What do such AAC devices leave as exercise 12?	We leave this exact calculation as exercise 12.	this exact calculation	2.1685440624454673
What are Markov models the class of that assume such AAC devices can predict the probability of some future unit without looking too far into the past?	Markov models are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past.	of probabilistic models	2.168145949173318
What contains V word types?	The corpus contains V word types.	the corpus	2.1678215898659365
If Chen use log base 2, what will be measured in bits?	If we use log base 2, the resulting value of entropy will be measured in bits.	the resulting value of entropy	2.1663967324810285
What is if the probability of any word in the test set is 0 the entire probability of the test set?	Second, if the probability of any word in the test set is 0, the entire probability of the test set is 0.	0	2.16454396579796
What approach to continuous speech recognition?	A maximum likelihood approach to continuous speech recognition.	a maximum likelihood	2.164187310135305
What need a training corpus of questions?	To build a language model for a question-answering system, we need a training corpus of questions.	3. 3	2.1623703823895584
What was Computer Speech & Language?	Computer Speech & Language, 21,492518.	21	2.158475472711233
What might 3. 3 have a good estimate of words's probability for?	For any n-gram that occurred a sufficient number of times, we might have a good estimate of its probability.	for any n-gram that occurred a sufficient number of times	2.1580103058837086
What was the lower order n-gram weighed by?	If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed weight.	by a fixed weight	2.15702111048752
What may even simple extensions of the example sentence have counts of on the web?	Even simple extensions of the example sentence may have counts of zero on the web (such as ``Walden Pond's water is so transparent that the''; well, used to have counts of zero).	of zero	2.154910311139748
What is 3. 16 the entire sequence of?	3. 16 is the entire sequence of words in some test set.	of words in some test set	2.1542024053157904
What can such AAC devices compare the performance of by running the speech recognizer twice for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	of two language models	2.154184041626364
What may even simple extensions of the example sentence have counts of zero on?	Even simple extensions of the example sentence may have counts of zero on the web (such as ``Walden Pond's water is so transparent that the''; well, used to have counts of zero).	on the web	2.1537946747253014
What is instead of P in other words?	In other words, instead of P, which answers the question ``How likely is w?''	w	2.1534814751632436
What does the combined Good-Turing backoff algorithm involve?	The combined Good-Turing backoff algorithm involves quite detailed computation for estimating the Good-Turing smoothing and the P and values.	quite detailed computation for estimating the Good-Turing smoothing and the P and values	2.1527059314311456
What is optimized for speed and memory size?	SRILM offers a wider range of options and types of discounting, while KenLM is optimized for speed and memory size, making it possible to build web-scale language models.	KenLM	2.1514963907569995
What is perplexity commonly used as on an algorithm?	Nonetheless, because perplexity often correlates with such improvements, it is commonly used as a quick check on an algorithm.	as a quick check	2.151229273026563
Who uses three different discounts d1, d2, and d3?	Rather than use a single fixed discount d, modified Kneser-Ney uses three different discounts d1, d2, and d3 + for n-grams with counts of 1,2 and three or more, respectively.	modified Kneser-Ney	2.1463793891740552
What is dependent on the training corpus?	The n-gram model, like many statistical models, is dependent on the training corpus.	the n-gram model	2.1449071748784747
What is Speech Recognition?	and Lee, K. - F. , Readings in Speech Recognition, 450506.	450506	2.140151604644514
What did Improved back for?	Improved backing-off for Mgram language modeling.	for Mgram language modeling	2.1354131813246995
What is a better n-gram model perplexity?	A better n-gram model is one that assigns a higher probability to the A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY test data, and perplexity is a normalized version of the probability of the test set.	a normalized version of the probability of the test set	2.1317016546549934
What are a problem for two reasons?	These zeros-- things that do not ever occur in the training set but do occur in the test set-- are a problem for two reasons.	these zeros	2.1305837023632175
What may have counts of zero on the web?	Even simple extensions of the example sentence may have counts of zero on the web (such as ``Walden Pond's water is so transparent that the''; well, used to have counts of zero).	even simple extensions of the example sentence	2.1294311465898454
What come from the corpus with many of the statistical models in such AAC devices's field?	As with many of the statistical models in our field, the probabilities of an n-gram model come from the corpus it is trained on, the training set or training corpus.	the probabilities of an n-gram model	2.1278651816595078
What 'll we need a function to distribute this probability mass to the lower order is also called Katz backoff in addition to this explicit discount factor?	In addition to this explicit discount factor, we 'll need a function to distribute this probability mass to the lower order This kind of backoff with discounting is also called Katz backoff.	This kind of backoff with discounting	2.1273554999886137
What 'll have to give a bit of terminological ambiguity to the events we 've never seen?	To keep a language model from assigning zero probability to these unseen events, we 'll have to shave off a bit of probability mass from some more frequent events and give it to the events we 've never seen.	these zeros-- things that do not ever occur in the training set but do occur in the test set--	2.126330181069508
What also need to adjust the denominator to take into account the extra V observations?	Since there are V words in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra V observations.	these zeros-- things that do not ever occur in the training set but do occur in the test set--	2.125588838440195
What do such AAC devices not use raw probability as our metric for evaluating language models, but a variant called perplexity in?	In practice we do not use raw probability as our metric for evaluating language models, but a variant called perplexity.	in practice	2.124071824885154
What can compare the performance of two language models by seeing which gives the more accurate transcription for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	such AAC devices	2.1239034552495624
What is to be sure to use a training corpus that has a similar genre to whatever task 3. 3 are trying to accomplish?	One step is to be sure to use a training corpus that has a similar genre to whatever task we are trying to accomplish.	one step	2.122596485592017
What would be a vowel or a consonant?	The underlying mathematics of the n-gram was first proposed by Markov, who used what are now called Markov chains (bigrams and trigrams) to predict whether an upcoming letter in Pushkin's Eugene Onegin would be a vowel or a consonant.	an upcoming letter in Pushkin's Eugene Onegin	2.1224636053315886
What are neural network language models?	The highest accuracy language models are neural network language models.	the highest accuracy language models	2.1214122205367594
What does the backoff terminate in?	in referring to it as S: S (w wi1) otherwise The backoff terminates in the unigram, which has probability S = count et al.	in the unigram	2.1163159750901888
What do neural language models instead project into a continuous space in which words with similar contexts have similar representations?	Neural language models instead project words into a continuous space in which words with similar contexts have similar representations.	words	2.1154862430957246
What do 3. 3 need a training corpus of?	To build a language model for translating legal documents, we need a training corpus of legal documents.	of legal documents	2.115229900112253
What does model assign a higher probability to?	The answer is simple: whichever model assigns a higher probability to the test set-- meaning it more accurately predicts the test set-- is a better model.	to the test set	2.115137128183175
Who was P.?	In Oostdijk, N. and de Haan, P. , Corpus-Based Research into Language, 189198.	N. and de Haan	2.113169169830442
What can such AAC devices compare the performance of two language models by running the speech recognizer twice for?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	for speech recognition	2.111851289533673
What is the entropy of the choice of horses then = log = 3 bits N- GRAM L ANGUAGE M ODELS?	The entropy of the choice of horses is then = log = 3 bits N- GRAM L ANGUAGE M ODELS Until now we have been computing the entropy of a single variable.	Until now Chen have been computing the entropy of a single variable	2.108054618779103
How many pseudo-words can such AAC devices use for the first trigram We always represent and compute language model probabilities in log format as log probabilities for example?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.	two pseudo-words	2.1077512452317544
What want our test set to be as large as possible?	We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible.	such AAC devices	2.1072535867685174
What does add-k smoothing require?	Add-k smoothing requires that we have a method for choosing k; this can be done, for example, by optimizing on a devset.	that these zeros-- things that do not ever occur in the training set but do occur in the test set-- have a method for choosing k	2.1068188014459466
What will Chen be computing the entropy of w0 w2, . . . for a grammar for example?	For a grammar, for example, we will be computing the entropy of some sequence of words W = w0, w1, w2, . . . , wn.	of some sequence of words W =	2.10576144461883
What was Bell System Technical Journal?	Bell System Technical Journal, 27,379423.	27	2.1055836807363044
What is an intrinsic evaluation metric?	An intrinsic evaluation metric is one that measures the quality of a model independent of any application.	one that measures the quality of a model independent of any application	2.1026120677030105
Did the resulting parameter set maximizes the likelihood of the training set T given the model M in MLE?	In MLE, the resulting parameter set maximizes the likelihood of the training set T given the model M (i. e. , P (T M) ).		2.101848044498356
What would such AAC devices have to get by?	We would have to get the count of its water is so transparent and divide by the sum of the counts of all possible five word sequences.	the count of an entire sentence's water is so transparent and divide by the sum of the counts of all possible five word sequences	2.097182480244131
What can such AAC devices compare the performance of by seeing which gives the more accurate transcription for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	of two language models	2.096829844285885
What do such AAC devices call in such cases?	In such cases, we call the initial test set the development test set or, devset.	the initial test set the development test set or, devset	2.0960070845149588
Who 'll use data from the now-defunct Berkeley Restaurant Project?	We 'll use data from the now-defunct Berkeley Restaurant Project, a dialogue system from the last century that answered questions about a database of restaurants in Berkeley, California (Jurafsky et al. , 1994).	we	2.09335182756974
What was weighed by a fixed weight?	If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed weight.	the lower order n-gram	2.0929592103910664
What is the n-gram model dependent on?	The n-gram model, like many statistical models, is dependent on the training corpus.	on the training corpus	2.092875104090233
What do Let's say?	Let's say that 0 occur 91 times in the training set, and each of the other digits occurred 1 time each.	that 0 occur 91 times in the training set, and each of the other digits occurred 1 time each	2.09163140253602
What 'll have to shave off a bit of probability mass from some more frequent events?	To keep a language model from assigning zero probability to these unseen events, we 'll have to shave off a bit of probability mass from some more frequent events and give it to the events we 've never seen.	these zeros-- things that do not ever occur in the training set but do occur in the test set--	2.0910963663726423
What does the unigram have?	in referring to it as S: S (w wi1) otherwise The backoff terminates in the unigram, which has probability S = count et al.	probability S = count et al	2.0877429869537583
What show in Chapter 12?	But natural language is not stationary, since as we show in Chapter 12, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent.	Chen	2.086978153109063
What is one intuitive way to think about entropy as a lower bound on the number of bits?	One intuitive way to think about entropy is as a lower bound on the number of bits it would take to encode a certain decision or piece of information in the optimal Consider an example from the standard information theory textbook Cover and Thomas.	it would take to encode a certain decision or piece of information in the optimal Consider an example from the standard information theory textbook Cover and Thomas	2.0833590600417997
What was output then hand-corrected for to improve readability?	Output was then hand-corrected for capitalization to improve readability.	for capitalization	2.0808501924515714
Who is the EM algorithm?	There are various ways to find this optimal set of s. One way is to use the EM algorithm, an iterative learning algorithm that converges on locally optimal s (Jelinek and Mercer, 1980).	an iterative	2.0804479653749137
What (can never be lower than the true entropy? )	(The cross-entropy can never be lower than the true entropy, so a model cannot err by underestimating the true entropy. )	The cross-entropy	2.079196513318593
What bigrams?	A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams.	a comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English	2.0784031734311483
What does p answer ``How likely''?	In other words, instead of P, which answers the question ``How likely is w?''	the question	2.0782412591528905
What were punctuation marks treated as?	All characters were mapped to lower-case and punctuation marks were treated as words.	as words	2.077044043440241
How many possible bigrams is 844?	There are V 2 = 844,000,000 possible bigrams alone, and the number of possible 4-grams is V 4 = 7 1017.	000 possible bigrams	2.0730122059797553
What did such AAC devices then compute the perplexity of on a test set of 1. 5 million words with Eq?	We then computed the perplexity of each of these models on a test set of 1. 5 million words with Eq.	of each of these models	2.0712238994948433
What is equally important to get training data in the appropriate dialect?	It is equally important to get training data in the appropriate dialect, especially when processing social media posts or spoken transcripts.	a very large corpus	2.0602835276964244
What were treated as words?	All characters were mapped to lower-case and punctuation marks were treated as words.	punctuation marks	2.0600978300940826
What is called the OOV rate?	The percentage of OOV words that appear in the test set is called the OOV rate.	the percentage of OOV words that appear in the test set	2.0580993504547576
What can such AAC devices compare the performance of two language models by seeing which gives the more accurate transcription for?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	for speech recognition	2.0576501117284343
What will be computing the entropy of some sequence of words W = w0 w2, . . . for a grammar for example?	For a grammar, for example, we will be computing the entropy of some sequence of words W = w0, w1, w2, . . . , wn.	Chen	2.0573558236193077
Who was R.?	Originally distributed as IBM technical report in Jelinek, F. and Mercer, R. .	Mercer	2.0532030711120606
What do such AAC devices combine by adding them?	E VALUATING L ANGUAGE M ODELS Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them.	log probabilities	2.053126789378709
What n-grams computed from Shakespeare's works?	A Figure 3. 3 Eight sentences randomly generated from four n-grams computed from Shakespeare's works.	a Figure 3. 3 Eight sentences randomly generated from four	2.0517716161498236
If Chen use log base 2, what will the resulting value of entropy be measured in?	If we use log base 2, the resulting value of entropy will be measured in bits.	in bits	2.0500995424361674
What metric is based on test set probability?	Since our evaluation metric is based on test set probability, it's important not to let the test sentences into the training set.	such AAC devices's evaluation	2.0483898628247585
What was Information Theory?	IEEE Transactions on Information Theory, 37,10851094.	37	2.0482743738002984
What does model assign to the test set?	The answer is simple: whichever model assigns a higher probability to the test set-- meaning it more accurately predicts the test set-- is a better model.	a higher probability	2.0471443644998457
What occurs following every long string!	As we said above, we cannot just estimate by counting the number of times every word occurs following every long string, because language is creative and any particular context might have never occurred before!	every word	2.045686973117263
Who is Witten?	In NAACL-HLT 07,173 Witten, I. H. C. .	I. H. C.	2.044848148513819
What might sentences and sequences of words, the n-gram expect overlap between our n-grams for the two genres?	Shakespeare and the Wall Street Journal are both English, so we might expect some overlap between our n-grams for the two genres.	some	2.0444538791882745
What is the number of possible 4-grams?	There are V 2 = 844,000,000 possible bigrams alone, and the number of possible 4-grams is V 4 = 7 1017.	V 4 = 7 1017	2.041979061478953
What is the MLE of its probability or?	The MLE of its probability is 1000000 or.	1000000	2.0415874279139583
What is like many statistical models?	The n-gram model, like many statistical models, is dependent on the training corpus.	the n-gram model	2.0393622820539465
What 'll we see to?	We 'll see how to use n-gram models to estimate the probability of the last word of an n-gram given the previous words, and also to assign probabilities to entire sequences.	how to use n-gram models to estimate the probability of the last word of an n-gram given the previous words, and also to assign probabilities to entire sequences	2.0380075177800636
What is a measure of information?	Entropy is a measure of information.	Entropy	2.0378588700437343
What is instead of P w in?	In other words, instead of P, which answers the question ``How likely is w?''	in other words	2.0349151051690506
'll such AAC devices use data from the now-defunct Berkeley Restaurant Project?	We 'll use data from the now-defunct Berkeley Restaurant Project, a dialogue system from the last century that answered questions about a database of restaurants in Berkeley, California (Jurafsky et al. , 1994).		2.03198523743621
What are n-grams stored in?	Probabilities are generally quantized using only 4-8 bits (instead of 8-byte floats), and n-grams are stored in reverse tries.	in reverse tries	2.0286451485103325
What do smoothing algorithms provide?	Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams.	a more sophisticated way to estimate the probability of n-grams	2.027116104063339
What combine log probabilities by adding them?	E VALUATING L ANGUAGE M ODELS Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them.	such AAC devices	2.0251368305154913
What is the test set normalized by?	The perplexity (sometimes called PP for short) of a language model on a test set is the inverse probability of the test set, normalized by the number of words.	by the number of words	2.0224718234350614
What cannot err by underestimating the true entropy?	(The cross-entropy can never be lower than the true entropy, so a model cannot err by underestimating the true entropy. )	a model	2.0200375320233057
Does PCONTINUATION answer the question ``How likely is w to appear as a novel continuation?''?	, we 'd like to create a unigram model that we might call PCONTINUATION, which answers the question ``How likely is w to appear as a novel continuation?''		2.0177777645767185
What are generally quantized using only 4-8 bits?	Probabilities are generally quantized using only 4-8 bits (instead of 8-byte floats), and n-grams are stored in reverse tries.	probabilities	2.016233308511625
What is to add one to all the bigram counts?	The simplest way to do smoothing is to add one to all the bigram counts, before we normalize them into probabilities.	the simplest way to do smoothing	2.0147681879426784
What will Chen be computing the entropy of some sequence of words W = w0 w2, . . . for for example?	For a grammar, for example, we will be computing the entropy of some sequence of words W = w0, w1, w2, . . . , wn.	for a grammar	2.012780030651608
How many bits are probabilities generally quantized using?	Probabilities are generally quantized using only 4-8 bits (instead of 8-byte floats), and n-grams are stored in reverse tries.	only 4-8 bits	2.011883405415088
What will Chen be computing the entropy of some sequence of words W = w0 w2, . . . for a grammar for?	For a grammar, for example, we will be computing the entropy of some sequence of words W = w0, w1, w2, . . . , wn.	for example	2.0106611912316343
What overlap in generated sentences?	While they both model ``English-like sentences'', there is clearly no overlap in generated sentences, and little overlap even in small phrases.	no	2.0095047533043453
What did Chen show that caches and class-based models provided?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.	only minor additional improvement	2.007809777735911
What do a maximum likelihood approach to?	A maximum likelihood approach to continuous speech recognition.	to continuous speech recognition	2.003838808103987
What do Chen show in?	But natural language is not stationary, since as we show in Chapter 12, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent.	in Chapter 12	2.0033883360978484
What should always be confirmed by an end-to-end evaluation of a real task before concluding the evaluation of the model?	But a model's improvement in perplexity should always be confirmed by an end-to-end evaluation of a real task before concluding the evaluation of the model.	a model's improvement in perplexity	2.0021968434093824
What is defined in the limit?	Cross-entropy is defined in the limit, as the length of the observed word sequence goes to infinity.	cross-entropy	1.9984862142785997
What use the trigram if the evidence is sufficient?	In backoff, we use the trigram if the evidence is sufficient, otherwise we use the bigram, otherwise the unigram.	these zeros-- things that do not ever occur in the training set but do occur in the test set--	1.9960685430245937
What is the interpolated Kneser-Ney algorithm?	One of the most commonly used and best performing n-gram smoothing methods is the interpolated Kneser-Ney algorithm (Kneser and Ney 1995, Chen and Goodman 1998).	one of the most commonly used and best performing n-gram smoothing methods	1.9944950228122955
Who was T.?	Izvistia Imperatorskoi Akademii Nauk (Bulletin de l'Academie Imperiale des Sciences de Mikolov, T. .	(Bulletin de l'Academie Imperiale des Sciences de Mikolov	1.9922682683268638
What might sentences and sequences of words, the n-gram expect some overlap between?	Shakespeare and the Wall Street Journal are both English, so we might expect some overlap between our n-grams for the two genres.	between our n-grams for the two genres	1.9912735595392397
What does PCONTINUATION answer the question as?	, we 'd like to create a unigram model that we might call PCONTINUATION, which answers the question ``How likely is w to appear as a novel continuation?''	``How likely is w to appear as a novel continuation?''	1.9911663546460854
What usually drop the word ``model''?	In a bit of terminological ambiguity, we usually drop the word ``model'', and thus the term ngram is used to mean either the word sequence itself or the predictive model that assigns it a probability.	such AAC devices	1.9863477863969285
Did Izvistia Imperatorskoi Akademii nauk (Bulletin de l'Academie Imperiale des Sciences de Mikolov?	Izvistia Imperatorskoi Akademii Nauk (Bulletin de l'Academie Imperiale des Sciences de Mikolov, T. .		1.9860514004538499
What hear both?	3. 3 shows random sentences generated from unigram, bigram, trigram, and 4-gram To him swallowed confess hear both.	3. 3 shows random sentences generated from unigram, bigram, trigram, and 4-gram To devset swallowed confess	1.9826802269352302
What are Markov models?	Markov models are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past.	the class of probabilistic models that assume such AAC devices can predict the probability of some future unit without looking too far into the past	1.977239108169002
What were all characters mapped to?	All characters were mapped to lower-case and punctuation marks were treated as words.	to lower-case	1.976252582587639
What are the highest accuracy language models?	The highest accuracy language models are neural network language models.	neural network language models	1.9761156345247446
What do such AAC devices call?	We call this situation training on the test set.	this situation training on the test set	1.9748917281958118
What continue choosing random numbers?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.	such AAC devices	1.9742814803532813
What is cross-entropy defined in?	Cross-entropy is defined in the limit, as the length of the observed word sequence goes to infinity.	in the limit	1.9718461031843015
What do the bigram sentences have?	The bigram sentences have some local word-to-word coherence (especially if we consider that punctuation counts as a word).	some local word-to-word coherence	1.9713674015327458
What is w in other words?	In other words, instead of P, which answers the question ``How likely is w?''	instead of P	1.9700841903598587
What goes to infinity?	Cross-entropy is defined in the limit, as the length of the observed word sequence goes to infinity.	the length of the observed word sequence	1.9658920032991773
Is a better n-gram model one that assigns a higher probability to the A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY test data?	A better n-gram model is one that assigns a higher probability to the A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY test data, and perplexity is a normalized version of the probability of the test set.		1.9611761341254081
What is each word covering an interval proportional to log space's frequency?	Imagine all the words of the English language covering the probability space between 0 and 1, each word covering an interval proportional to its frequency.	between 0 and 1	1.9595424383435704
What get numbers that are not as small?	By using log probabilities instead of raw probabilities, we get numbers that are not as small.	such AAC devices	1.9547779551183015
What do these solve?	These solve a major problem with n-gram language models: the number of parameters increases exponentially as the n-gram order increases, and n-grams have no way to generalize from training to test set.	a major problem with n-gram language models: the number of parameters increases exponentially as the n-gram order increases	1.9523692497193097
What will the sorts of words that might occur hurt the performance of?	First, their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data.	of any application these zeros-- things that do not ever occur in the training set but do occur in the test set-- want to run on this data	1.9522400727357778
What 'll we first need to augment each sentence with?	We 'll first need to augment each sentence with a special symbol <s> at the beginning of the sentence, to give us the bigram context of the first word.	with a special symbol <s> at the beginning of the sentence	1.9487673813595383
When are new sentences created?	This is because language is creative; new sentences are created all the time, and we will not always be able to count entire sentences.	all the time	1.948237494248825
Has Modified Interpolated KneserNey since become the standard baseline for language modeling?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.		1.9474985671046183
What 'll we need a function to distribute this probability mass to the lower order This kind of backoff with discounting is also called Katz backoff in?	In addition to this explicit discount factor, we 'll need a function to distribute this probability mass to the lower order This kind of backoff with discounting is also called Katz backoff.	in addition to this explicit discount factor	1.9469588839458742
What is the branching factor of a language?	The branching factor of a language is the number of possible next words that can follow any word.	the number of possible next words that can follow any word	1.9460879778911606
What did convert in the training set to the unknown word token <UNK> in a text normalization step?	Convert in the training set any word that is not in this set (any OOV word) to the unknown word token <UNK> in a text normalization step.	any word that is not in this set	1.9454289560179399
What is the entropy of the choice of horses then = log = 3 bits N- Until now Chen have been computing the entropy of a single variable?	The entropy of the choice of horses is then = log = 3 bits N- GRAM L ANGUAGE M ODELS Until now we have been computing the entropy of a single variable.	GRAM L ANGUAGE M ODELS	1.9377753576584291
What does not produce a probability distribution?	This algorithm does not produce a probability distribution, so we 'll follow Brants et al.	this algorithm	1.937613124872949
What proceed to train the language model as before in either case?	In either case we then proceed to train the language model as before, treating <UNK> like a regular word.	these zeros-- things that do not ever occur in the training set but do occur in the test set--	1.9362450878680215
What is Lee?	and Lee, K. - F. , Readings in Speech Recognition, 450506.	K. - F.	1.9332455078260844
What are bound to be missing from a very large corpus?	But because any corpus is limited, some perfectly acceptable English word sequences are bound to be missing from it.	some perfectly acceptable English word sequences	1.9325237400759578
Is another option to build approximate language models using techniques like Bloom filters (Talbot and Osborne 2007?	Another option is to build approximate language models using techniques like Bloom filters (Talbot and Osborne 2007, Church et al.		1.9316391997444895
What is a held-out corpus like?	A held-out corpus is an additional training corpus that we use to set hyperparameters like these values, by choosing the values that maximize the likelihood of the held-out corpus.	an additional training corpus that these zeros-- things that do not ever occur in the training set but do occur in the test set-- use to set hyperparameters like these values	1.9314898232595763
What is called stupid backoff?	The algorithm is called stupid backoff.	the algorithm	1.931080961203888
What was Mercer?	Originally distributed as IBM technical report in Jelinek, F. and Mercer, R. .	R.	1.9308911080819724
What is the probability distribution for words at time t the same as the probability distribution at time t + 1 in?	In other words, the probability distribution for words at time t is the same as the probability distribution at time t + 1.	in other words	1.9299627845379892
What will the sorts of words that might occur hurt?	First, their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data.	the performance of any application these zeros-- things that do not ever occur in the training set but do occur in the test set-- want to run on this data	1.9293340934073766
What is the n-gram model like?	The n-gram model, like many statistical models, is dependent on the training corpus.	like many statistical models	1.9276876361304087
What can such AAC devices then measure the quality of an n-gram model by its performance on?	We can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus.	on some unseen data called the test set or test corpus	1.9266578203817086
What is the result?	The result is the smoothed bigram probabilities in Fig.	the smoothed bigram probabilities in Fig	1.9264033416558843
What can (The cross-entropy never be lower than? )	(The cross-entropy can never be lower than the true entropy, so a model cannot err by underestimating the true entropy. )	than the true entropy	1.925737028545848
What are called language models or LMs?	Models that assign probabilities to sequences of words are called language models or LMs.	models that assign probabilities to sequences of words	1.9169487555942482
What does kneser-Ney have?	Kneser-Ney has its roots in a method called absolute discounting.	the now-defunct Berkeley Restaurant Project's roots in a method called absolute discounting	1.9148221442474207
Was a trigram model used in the IBM TANGORA speech recognition system in the 1970s?	A trigram model was used in the IBM TANGORA speech recognition system in the 1970s, but the idea was not written up until later.		1.9114605467207557
Does efficient language model toolkits like KenLM (Heafield 2011?	Finally, efficient language model toolkits like KenLM (Heafield 2011, Heafield et al.		1.909277483568562
Did Markov classify 20,000 letters as V or C?	Markov classified 20,000 letters as V or C and computed the bigram and trigram probability that a given letter would be a vowel given the previous one or two letters.		1.9089762781914263
What is because language is creative?	This is because language is creative; new sentences are created all the time, and we will not always be able to count entire sentences.	this	1.9087253417950532
Who is ambiguous input?	Probabilities are essential in any task in which we have to identify words in noisy, ambiguous input, like speech recognition.	noisy	1.90608643172401
What do such AAC devices choose?	We choose a random value between 0 and 1 and print the word whose interval includes this chosen value.	a random value between 0 and 1	1.9026042098594083
'll we first need to augment each sentence with a special symbol <s> at the beginning of the sentence?	We 'll first need to augment each sentence with a special symbol <s> at the beginning of the sentence, to give us the bigram context of the first word.		1.9025258477695406
What do Chen see?	Now we see the following test set: 0 0 0 0 0 3 0 0 0 0.	the following test set: 0 0 0 0 0 3 0 0 0 0	1.9020700435201525
What is very predictable?	We should expect the perplexity of this test set to be lower since most of the time the next number will be zero, which is very predictable, i. e. Thus, although the branching factor is still 10, the perplexity or weighted branching factor is smaller.	the perplexity of this test set to be lower since most of the time the next number will be zero i. e.	1.9013142400596248
When did these arguments lead many linguists and computational linguists to ignore work in statistical modeling?	These arguments led many linguists and computational linguists to ignore work in statistical modeling for decades.	for decades	1.9003757299943604
Who is 450506?	and Lee, K. - F. , Readings in Speech Recognition, 450506.	Speech Recognition	1.9003669745573868
What just divide our data into 80 % training, 10 % development, and 10 % test in practice?	In practice, we often just divide our data into 80 % training, 10 % development, and 10 % test.	such AAC devices	1.899721932259165
What do such AAC devices continue choosing?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.	random numbers	1.8945061286446698
What does the table below show the perplexity of?	The table below shows the perplexity of a 1. 5 million word WSJ test set according to each of these grammars.	of a 1. 5 million word WSJ test set according to each of these grammars	1.8943949953571246
What is an iterative?	There are various ways to find this optimal set of s. One way is to use the EM algorithm, an iterative learning algorithm that converges on locally optimal s (Jelinek and Mercer, 1980).	the EM algorithm	1.8940802803697754
What will also serve to assign a probability to an entire sentence?	The same models will also serve to assign a probability to an entire sentence.	the same models	1.893930833132385
What is the probability-based metric?	Training on the test set introduces a bias that makes the probabilities all look too high, and causes huge inaccuracies in perplexity, the probability-based metric we introduce below.	perplexity	1.891289826910601
What do such AAC devices just divide our data into in practice?	In practice, we often just divide our data into 80 % training, 10 % development, and 10 % test.	into 80 % training, 10 % development, and 10 % test	1.8903818938831374
What are ready to introduce cross-entropy?	Now we are ready to introduce cross-entropy.	Chen	1.8888086777596498
What is such AAC devices's evaluation metric based on?	Since our evaluation metric is based on test set probability, it's important not to let the test sentences into the training set.	on test set probability	1.8873968824592087
What is 000 possible bigrams?	There are V 2 = 844,000,000 possible bigrams alone, and the number of possible 4-grams is V 4 = 7 1017.	844	1.885219990972981
Who will Chen need an approximation to?	We will need an approximation to crossentropy, relying on a (sufficiently long) sequence of fixed length.	to crossentropy	1.8843590343161516
What is called extrinsic evaluation?	Such end-to-end evaluation is called extrinsic evaluation.	such end-to-end evaluation	1.8832075612849077
What has a much wider distribution?	The word glasses has a much wider distribution.	the word glasses	1.880269394914708
What add a fractional count k (. 05?	Instead of adding 1 to each count, we add a fractional count k (. 05?	these zeros-- things that do not ever occur in the training set but do occur in the test set--	1.877996994048202
What can see how much a smoothing algorithm has changed the original counts?	It is often convenient to reconstruct the count matrix so we can see how much a smoothing algorithm has changed the original counts.	these zeros-- things that do not ever occur in the training set but do occur in the test set--	1.8776787260371721
What did the previous section discuss the problem of?	The previous section discussed the problem of words whose bigram probability is zero.	of words whose bigram probability is zero	1.8741278524920197
What does every word occur following!	As we said above, we cannot just estimate by counting the number of times every word occurs following every long string, because language is creative and any particular context might have never occurred before!	every long string	1.8711659654494728
What did Markov use?	The underlying mathematics of the n-gram was first proposed by Markov, who used what are now called Markov chains (bigrams and trigrams) to predict whether an upcoming letter in Pushkin's Eugene Onegin would be a vowel or a consonant.	what are now called Markov chains to predict whether an upcoming letter in Pushkin's Eugene Onegin would be a vowel or a consonant	1.8651635234233812
What does Equation 3. 4 suggest that we could estimate by multiplying together a number of conditional probabilities?	Equation 3. 4 suggests that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities.	the joint probability of an entire sequence of words	1.8649796826309264
What is 3. 16?	3. 16 is the entire sequence of words in some test set.	the entire sequence of words in some test set	1.8622088300388357
What will need an approximation to crossentropy?	We will need an approximation to crossentropy, relying on a (sufficiently long) sequence of fixed length.	Chen	1.860220001854589
Did iEEE Transactions on Information Theory 10851094?	IEEE Transactions on Information Theory, 37,10851094.		1.8580751312454358
What overlap even in small phrases?	While they both model ``English-like sentences'', there is clearly no overlap in generated sentences, and little overlap even in small phrases.	little	1.8561788620807897
What 532557?	Proceedings of the IEEE, 64,532557.	Proceedings of the IEEE	1.8534116120310367
What do let's look at?	To get an idea of the dependence of a grammar on its training set, let's look at an n-gram grammar trained on a completely different corpus: the Wall Street Journal newspaper.	at an n-gram grammar trained on a completely different corpus: the Wall Street Journal newspaper	1.847434833566385
What is used to mean either the word sequence itself or the predictive model that assigns it a probability?	In a bit of terminological ambiguity, we usually drop the word ``model'', and thus the term ngram is used to mean either the word sequence itself or the predictive model that assigns it a probability.	the term ngram	1.8447877232078014
What is output hand-corrected The longer on?	Output is hand-corrected The longer the context on which we train the model, the more coherent the sentences.	the context on which such AAC devices train the model	1.844739543959757
What do such AAC devices just divide our data into 80 % training, 10 % development, and 10 % test in?	In practice, we often just divide our data into 80 % training, 10 % development, and 10 % test.	in practice	1.8434468861894802
What are less than or equal to 1?	Since probabilities are (by definition) less than or equal to 1, the more probabilities we multiply together, the smaller the product becomes.	probabilities	1.8429660753717183
'll Chen introduce both feedforward language models (Bengio et al?	We 'll introduce both feedforward language models (Bengio et al.		1.8410972761729907
What will Chen need to crossentropy?	We will need an approximation to crossentropy, relying on a (sufficiently long) sequence of fixed length.	an approximation	1.840570754090963
What is then = log = 3 bits N- GRAM L ANGUAGE M ODELS Until now we have been computing the entropy of a single variable?	The entropy of the choice of horses is then = log = 3 bits N- GRAM L ANGUAGE M ODELS Until now we have been computing the entropy of a single variable.	the entropy of the choice of horses	1.8369043131514577
What 10851094?	IEEE Transactions on Information Theory, 37,10851094.	iEEE Transactions on Information Theory	1.8325709523268534
What is therefore called add-k smoothing?	This algorithm is therefore called add-k smoothing.	this algorithm	1.8256204426103695
What will the same models also serve to assign to an entire sentence?	The same models will also serve to assign a probability to an entire sentence.	a probability	1.8202000425638996
Did Proceedings of the IEEE 532557?	Proceedings of the IEEE, 64,532557.		1.820157248929831
What is too much probability mass moved to?	The sharp change in counts and probabilities occurs because too much probability mass is moved to all the zeros.	to all the zeros	1.819953914341524
What do such AAC devices also want as much training as possible?	We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible.	data	1.8194957573353827
What do we continue backing off until we reach a In?	We continue backing off until we reach a In order for a backoff model to give a correct probability distribution, we have to discount the higher-order n-grams to save some probability mass for the lower order n-grams.	In order for a backoff model to give a correct probability distribution, we have to discount the higher-order n-grams to save some probability mass for the lower order n-grams	1.8194691525668696
What also want as much training data as possible?	We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible.	such AAC devices	1.8151362280229357
Did Chen and Goodman start in the late 1990s?	Starting in the late 1990s, Chen and Goodman produced a highly influential series of papers with a comparison of different language models (Chen and Goodman 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).		1.8148483255049679
What do no overlap in?	While they both model ``English-like sentences'', there is clearly no overlap in generated sentences, and little overlap even in small phrases.	in generated sentences	1.8122599598528943
How many probabilities are one?	Give two probabilities, one using Fig.	two probabilities	1.8074144644695018
Will a standard unigram model assign Kong a higher probability than glasses?	A standard unigram model will assign Kong a higher probability than glasses.		1.8063251348114604
Was CMU influenced by the work of Baum and colleagues?	The resurgence of n-gram models came from Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and Baker at CMU, who was influenced by the work of Baum and colleagues.		1.8056309649682225
What are the n-gram?	In this chapter we introduce the simplest model that assigns probabilities to sentences and sequences of words, the n-gram.	words	1.805404702109262
Does a comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams?	A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams.		1.8037162837474203
Were Markov models based on Shannon's work?	Based on Shannon's work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s.		1.803134027981533
Is extrinsic evaluation the only way to know if a particular improvement in a component is really going to help the task at hand?	Extrinsic evaluation is the only way to know if a particular improvement in a component is really going to help the task at hand.		1.802784568312655
What is such end-to-end evaluation called?	Such end-to-end evaluation is called extrinsic evaluation.	extrinsic evaluation	1.8019716700470045
What are created all the time?	This is because language is creative; new sentences are created all the time, and we will not always be able to count entire sentences.	new sentences	1.8007797299004447
What was 64?	Proceedings of the IEEE, 64,532557.	Proceedings of the IEEE	1.8005675959596017
What is Entropy?	Entropy is a measure of information.	a measure of information	1.7995339253709803
What will the more accurate model be the one with between two models m1 and m2?	Between two models m1 and m2, the more accurate model will be the one with the lower cross-entropy.	with the lower cross-entropy	1.7986445851389212
What are preceding words?	We do not know any way to compute the exact probability of a word given a long sequence of preceding words, 1).	1	1.7980909966663807
Is the intuition of the Shannon-McMillan-Breiman theorem that a long-enough sequence of words will contain in it many other shorter sequences?	The intuition of the Shannon-McMillan-Breiman theorem is that a long-enough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence according to their probabilities.		1.7923417782915578
What do 3. 3 need?	To build a language model for translating legal documents, we need a training corpus of legal documents.	a training corpus of legal documents	1.787161067552547
What can be estimated by normalizing?	n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).	n-gram probabilities	1.7852762619442568
Does kneser-Ney have the now-defunct Berkeley Restaurant Project's roots in a method called absolute discounting?	Kneser-Ney has its roots in a method called absolute discounting.		1.7829034097677994
Who did these arguments lead to ignore work in statistical modeling for decades?	These arguments led many linguists and computational linguists to ignore work in statistical modeling for decades.	many linguists and computational linguists	1.78112374237833
What shows the reconstructed counts?	Figure 3. 7 shows the reconstructed counts.	figure 3. 7	1.7785377177883066
What did these arguments lead many linguists and computational linguists to ignore in statistical modeling for decades?	These arguments led many linguists and computational linguists to ignore work in statistical modeling for decades.	work	1.7773248125385686
Who are these papers recommended for with further Two commonly used toolkits for building language models are SRILM and KenLM (Heafield 2011?	These papers are recommended for any reader with further Two commonly used toolkits for building language models are SRILM (Stolcke, 2002) and KenLM (Heafield 2011, Heafield et al.	for any reader	1.776763392331128
What is the m?	It A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allows us to use some m, which is a model of p (i. e. , an approximation to p).	a model of p	1.7756133766219118
What does Equation 3. 28 show?	Equation 3. 28 shows the equation for interpolation with context-conditioned weights: P (wn wn2 wn1) = 1 (wn1 How are these values set?	the equation for interpolation with context-conditioned weights: P = 1 (wn1 How are these values set	1.775549562898717
Who 'll introduce both feedforward language models (Bengio et al?	We 'll introduce both feedforward language models (Bengio et al.	we	1.7724479853797206
'll such AAC devices introduce more sophisticated language models like the RNN LMs of Chapter 9 in later chapters?	In later chapters we 'll introduce more sophisticated language models like the RNN LMs of Chapter 9.		1.770186321473898
What will the same models also serve to assign a probability to?	The same models will also serve to assign a probability to an entire sentence.	to an entire sentence	1.7667783732775186
Who 'll introduce more sophisticated language models like the RNN LMs of Chapter 9 in later chapters?	In later chapters we 'll introduce more sophisticated language models like the RNN LMs of Chapter 9.	we	1.7642819544298587
Can a language model achieve low perplexity by assigning the unknown word a high probability?	A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability.		1.7639649745653467
What does cross-entropy explain?	The perplexity measure actually arises from the information-theoretic concept of cross-entropy, which explains otherwise mysterious properties of perplexity (why the inverse probability, for example? )	otherwise mysterious properties of perplexity (why the inverse probability,	1.7630287859607634
Who formalizes this intuition by subtracting a fixed discount d from each count?	Absolute discounting formalizes this intuition by subtracting a fixed discount d from each count.	absolute discounting	1.7624339137313743
What was Language?	In Oostdijk, N. and de Haan, P. , Corpus-Based Research into Language, 189198.	189198	1.7610751396002136
What does the corpus contain?	The corpus contains V word types.	V word types	1.7609702823045446
Does Lee readings in Speech Recognition?	and Lee, K. - F. , Readings in Speech Recognition, 450506.		1.7596928480375373
When do such AAC devices randomly generate?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.	the sentence-final token </s>	1.7595584537879922
Who showed the advantages of Modified Interpolated KneserNey?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.	they	1.7548581536540664
What is the entropy of the choice of horses then Until now Chen have been computing the entropy of a single variable?	The entropy of the choice of horses is then = log = 3 bits N- GRAM L ANGUAGE M ODELS Until now we have been computing the entropy of a single variable.	= log = 3 bits N- GRAM L ANGUAGE M ODELS	1.7543911540517032
How many times did a bigram that occurred 4 times in the first 22 million words occur in the next 22 million words on average?	On average, a bigram that occurred 4 times in the first 22 million words occurred 3. 23 times in the next 22 million words.	3. 23 times	1.7543435795281237
What will cross many sentence boundaries?	Since this sequence will cross many sentence boundaries, we need to include the begin- and end-sentence markers <s> and </s> in the probability computation.	this sequence	1.7504568574185535
What hypothesize that words that have appeared in more contexts in the past are more likely to appear in some new context as well?	We hypothesize that words that have appeared in more contexts in the past are more likely to appear in some new context as well.	these zeros-- things that do not ever occur in the training set but do occur in the test set--	1.7478084605318647
What is called a relative frequency?	This ratio is called a relative frequency.	this ratio	1.7471405160739022
What did the previous section discuss?	The previous section discussed the problem of words whose bigram probability is zero.	the problem of words whose bigram probability is zero	1.7471000121062117
What did these arguments lead many linguists and computational linguists to ignore work in for decades?	These arguments led many linguists and computational linguists to ignore work in statistical modeling for decades.	in statistical modeling	1.7448145316570187
What may the astute reader have noticed for!	The astute reader may have noticed that except for the held-out counts for 0 and 1, all the other bigram counts in the held-out set could be estimated pretty well by just subtracting 0. 75 from the count in the training set!	that except for the held-out counts for 0 and 1, all the other bigram counts in the held-out set could be estimated pretty well by just subtracting 0. 75 from the count in the training set	1.7431428112778113
Thus if some words have zero probability, what cannot compute perplexity at all!	Thus if some words have zero probability, we cannot compute perplexity at all, since we cannot divide by 0!	these zeros-- things that do not ever occur in the training set but do occur in the test set--	1.7412172779193253
What has chosen the first 4-gram?	Thus, once the generator has chosen the first 4-gram (It cannot be but), there are only five possible continuations (that, I, he, thou, and so) ; indeed, for many 4-grams, there is only one continuation.	the generator	1.7408211679465528
What will be the one with the lower cross-entropy between two models m1 and m2?	Between two models m1 and m2, the more accurate model will be the one with the lower cross-entropy.	the more accurate model	1.7407014443695357
What was 189198?	In Oostdijk, N. and de Haan, P. , Corpus-Based Research into Language, 189198.	Language	1.7403227171253666
What is the unigram?	In backoff, we use the trigram if the evidence is sufficient, otherwise we use the bigram, otherwise the unigram.	the bigram	1.7355773030267025
Did Chen and Goodman produce a highly influential series of papers with a comparison of different language models?	Starting in the late 1990s, Chen and Goodman produced a highly influential series of papers with a comparison of different language models (Chen and Goodman 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).		1.7352470306052297
What know all the words that can occur?	Sometimes we have a language task in which this cannot happen because we know all the words that can occur.	these zeros-- things that do not ever occur in the training set but do occur in the test set--	1.7334556093788624
Are n-grams Markov models that estimate words from a fixed window of previous words?	n-grams are Markov models that estimate words from a fixed window of previous words.		1.7303094596168433
What do such AAC devices present?	We present ways to modify the MLE estimates slightly to get better probability estimates in Section 3. 4.	ways to modify the MLE estimates slightly to get better probability estimates in Section 3. 4	1.729054289271284
Would an upcoming letter in Pushkin's Eugene Onegin be a vowel or a consonant?	The underlying mathematics of the n-gram was first proposed by Markov, who used what are now called Markov chains (bigrams and trigrams) to predict whether an upcoming letter in Pushkin's Eugene Onegin would be a vowel or a consonant.		1.727892700474995
Does add-one smoothing derive from Laplace's 1812 law of succession?	Add-one smoothing derives from Laplace's 1812 law of succession and was first applied as an engineering solution to the zero-frequency problem by Jeffreys based on an earlier Add-K suggestion by Johnson.		1.7276273235606712
Is the percentage of OOV words that appear in the test set called the OOV rate?	The percentage of OOV words that appear in the test set is called the OOV rate.		1.7256139043402468
What can the probability of upcoming words be dependent on?	But natural language is not stationary, since as we show in Chapter 12, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent.	on events that were arbitrarily distant and time dependent	1.7243211734664572
What does a DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allow us to use?	It A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allows us to use some m, which is a model of p (i. e. , an approximation to p).	some m	1.7207480459251303
What said above that this use of relative frequencies as a way to estimate probabilities is an example of maximum likelihood estimation or MLE?	We said above that this use of relative frequencies as a way to estimate probabilities is an example of maximum likelihood estimation or MLE.	such AAC devices	1.719497557721076
Are problems with the addone algorithm summarized in Gale and Church?	Problems with the addone algorithm are summarized in Gale and Church.		1.7184498125620347
Are probabilities also important for augmentative and alternative communication systems (Trnka et al?	Probabilities are also important for augmentative and alternative communication systems (Trnka et al.		1.7180106711257435
What randomly generate the sentence-final token </s>?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.	such AAC devices	1.708594775377723
What was 37?	IEEE Transactions on Information Theory, 37,10851094.	Information Theory	1.7078098464533675
Did N. and de Haan base Research into Language in Oostdijk?	In Oostdijk, N. and de Haan, P. , Corpus-Based Research into Language, 189198.		1.7065468967861777
What did a bigram that occurred 4 times in the first 22 million words occur 3. 23 times in the next 22 million words on?	On average, a bigram that occurred 4 times in the first 22 million words occurred 3. 23 times in the next 22 million words.	on average	1.7050524680925137
Did originally distribute as IBM technical report in Jelinek, F. and Mercer?	Originally distributed as IBM technical report in Jelinek, F. and Mercer, R. .		1.7004274635465748
What 'll we introduce more sophisticated language models like the RNN LMs of Chapter 9 in?	In later chapters we 'll introduce more sophisticated language models like the RNN LMs of Chapter 9.	in later chapters	1.696825026461255
Does figure 3. 6 show the add-one smoothed probabilities for the bigrams in Fig?	Figure 3. 6 shows the add-one smoothed probabilities for the bigrams in Fig.		1.6965971216600262
What do little overlap even in?	While they both model ``English-like sentences'', there is clearly no overlap in generated sentences, and little overlap even in small phrases.	in small phrases	1.6945084227759544
What was Practice?	In Gelsema, E. S. and Kanal, L. N. , Proceedings, Workshop on Pattern Recognition in Practice, 381397.	381397	1.693409958420367
What do such AAC devices print?	We choose a random value between 0 and 1 and print the word whose interval includes this chosen value.	the word whose interval includes this chosen value	1.692840734498005
Did Shannon apply n-grams to compute approximations to English word sequences?	Shannon applied n-grams to compute approximations to English word sequences.		1.6905832681606725
Do let's look at an n-gram grammar trained on a completely different corpus: the Wall Street Journal newspaper?	To get an idea of the dependence of a grammar on its training set, let's look at an n-gram grammar trained on a completely different corpus: the Wall Street Journal newspaper.		1.6893943890013974
What could we encode the most likely horse with for example?	For example, we could encode the most likely horse with the code 0, and the remaining horses as 10, then 110,1110,111100,111101,111110, and 111111.	with the code 0, and the remaining horses as 10, then 110,1110,111100,111101,111110, and 111111	1.6856764858432634
Who is the algorithm called?	The algorithm is called stupid backoff.	stupid backoff	1.6837168960108795
What will the more accurate model be the one with the lower cross-entropy between?	Between two models m1 and m2, the more accurate model will be the one with the lower cross-entropy.	between two models m1 and m2	1.6836677049468465
What is to base our estimate of PCONTINUATION on the number of different contexts word w has appeared in?	The Kneser-Ney intuition is to base our estimate of PCONTINUATION on the number of different contexts word w has appeared in, that is, the number of bigram types it completes.	the Kneser-Ney intuition	1.68333798761611
What 492518?	Computer Speech & Language, 21,492518.	Computer Speech & Language	1.6767899450378991
What do such AAC devices see?	Now we see the following test set: 0 0 0 0 0 3 0 0 0 0.	the following test set: 0 0 0 0 0 3 0 0 0 0	1.676313889550684
Who said above that this use of relative frequencies as a way to estimate probabilities is an example of maximum likelihood estimation or MLE?	We said above that this use of relative frequencies as a way to estimate probabilities is an example of maximum likelihood estimation or MLE.	we	1.675773522476807
Does Chinese occur 400 times in a corpus of a million words like the Brown corpus for example?	For example, suppose the word Chinese occurs 400 times in a corpus of a million words like the Brown corpus.		1.674062702510473
Does kneser-Ney discounting augment absolute discounting with a more sophisticated way to handle the lower-order unigram distribution?	Kneser-Ney discounting (Kneser and Ney, 1995) augments absolute discounting with a more sophisticated way to handle the lower-order unigram distribution.		1.6735480144545982
Have such AAC devices chosen the sample words to cohere with each other in fact?	In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse.		1.6717454434853716
What do such AAC devices just divide into 80 % training, 10 % development, and 10 % test in practice?	In practice, we often just divide our data into 80 % training, 10 % development, and 10 % test.	our data	1.6694365135039335
What was engineering?	Based on Shannon's work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s.	linguistic, and psychological work on modeling word sequences	1.6671788816218447
Does the unigram have probability S = count et al?	in referring to it as S: S (w wi1) otherwise The backoff terminates in the unigram, which has probability S = count et al.		1.6662536275136763
What is the lower?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.	the word sequence	1.6647416697362865
What 'll we introduce in this section and the following ones?	In this section and the following ones we 'll introduce a variety of ways to do smoothing: add-1 smoothing, add-k smoothing, stupid backoff, and Kneser-Ney smoothing.	a variety of ways to do smoothing: add-1 smoothing, add-k smoothing, stupid backoff, and Kneser-Ney smoothing	1.6628008380088484
What is between 0 and 1?	Imagine all the words of the English language covering the probability space between 0 and 1, each word covering an interval proportional to its frequency.	each word covering an interval proportional to log space's frequency	1.6627340738137955
What was 381397?	In Gelsema, E. S. and Kanal, L. N. , Proceedings, Workshop on Pattern Recognition in Practice, 381397.	Practice	1.6601136929129559
What do these zeros-- things that do not ever occur in the training set but do occur in the test set-- reach a In?	We continue backing off until we reach a In order for a backoff model to give a correct probability distribution, we have to discount the higher-order n-grams to save some probability mass for the lower order n-grams.	In order for a backoff model to give a correct probability distribution, we have to discount the higher-order n-grams to save some probability mass for the lower order n-grams	1.6589948688189926
'll Chen follow Brants et al?	This algorithm does not produce a probability distribution, so we 'll follow Brants et al.		1.6539319434639501
Are the trigram and 4-gram sentences beginning to look a lot like Shakespeare?	The trigram and 4-gram sentences are beginning to look a lot like Shakespeare.		1.6516655033547063
When this is followed by the?	One way to estimate this probability is from relative frequency counts: take a very large corpus, count the number of times we see its water is so transparent that, and count the number of times this is followed by the.	one way to estimate this probability is from relative frequency counts: take a very large corpus, count the number of times such AAC devices see its water is so transparent that, and count the number of times	1.6498153191146034
Who is a held-out corpus an additional training corpus that we use to set like these values?	A held-out corpus is an additional training corpus that we use to set hyperparameters like these values, by choosing the values that maximize the likelihood of the held-out corpus.	hyperparameters	1.6492665364636059
Did Chen show the advantages of Modified Interpolated KneserNey?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.		1.6459380457915351
Is the equation for interpolated absolute discounting applied to bigrams: PAbsoluteDiscounting = P The first term the discounted bigram?	The equation for interpolated absolute discounting applied to bigrams: PAbsoluteDiscounting (wi wi1) = P The first term is the discounted bigram, and the second term is the unigram with an interpolation weight.		1.6430433117235208
What does figure 3. 7 show?	Figure 3. 7 shows the reconstructed counts.	the reconstructed counts	1.6425856584975607
Are models that assign probabilities to sequences of words called language models or LMs?	Models that assign probabilities to sequences of words are called language models or LMs.		1.639396369748027
What are essential in any task in which we have to identify words in noisy like speech recognition?	Probabilities are essential in any task in which we have to identify words in noisy, ambiguous input, like speech recognition.	probabilities	1.6356897266461758
What might even be cultural rather than linguistic?	And some might even be cultural rather than linguistic, like the higher probability that people are looking for Chinese versus English food.	some	1.6354044852078942
What can such AAC devices compare the performance of two language models by seeing for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	which gives the more accurate transcription	1.6346035236853422
Am devset Sam corpus Calculate the probability of the sentence i want chinese food?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.		1.632598858313742
What 5064?	Bell System Technical Journal, 30,5064.	Bell System Technical Journal	1.632414079566844
Were the IBM Thomas J. Watson Research Center influenced by Shannon, and Baker?	The resurgence of n-gram models came from Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and Baker at CMU, who was influenced by the work of Baum and colleagues.		1.631575701328845
What does the table below show?	The table below shows the perplexity of a 1. 5 million word WSJ test set according to each of these grammars.	the perplexity of a 1. 5 million word WSJ test set according to each of these grammars	1.6310908586536734
Who am I corpus Calculate the probability of the sentence i want chinese food?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.	Sam	1.6289931561619326
What 379423?	Bell System Technical Journal, 27,379423.	Bell System Technical Journal	1.6283657901709492
Who am I Sam corpus the probability of the sentence i want chinese food?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.	Calculate	1.6269324305789006
What do we reach a In?	We continue backing off until we reach a In order for a backoff model to give a correct probability distribution, we have to discount the higher-order n-grams to save some probability mass for the lower order n-grams.	In order for a backoff model to give a correct probability distribution, we have to discount the higher-order n-grams to save some probability mass for the lower order n-grams	1.6260624603780127
Is Goodman 1999 Goodman 2006?	Starting in the late 1990s, Chen and Goodman produced a highly influential series of papers with a comparison of different language models (Chen and Goodman 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).		1.625313237683125
Who does a DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allow to use some m?	It A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allows us to use some m, which is a model of p (i. e. , an approximation to p).	us	1.6237655815491472
What is the bigram?	In backoff, we use the trigram if the evidence is sufficient, otherwise we use the bigram, otherwise the unigram.	the unigram	1.6234445238877921
What do the probabilities of an n-gram model come from with many of the statistical models in such AAC devices's field?	As with many of the statistical models in our field, the probabilities of an n-gram model come from the corpus it is trained on, the training set or training corpus.	from the corpus	1.6218584441294135
What 'll we introduce a variety of in this section and the following ones?	In this section and the following ones we 'll introduce a variety of ways to do smoothing: add-1 smoothing, add-k smoothing, stupid backoff, and Kneser-Ney smoothing.	of ways to do smoothing: add-1 smoothing, add-k smoothing, stupid backoff, and Kneser-Ney smoothing	1.6209515674237394
Did Chen see above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code?	We saw above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code, so the average was 3.		1.6182239715111502
Does stupid backoff give up the idea of trying to make the language model a true probability distribution?	Stupid backoff gives up the idea of trying to make the language model a true probability distribution.		1.6170914345195855
Is the result the smoothed bigram probabilities in Fig?	The result is the smoothed bigram probabilities in Fig.		1.6149786664106993
Does the interpolated Kneser-Ney smoothing algorithm mix a discounted probability with a lower-order continuation probability?	The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.		1.6128034944953806
What involves sequences?	But most of what we will use entropy for involves sequences.	most of what Chen will use entropy for	1.6111319411609133
Is KenLM (Heafield 2011 Heafield et al?	Finally, efficient language model toolkits like KenLM (Heafield 2011, Heafield et al.		1.6068224428384101
Does figure 3. 2 show the bigram probabilities after normalization (dividing each cell in Fig?	Figure 3. 2 shows the bigram probabilities after normalization (dividing each cell in Fig.		1.6029071489667661
What will the more accurate model be with the lower cross-entropy between two models m1 and m2?	Between two models m1 and m2, the more accurate model will be the one with the lower cross-entropy.	the one	1.6011512623283246
What seems rather a lot to estimate!	That seems rather a lot to estimate!	that	1.6008500761193316
Who is Sam corpus Calculate the probability of the sentence i want chinese food?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.	I	1.5967431595009272
What are probabilities generally quantized using?	Probabilities are generally quantized using only 4-8 bits (instead of 8-byte floats), and n-grams are stored in reverse tries.	only 4-8 bits	1.596227298571918
Did Chen perform a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters?	They performed a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters.		1.5938237406401718
Is Osborne 2007 Church et al?	Another option is to build approximate language models using techniques like Bloom filters (Talbot and Osborne 2007, Church et al.		1.5937520165887915
What is an additional training corpus that we use to set hyperparameters like these values?	A held-out corpus is an additional training corpus that we use to set hyperparameters like these values, by choosing the values that maximize the likelihood of the held-out corpus.	a held-out corpus	1.5935376591289612
What has the generator chosen?	Thus, once the generator has chosen the first 4-gram (It cannot be but), there are only five possible continuations (that, I, he, thou, and so) ; indeed, for many 4-grams, there is only one continuation.	the first 4-gram	1.5922952356689706
What am I Sam corpus Calculate i want chinese food?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.	the probability of the sentence	1.5899690562397941
What was 21?	Computer Speech & Language, 21,492518.	Computer Speech & Language	1.5897790147280124
Does SRILM offer a wider range of options and types of discounting?	SRILM offers a wider range of options and types of discounting, while KenLM is optimized for speed and memory size, making it possible to build web-scale language models.		1.5878504999712142
Do such AAC devices need the end-symbol to make the bigram grammar a true probability distribution?	2 We need the end-symbol to make the bigram grammar a true probability distribution.		1.5875794243424264
What am I Sam corpus Calculate the probability of the sentence i want?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.	chinese food	1.587551427330192
What is moved to all the zeros?	The sharp change in counts and probabilities occurs because too much probability mass is moved to all the zeros.	too much probability mass	1.5872770800631741
What will mainly modify the smaller counts?	It will mainly modify the smaller counts, N- GRAM L ANGUAGE M ODELS for which we do not necessarily trust the estimate anyway, and Fig.	this intuition	1.584904058383911
What do such AAC devices want?	We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible.	our test set to be as large as possible	1.584775200152015
Does cross-entropy explain otherwise mysterious properties of perplexity (why the inverse probability, ?	The perplexity measure actually arises from the information-theoretic concept of cross-entropy, which explains otherwise mysterious properties of perplexity (why the inverse probability, for example? )		1.583214440592985
Did Markov use what are now called Markov chains to predict whether an upcoming letter in Pushkin's Eugene Onegin would be a vowel or a consonant?	The underlying mathematics of the n-gram was first proposed by Markov, who used what are now called Markov chains (bigrams and trigrams) to predict whether an upcoming letter in Pushkin's Eugene Onegin would be a vowel or a consonant.		1.5821329008220295
Were Markov models commonly used in engineering by the 1950s?	Based on Shannon's work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s.		1.5814535737588957
What is 1000000 or?	The MLE of its probability is 1000000 or.	the MLE of its probability	1.5809143168833402
Was (Bulletin de l'Academie Imperiale des Sciences de Mikolov T.?	Izvistia Imperatorskoi Akademii Nauk (Bulletin de l'Academie Imperiale des Sciences de Mikolov, T. .		1.5791595784682353
Is E VALUATING L ANGUAGE M ODELS Adding in log space equivalent to multiplying in linear space?	E VALUATING L ANGUAGE M ODELS Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them.		1.5791366744607007
What's simplest to visualize how this works for the unigram case?	It's simplest to visualize how this works for the unigram case.	one implication of this	1.5788714390175285
What was P?	In MLE, the resulting parameter set maximizes the likelihood of the training set T given the model M (i. e. , P (T M) ).	i. e.	1.5765149698968302
What 'll we introduce a variety of ways to do smoothing: add-1 smoothing, add-k smoothing, stupid backoff, and Kneser-Ney smoothing in?	In this section and the following ones we 'll introduce a variety of ways to do smoothing: add-1 smoothing, add-k smoothing, stupid backoff, and Kneser-Ney smoothing.	in this section and the following ones	1.5749178322817858
Do such AAC devices call the initial test set the development test set or, devset in such cases?	In such cases, we call the initial test set the development test set or, devset.		1.573543952369771
Who can use a clever idea from Church and Gale?	To see this, we can use a clever idea from Church and Gale.	we	1.5727858009077527
What is the word sequence?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.	the lower	1.5705775347423556
What is a held-out corpus an additional training corpus that we use to set hyperparameters like?	A held-out corpus is an additional training corpus that we use to set hyperparameters like these values, by choosing the values that maximize the likelihood of the held-out corpus.	like these values	1.5704716136334116
Does N-gram count and language models from the common crawl?	N-gram counts and language models from the common crawl.		1.5698899903224195
Did Improved back for Mgram language modeling?	Improved backing-off for Mgram language modeling.		1.5686560077124176
Is an intuitive way to estimate probabilities called maximum likelihood estimation or MLE?	An intuitive way to estimate probabilities is called maximum likelihood estimation or MLE.		1.5681878666095437
Do Let's say that 0 occur 91 times in the training set, and each of the other digits occurred 1 time each?	Let's say that 0 occur 91 times in the training set, and each of the other digits occurred 1 time each.		1.5681444904398059
What are n-grams?	n-grams are Markov models that estimate words from a fixed window of previous words.	Markov models that estimate words from a fixed window of previous words	1.5668238880396284
What are 1?	We do not know any way to compute the exact probability of a word given a long sequence of preceding words, 1).	preceding words	1.5638497915983967
What does the word glasses have?	The word glasses has a much wider distribution.	a much wider distribution	1.56172812282329
If Chen spend the whole day betting and each horse is coded with 3 bits would we be sending 3 bits per race?	If we spend the whole day betting and each horse is coded with 3 bits, on average we would be sending 3 bits per race.		1.5615042206791894
What will now have a count of 1 and so on?	All the counts that used to be zero will now have a count of 1, the counts of 1 will be 2, and so on.	all the counts that used to be zero	1.5590124125477236
What is perplexity?	Training on the test set introduces a bias that makes the probabilities all look too high, and causes huge inaccuracies in perplexity, the probability-based metric we introduce below.	the probability-based metric	1.5565713151787188
Is the branching factor of a language the number of possible next words that can follow any word?	The branching factor of a language is the number of possible next words that can follow any word.		1.5549869864966488
Who recursively back off to the Katz probability for the shorter-history- gram?	Otherwise, we recursively back off to the Katz probability for the shorter-history- gram.	we	1.5534491635681449
What was 27?	Bell System Technical Journal, 27,379423.	Bell System Technical Journal	1.5532699529701426
Are both the simple interpolation and conditional interpolation s learned from a held-out corpus?	Both the simple interpolation and conditional interpolation s are learned from a held-out corpus.		1.5507249805227374
Will Chen be computing the entropy of some sequence of words W = w0 w2, . . . for a grammar for example?	For a grammar, for example, we will be computing the entropy of some sequence of words W = w0, w1, w2, . . . , wn.		1.5506336354000503
Whose works does a Figure 3. 3 Eight sentences randomly generated from four n-grams computed from?	A Figure 3. 3 Eight sentences randomly generated from four n-grams computed from Shakespeare's works.	from Shakespeare's works	1.5459438308554245
Is an intrinsic evaluation metric one that measures the quality of a model independent of any application?	An intrinsic evaluation metric is one that measures the quality of a model independent of any application.		1.545191126857059
If the probability of any word in the test set is 0 is the entire probability of the test set 0?	Second, if the probability of any word in the test set is 0, the entire probability of the test set is 0.		1.5428140547349916
Did L. N. workshop on Pattern Recognition in Practice in Gelsema, E. S. and Kanal?	In Gelsema, E. S. and Kanal, L. N. , Proceedings, Workshop on Pattern Recognition in Practice, 381397.		1.5411499625537401
Does kneser-Ney smoothing make use of the probability of a word being a novel continuation?	Kneser-Ney smoothing makes use of the probability of a word being a novel continuation.		1.5385948905964675
What was i. e.?	In MLE, the resulting parameter set maximizes the likelihood of the training set T given the model M (i. e. , P (T M) ).	P	1.5382912134807456
What the more information gives us about the word sequence the perplexity (since as Eq?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.	the n-gram	1.5374690003614746
Did Computer Speech & Language 492518?	Computer Speech & Language, 21,492518.		1.5368522043025794
Who thesis?	Ph. . D. thesis, Ph. .	Ph. . D.	1.5367920152433598
Did the previous section discuss the problem of words whose bigram probability is zero?	The previous section discussed the problem of words whose bigram probability is zero.		1.5311880260199473
What was not written up until later?	A trigram model was used in the IBM TANGORA speech recognition system in the 1970s, but the idea was not written up until later.	the idea	1.5268201874882887
What was 30?	Bell System Technical Journal, 30,5064.	Bell System Technical Journal	1.5265463433833193
What does a Figure 3. 3 Eight sentences randomly generated from four n-grams computed from?	A Figure 3. 3 Eight sentences randomly generated from four n-grams computed from Shakespeare's works.	from Shakespeare's works	1.521910446415452
What is 000?	There are V 2 = 844,000,000 possible bigrams alone, and the number of possible 4-grams is V 4 = 7 1017.	844	1.521672293971065
What are probabilities essential in in which we have to identify words in noisy like speech recognition?	Probabilities are essential in any task in which we have to identify words in noisy, ambiguous input, like speech recognition.	in any task	1.518578334385702
What is a held-out corpus corpus that we use to set hyperparameters like these values?	A held-out corpus is an additional training corpus that we use to set hyperparameters like these values, by choosing the values that maximize the likelihood of the held-out corpus.	an additional training	1.5180414131159634
What is the probability that makes it most likely that Chinese will occur 400 times in a million-word corpus?	But it is the probability that makes it most likely that Chinese will occur 400 times in a million-word corpus.	MLE	1.5161929342331395
Was the underlying mathematics of the n-gram first proposed by Markov?	The underlying mathematics of the n-gram was first proposed by Markov, who used what are now called Markov chains (bigrams and trigrams) to predict whether an upcoming letter in Pushkin's Eugene Onegin would be a vowel or a consonant.		1.515296052531554
Can a language model achieve low perplexity by choosing a small vocabulary?	A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability.		1.512130082959906
Who can compute the entropy of some stochastic process by computing its average log probability?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.	we	1.5098389856291097
Does Equation 3. 4 suggest that such AAC devices could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities?	Equation 3. 4 suggests that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities.		1.5094585751458922
Was L. N. Proceedings?	In Gelsema, E. S. and Kanal, L. N. , Proceedings, Workshop on Pattern Recognition in Practice, 381397.		1.5090079698877181
Can n-gram probabilities be estimated by counting in a corpus?	n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).		1.5060605892059602
Does the exact choice of <UNK> model have an effect on metrics like perplexity?	The exact choice of <UNK> model does have an effect on metrics like perplexity.		1.5060025682502003
Do compressing trigram language models with Golomb coding?	Compressing trigram language models with Golomb coding.		1.5044193405462956
What does Equation 3. 4 suggest that we could estimate the joint probability of by multiplying together a number of conditional probabilities?	Equation 3. 4 suggests that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities.	of an entire sequence of words	1.5032311245669487
What can we use from Church and Gale?	To see this, we can use a clever idea from Church and Gale.	a clever idea	1.5009110377663197
What is the Kneser-Ney intuition to base our estimate of PCONTINUATION on the number of different contexts has appeared in?	The Kneser-Ney intuition is to base our estimate of PCONTINUATION on the number of different contexts word w has appeared in, that is, the number of bigram types it completes.	word w	1.4951728712215115
If Chen spend the whole day betting and each horse is coded with 3 bits who would be sending 3 bits per race?	If we spend the whole day betting and each horse is coded with 3 bits, on average we would be sending 3 bits per race.	we	1.4950182025599026
If a higher-order n-gram has a zero count, do Chen simply backoff to a lower order n-gram?	If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed weight.		1.492598129547095
Who set any word that is not in this set to the unknown word token <UNK> in a text normalization step?	Convert in the training set any word that is not in this set (any OOV word) to the unknown word token <UNK> in a text normalization step.	convert in the training	1.490882404816635
What was the average?	We saw above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code, so the average was 3.	3	1.4900216579835879
Did the resurgence of n-gram models come from Jelinek and colleagues at the IBM Thomas J. Watson Research Center at CMU?	The resurgence of n-gram models came from Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and Baker at CMU, who was influenced by the work of Baum and colleagues.		1.4891468396848127
Who can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus?	We can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus.	we	1.4885121368487444
Is output hand-corrected The longer the context on which such AAC devices train the model?	Output is hand-corrected The longer the context on which we train the model, the more coherent the sentences.		1.4881271370235258
What would be even more sparse?	In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse.	a matrix selected from a random set of seven words	1.4859606471586335
Did Bell System Technical Journal 5064?	Bell System Technical Journal, 30,5064.		1.4854593356363854
Does a DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allow Chen to use some m?	It A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allows us to use some m, which is a model of p (i. e. , an approximation to p).		1.4853282655867486
Do such AAC devices continue generating words until we randomly generate the sentence-final token </s>?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.		1.4834543061921415
Did Bell System Technical Journal 379423?	Bell System Technical Journal, 27,379423.		1.4814110462404906
Did C change from 609 to 238!	C (want to) changed from 609 to 238!		1.4768076693616452
What was Mind?	Mind, 41,421423.	41	1.4753950579148403
Who are these papers recommended for any reader with for building language models are SRILM and KenLM (Heafield 2011?	These papers are recommended for any reader with further Two commonly used toolkits for building language models are SRILM (Stolcke, 2002) and KenLM (Heafield 2011, Heafield et al.	with further Two commonly used toolkits	1.4746513808500015
Who has its roots in a method called absolute discounting?	Kneser-Ney has its roots in a method called absolute discounting.	kneser-Ney	1.4736546691218133
Does KenLM make memory size possible to build web-scale language models?	SRILM offers a wider range of options and types of discounting, while KenLM is optimized for speed and memory size, making it possible to build web-scale language models.		1.4731749552023605
Can such AAC devices then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus?	We can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus.		1.4721692635907888
What may be accidentally unrepresentative?	We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible.	a small test set	1.471603459155597
Do commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation?	Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.		1.4713816611370722
What suggests that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities?	Equation 3. 4 suggests that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities.	Equation 3. 4	1.4710600240124645
Who requires that we have a method for choosing k?	Add-k smoothing requires that we have a method for choosing k; this can be done, for example, by optimizing on a devset.	add-k smoothing	1.4698791577904573
Who am devset?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.	Sam corpus Calculate the probability of the sentence i want chinese food	1.4687431350835571
What did a bigram that occurred 4 times in the first 22 million words occur in the next 22 million words on average?	On average, a bigram that occurred 4 times in the first 22 million words occurred 3. 23 times in the next 22 million words.	3. 23 times	1.4681389410881514
Who might have a good estimate of its probability for any n-gram that occurred a sufficient number of times?	For any n-gram that occurred a sufficient number of times, we might have a good estimate of its probability.	we	1.4680031369475732
What will most of the time the next number be?	We should expect the perplexity of this test set to be lower since most of the time the next number will be zero, which is very predictable, i. e. Thus, although the branching factor is still 10, the perplexity or weighted branching factor is smaller.	zero	1.4655612888200522
Did such AAC devices next choose a random bigram starting with w (again?	We next chose a random bigram starting with w (again, drawn according to its To give an intuition for the increasing power of higher-order n-grams, Fig.		1.4652556550395799
What introduce the simplest model that assigns probabilities to sentences and sequences of words in this chapter?	In this chapter we introduce the simplest model that assigns probabilities to sentences and sequences of words, the n-gram.	such AAC devices	1.4652028372847679
Is the perplexity of a language model on a test set the inverse probability of the test set?	The perplexity (sometimes called PP for short) of a language model on a test set is the inverse probability of the test set, normalized by the number of words.		1.4621923081799606
Should a model's improvement in perplexity always be confirmed by an end-to-end evaluation of a real task before concluding the evaluation of the model?	But a model's improvement in perplexity should always be confirmed by an end-to-end evaluation of a real task before concluding the evaluation of the model.		1.4620686716481943
What is still 10?	We should expect the perplexity of this test set to be lower since most of the time the next number will be zero, which is very predictable, i. e. Thus, although the branching factor is still 10, the perplexity or weighted branching factor is smaller.	the branching factor	1.4607982712514225
Is a stochastic process said to be stationary if the probabilities it assigns to a sequence are invariant with respect to shifts in the time index?	A stochastic process is said to be stationary if the probabilities it assigns to a sequence are invariant with respect to shifts in the time index.		1.4577849121280804
Can any kind of knowledge of the test set cause the perplexity to be artificially low?	Any kind of knowledge of the test set can cause the perplexity to be artificially low.		1.4556580250723719
What does the more information the n-gram give us about the perplexity (since as Eq?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.	about the word sequence	1.4542181992686372
Who performed a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters?	They performed a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters.	they	1.4526129277023796
If we have no examples of a particular trigram wn2 wn1 wn, who can instead estimate its probability by using the bigram probability P?	If we are trying to compute P (wn wn2 wn1) but we have no examples of a particular trigram wn2 wn1 wn, we can instead estimate its probability by using the bigram probability P (wn wn1).	we	1.4526018607697622
What are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past?	Markov models are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past.	Markov models	1.45240670863206
Can word prediction be used to suggest likely words for the menu?	Word prediction can be used to suggest likely words for the menu.		1.451898373389664
What is 844?	There are V 2 = 844,000,000 possible bigrams alone, and the number of possible 4-grams is V 4 = 7 1017.	000 possible bigrams	1.4500931238389394
What can be done?	Add-k smoothing requires that we have a method for choosing k; this can be done, for example, by optimizing on a devset.	this	1.449288470331909
If we do not have counts to compute P who can look to the unigram P?	Similarly, if we do not have counts to compute P (wn wn1), we can look to the unigram P (wn).	we	1.4474046009871806
What might sentences and sequences of words, the n-gram expect between?	Shakespeare and the Wall Street Journal are both English, so we might expect some overlap between our n-grams for the two genres.	some overlap between our n-grams for the two genres	1.4460256540859198
What turns out that even the web is not big enough to give such AAC devices good estimates in most cases?	You should pause now, go to the web, and While this method of estimating probabilities directly from counts works fine in many cases, it turns out that even the web is not big enough to give us good estimates in most cases.	this method of estimating probabilities directly from counts	1.4454849044642664
Who present ways to modify the MLE estimates slightly to get better probability estimates in Section 3. 4?	We present ways to modify the MLE estimates slightly to get better probability estimates in Section 3. 4.	we	1.443301158046908
Who can use two pseudo-words for the first trigram We always represent and compute language model probabilities in log format as log probabilities for example?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.	we	1.4430748416443624
Who computed a bigram grammar from 22 million words of AP newswire?	They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words.	they	1.442346577666394
Does most of what Chen will use entropy for involve sequences?	But most of what we will use entropy for involves sequences.		1.4417134523183797
What are words?	In this chapter we introduce the simplest model that assigns probabilities to sentences and sequences of words, the n-gram.	the n-gram	1.4413592083571414
What is noisy?	Probabilities are essential in any task in which we have to identify words in noisy, ambiguous input, like speech recognition.	ambiguous input	1.4410899235283796
Do a maximum likelihood approach to continuous speech recognition?	A maximum likelihood approach to continuous speech recognition.		1.4399365737650975
Can such AAC devices use two pseudo-words for the first trigram We always represent and compute language model probabilities in log format as log probabilities for example?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.		1.4392255863206507
What is not stationary?	But natural language is not stationary, since as we show in Chapter 12, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent.	natural language	1.4390071310198622
What queries?	KenLM: Faster and smaller language model queries.	kenLM: Faster and smaller language model	1.435523631706833
Do Chen's statistical models only give an approximation to the correct distributions and entropies of natural language?	Thus, our statistical models only give an approximation to the correct distributions and entropies of natural language.		1.43492812283315
What makes it possible to build web-scale language models?	SRILM offers a wider range of options and types of discounting, while KenLM is optimized for speed and memory size, making it possible to build web-scale language models.	KenLM	1.4318232462825504
What would this model define an infinite set of?	This model would define an infinite set of probability distributions, with one distribution per sentence length.	of probability distributions	1.430666329110442
What the smaller becomes?	Since probabilities are (by definition) less than or equal to 1, the more probabilities we multiply together, the smaller the product becomes.	the product	1.429317313273599
Who rely on a discounted probability P if we 've seen this n-gram before (i. e. , if we have non-zero counts in Katz backoff?	In Katz backoff we rely on a discounted probability P if we 've seen this n-gram before (i. e. , if we have non-zero counts).	we	1.4288134373208412
Will Chen need an approximation to crossentropy?	We will need an approximation to crossentropy, relying on a (sufficiently long) sequence of fixed length.		1.4275723632862385
Do the bigram sentences have some local word-to-word coherence?	The bigram sentences have some local word-to-word coherence (especially if we consider that punctuation counts as a word).		1.4274853817936726
What does the more information the n-gram give us about the word sequence?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.	the perplexity (since as Eq	1.4251765536951235
Is the n-gram model dependent on the training corpus?	The n-gram model, like many statistical models, is dependent on the training corpus.		1.4236128078063928
What can we use a clever idea from?	To see this, we can use a clever idea from Church and Gale.	from Church and Gale	1.4226514448379515
What is called smoothing or discounting?	This modification is called smoothing or discounting.	this modification	1.42240317458958
Who introduce the simplest model that assigns probabilities to sentences and sequences of words in this chapter?	In this chapter we introduce the simplest model that assigns probabilities to sentences and sequences of words, the n-gram.	we	1.422085616392525
Who have chosen the sample words to cohere with each other in fact?	In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse.	we	1.4214010701802948
Do such AAC devices not know any way to compute the exact probability of a word given a long sequence of preceding words)?	We do not know any way to compute the exact probability of a word given a long sequence of preceding words, 1).		1.418051281408156
Is P. generally represented in memory as a 64-bit hash number?	Rather than store each word as a string, it is generally represented in memory as a 64-bit hash number, with the words themselves stored on disk.		1.4172162102976014
What 421423?	Mind, 41,421423.	Mind	1.416595625092889
What did they compute from 22 million words of AP newswire?	They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words.	a bigram grammar	1.416545528522298
What is the intuition of the Shannon-McMillan-Breiman theorem in?	The intuition of the Shannon-McMillan-Breiman theorem is that a long-enough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence according to their probabilities.	that a long-enough sequence of words will contain in it many other shorter sequences	1.4154351352974706
Is the first one to turn the problem back into a closed vocabulary one by choosing a fixed vocabulary in advance: 1?	The first one is to turn the problem back into a closed vocabulary one by choosing a fixed vocabulary in advance: 1.		1.4148527034724312
Will the more accurate model be the one with the lower cross-entropy between two models m1 and m2?	Between two models m1 and m2, the more accurate model will be the one with the lower cross-entropy.		1.4130369156601357
Do smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams?	Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams.		1.412253916709648
What is V 4 = 7 1017?	There are V 2 = 844,000,000 possible bigrams alone, and the number of possible 4-grams is V 4 = 7 1017.	the number of possible 4-grams	1.41181963847319
What works fine in many cases?	You should pause now, go to the web, and While this method of estimating probabilities directly from counts works fine in many cases, it turns out that even the web is not big enough to give us good estimates in most cases.	this method of estimating probabilities directly from counts	1.4116511837729362
Is the perplexity of a test set according to a language model the geometric mean of the inverse test set probability computed by the model?	The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.		1.4096314786448119
Is the second term the unigram with an interpolation weight?	The equation for interpolated absolute discounting applied to bigrams: PAbsoluteDiscounting (wi wi1) = P The first term is the discounted bigram, and the second term is the unigram with an interpolation weight.		1.4089601561140799
What do we not know?	We do not know any way to compute the exact probability of a word given a long sequence of preceding words, 1).	any way to compute the exact probability of a word given a long sequence of preceding words)	1.407472468666827
What shows the equation for interpolation with context-conditioned weights: P = 1 (wn1 How are these values set?	Equation 3. 28 shows the equation for interpolation with context-conditioned weights: P (wn wn2 wn1) = 1 (wn1 How are these values set?	Equation 3. 28	1.4048906605608975
Who next chose a random bigram starting with w (again?	We next chose a random bigram starting with w (again, drawn according to its To give an intuition for the increasing power of higher-order n-grams, Fig.	we	1.4037468005486287
Do such AAC devices need a test set for an intrinsic evaluation of a language model?	For an intrinsic evaluation of a language model we need a test set.		1.401938915315088
Did Chen introduce perplexity in Section 3. 2. 1 as a way to evaluate n-gram models on a test set?	We introduced perplexity in Section 3. 2. 1 as a way to evaluate n-gram models on a test set.		1.401657646680511
What have we chosen the sample words to cohere with in fact?	In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse.	with each other	1.400207987517775
What will all the counts that used to be zero now have a count of and so on?	All the counts that used to be zero will now have a count of 1, the counts of 1 will be 2, and so on.	of 1	1.4000297597498779
Is Witten I. H. C.?	In NAACL-HLT 07,173 Witten, I. H. C. .		1.3979600485343289
What 'll we need a function to distribute this probability mass to the lower order in addition to this explicit discount factor?	In addition to this explicit discount factor, we 'll need a function to distribute this probability mass to the lower order This kind of backoff with discounting is also called Katz backoff.	This kind of backoff with discounting is also called Katz backoff	1.3975764774477728
What do we introduce the simplest model that assigns probabilities to sentences and sequences of words in?	In this chapter we introduce the simplest model that assigns probabilities to sentences and sequences of words, the n-gram.	in this chapter	1.396939626755652
What have we chosen the sample words to cohere with each other in?	In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse.	in fact	1.3965470492783698
Do neural language models instead project words into a continuous space in which words with similar contexts have similar representations?	Neural language models instead project words into a continuous space in which words with similar contexts have similar representations.		1.396130151460147
Do both backoff and interpolation require discounting to create a probability distribution?	Both backoff and interpolation require discounting to create a probability distribution.		1.3954487969499438
If a higher-order n-gram has a zero count, who simply backoff to a lower order n-gram?	If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed weight.	we	1.3946631699965129
Is MLE the probability that makes it most likely that Chinese will occur 400 times in a million-word corpus?	But it is the probability that makes it most likely that Chinese will occur 400 times in a million-word corpus.		1.394293087689039
What have we chosen to cohere with each other in fact?	In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse.	the sample words	1.388883136249056
What was 3?	We saw above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code, so the average was 3.	the average	1.3888138167156907
Who 'll follow Brants et al?	This algorithm does not produce a probability distribution, so we 'll follow Brants et al.	we	1.386901764377967
What are two probabilities?	Give two probabilities, one using Fig.	one	1.3856422129006485
Was the lower order n-gram weighed by a fixed weight?	If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed weight.		1.3835811358821175
What 'll we introduce in later chapters?	In later chapters we 'll introduce more sophisticated language models like the RNN LMs of Chapter 9.	more sophisticated language models like the RNN LMs of Chapter 9	1.3832621678296548
What print the word whose interval includes this chosen value?	We choose a random value between 0 and 1 and print the word whose interval includes this chosen value.	such AAC devices	1.3806174490851468
Did Chen see there in Eq?	We are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq.		1.3779031466618616
What-- give us the highest probability of the held-out set?	3. 26-- give us the highest probability of the held-out set.	3. 26	1.377585777493949
Does a careful investigation of the 4-gram sentences show that they look a little too much like Shakespeare?	Indeed, a careful investigation of the 4-gram sentences shows that they look a little too much like Shakespeare.		1.3766644623778794
Does the more information the n-gram give such AAC devices about the word sequence the perplexity (since as Eq?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.		1.3763291406266904
How many pseudo-words can we use for the first trigram We always represent and compute language model probabilities in log format as log probabilities for example?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.	two pseudo-words	1.3736919619593047
Are the highest accuracy language models neural network language models?	The highest accuracy language models are neural network language models.		1.373144661244932
What can we compare by seeing which gives the more accurate transcription for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	the performance of two language models	1.3725601519419937
Who only ``back off'' to a lower-order n-gram if we have zero evidence for a higher-order n-gram in other words?	In other words, we only ``back off'' to a lower-order n-gram if we have zero evidence for a higher-order n-gram.	we	1.3721818077762493
Is Machine Baker J. K.?	IEEE Transactions on Pattern Analysis and Machine Baker, J. K. .		1.3717065804523587
Does the perplexity measure actually arise from the information-theoretic concept of cross-entropy for example? )	The perplexity measure actually arises from the information-theoretic concept of cross-entropy, which explains otherwise mysterious properties of perplexity (why the inverse probability, for example? )		1.3678254814166633
What is hand-corrected The longer the context on which we train the model?	Output is hand-corrected The longer the context on which we train the model, the more coherent the sentences.	output	1.366626604830564
Is Lee K. - F.?	and Lee, K. - F. , Readings in Speech Recognition, 450506.		1.3656330315127425
Does figure 3. 6 add smoothed bigram probabilities for eight of the words corpus of 9332 sentences?	Figure 3. 6 Add-one smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP corpus of 9332 sentences.		1.3649356681996037
Are some perfectly acceptable English word sequences bound to be missing from a very large corpus?	But because any corpus is limited, some perfectly acceptable English word sequences are bound to be missing from it.		1.3646716528857268
Was N. and de Haan P.?	In Oostdijk, N. and de Haan, P. , Corpus-Based Research into Language, 189198.		1.3638720548771666
Can such AAC devices generalize the bigram to the trigram?	We can generalize the bigram (which looks one word into the past) to the trigram (which looks two words into the past) and thus to the n-gram (which looks n 1 words into the past).		1.3631761210383466
What can only contain words from this lexicon in such a closed vocabulary system?	In such a closed vocabulary system the test set can only contain words from this lexicon, and there will be no unknown words.	the test set	1.362804891870116
What is the intuition of the Shannon-McMillan-Breiman theorem that a long-enough sequence of words will contain in many other shorter sequences?	The intuition of the Shannon-McMillan-Breiman theorem is that a long-enough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence according to their probabilities.	in it	1.361710168912913
Is Pi dependent only on Pi1 for example in a bigram?	For example, in a bigram, Pi is dependent only on Pi1.		1.3616538539348526
Will the same models also serve to assign a probability to an entire sentence?	The same models will also serve to assign a probability to an entire sentence.		1.3614992015634257
Is a better n-gram model perplexity a normalized version of the probability of the test set?	A better n-gram model is one that assigns a higher probability to the A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY test data, and perplexity is a normalized version of the probability of the test set.		1.361050380734366
Who need the end-symbol to make the bigram grammar a true probability distribution?	2 We need the end-symbol to make the bigram grammar a true probability distribution.	we	1.3609819530058447
Who need a test set for an intrinsic evaluation of a language model?	For an intrinsic evaluation of a language model we need a test set.	we	1.3602507050442738
Can Chen compute the entropy of some stochastic process by taking a very long sample of the output?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.		1.3593954428828745
What was 41?	Mind, 41,421423.	Mind	1.3582971154092691
Was Proceedings of the IEEE 64?	Proceedings of the IEEE, 64,532557.		1.3576964783904373
What is that each of these shorter sequences will reoccur in the longer sequence according to their probabilities?	The intuition of the Shannon-McMillan-Breiman theorem is that a long-enough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence according to their probabilities.	the intuition of the Shannon-McMillan-Breiman theorem	1.3550711487456324
Who call the initial test set the development test set or, devset in such cases?	In such cases, we call the initial test set the development test set or, devset.	we	1.3540133335722797
Who does the more information the n-gram give about the word sequence the perplexity (since as Eq?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.	us	1.353917189998202
If we spend the whole day betting and each horse is coded with 3 bits who would be sending 3 bits per race?	If we spend the whole day betting and each horse is coded with 3 bits, on average we would be sending 3 bits per race.	we	1.3531151533613346
Does the combined Good-Turing backoff algorithm involve quite detailed computation for estimating the Good-Turing smoothing and the P and values?	The combined Good-Turing backoff algorithm involves quite detailed computation for estimating the Good-Turing smoothing and the P and values.		1.3517747654085754
Does the backoff terminate in the unigram?	in referring to it as S: S (w wi1) otherwise The backoff terminates in the unigram, which has probability S = count et al.		1.3511185592149868
Is what convenient to describe how a smoothing algorithm affects the numerator?	Instead of changing both the numerator and denominator, it is convenient to describe how a smoothing algorithm affects the numerator, by defining an adjusted count c.		1.3500493561323705
Do 3. 3 shows random sentences generated from unigram, bigram, trigram, and 4-gram To devset swallowed confess hear both?	3. 3 shows random sentences generated from unigram, bigram, trigram, and 4-gram To him swallowed confess hear both.		1.3490965608199477
What does 3. 8 suggest in?	3. 8 suggests that in practice this discount is actually a good one for bigrams with counts 2 through 9.	that in practice this discount is actually a good one for bigrams with counts 2 through 9	1.3484711235832167
Is KenLM optimized for speed and memory size?	SRILM offers a wider range of options and types of discounting, while KenLM is optimized for speed and memory size, making it possible to build web-scale language models.		1.3478605776220371
What what are captured in these bigram statistics?	What kinds of linguistic phenomena are captured in these bigram statistics?	kinds of linguistic phenomena	1.3458098398148488
What below shows the perplexity of a 1. 5 million word WSJ test set according to each of these grammars?	The table below shows the perplexity of a 1. 5 million word WSJ test set according to each of these grammars.	the table	1.3447616083527025
When will be zero?	We should expect the perplexity of this test set to be lower since most of the time the next number will be zero, which is very predictable, i. e. Thus, although the branching factor is still 10, the perplexity or weighted branching factor is smaller.	most of the time the next number	1.3431170470151161
What might we have for any n-gram that occurred a sufficient number of times?	For any n-gram that occurred a sufficient number of times, we might have a good estimate of its probability.	a good estimate of its probability	1.342617154306438
Do such AAC devices present ways to modify the MLE estimates slightly to get better probability estimates in Section 3. 4?	We present ways to modify the MLE estimates slightly to get better probability estimates in Section 3. 4.		1.3419338109736327
What do 3. 3 shows random sentences generated from unigram, bigram, trigram, and 4-gram To him swallowed confess hear?	3. 3 shows random sentences generated from unigram, bigram, trigram, and 4-gram To him swallowed confess hear both.	both	1.3410156424143125
Can such AAC devices compare the performance of two language models by seeing which gives the more accurate transcription for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.		1.3401015275824912
Is the simplest way to do smoothing to add one to all the bigram counts?	The simplest way to do smoothing is to add one to all the bigram counts, before we normalize them into probabilities.		1.337818185559104
What do we need for an intrinsic evaluation of a language model?	For an intrinsic evaluation of a language model we need a test set.	a test set	1.3339408359724647
What does most of what we will use entropy for involve?	But most of what we will use entropy for involves sequences.	sequences	1.3338311872807287
What cannot divide by 0?	Thus if some words have zero probability, we cannot compute perplexity at all, since we cannot divide by 0!	these zeros-- things that do not ever occur in the training set but do occur in the test set--	1.3333836289215455
Would speech recognition be nice to have a metric that can be used to quickly evaluate potential improvements in a language model?	Instead, it would be nice to have a metric that can be used to quickly evaluate potential improvements in a language model.		1.333288932370857
Who can such AAC devices use two pseudo-words for the first trigram always represent and compute language model probabilities in log format as log probabilities for example?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.	We	1.3314410152766594
What is one implication of this that the probabilities often encode specific facts about?	One implication of this is that the probabilities often encode specific facts about a given training corpus.	about a given training corpus	1.331425760296217
Who can we use two pseudo-words for the first trigram always represent and compute language model probabilities in log format as log probabilities for example?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.	We	1.331170223867928
What did we then compute on a test set of 1. 5 million words with Eq?	We then computed the perplexity of each of these models on a test set of 1. 5 million words with Eq.	the perplexity of each of these models	1.3294929027453142
What can we compare by running the speech recognizer twice for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	the performance of two language models	1.3271054344550732
Is the test set normalized by the number of words?	The perplexity (sometimes called PP for short) of a language model on a test set is the inverse probability of the test set, normalized by the number of words.		1.3259894769510456
What was incremented?	Since there are V words in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra V observations.	each one	1.3239495490472994
What does KenLM make it possible to build?	SRILM offers a wider range of options and types of discounting, while KenLM is optimized for speed and memory size, making it possible to build web-scale language models.	web-scale language models	1.323009518060994
Does the sharp change in counts and probabilities occur because too much probability mass is moved to all the zeros?	The sharp change in counts and probabilities occurs because too much probability mass is moved to all the zeros.		1.3224106569842005
What do our statistical models only give to the correct distributions and entropies of natural language?	Thus, our statistical models only give an approximation to the correct distributions and entropies of natural language.	an approximation	1.3208198538730174
What Eq?	By Eq.	by	1.3180560853108008
Are Chen finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq?	We are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq.		1.3171121101830234
What can these adjusted counts be computed by?	These adjusted counts can be computed by Eq.	by Eq	1.316825667517446
What does add-k smoothing require that we have?	Add-k smoothing requires that we have a method for choosing k; this can be done, for example, by optimizing on a devset.	a method for choosing k	1.315293618372122
Who also need to adjust the denominator to take into account the extra V observations?	Since there are V words in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra V observations.	we	1.314779950055367
Was output then hand-corrected for capitalization to improve readability?	Output was then hand-corrected for capitalization to improve readability.		1.3142468004663037
If a higher-order n-gram has a zero count, what do we simply backoff to?	If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed weight.	to a lower order n-gram	1.3140862388520027
Are n-grams stored in reverse tries?	Probabilities are generally quantized using only 4-8 bits (instead of 8-byte floats), and n-grams are stored in reverse tries.		1.3138623828777325
What is useful when we do not know the actual probability distribution p that generated some data?	The cross-entropy is useful when we do not know the actual probability distribution p that generated some data.	the cross-entropy	1.3134172743425307
What showed?	3. 15 showed, perplexity is related inversely to the likelihood of the test sequence according to the model).	3. 15	1.3121372714891912
What did we next choose starting with w (again?	We next chose a random bigram starting with w (again, drawn according to its To give an intuition for the increasing power of higher-order n-grams, Fig.	a random bigram	1.311689277338996
Is perplexity based on the inverse probability of the test set by definition?	By definition, perplexity is based on the inverse probability of the test set.		1.3111605950867764
What is still not sufficient?	beta get ur IT placement wiv twitter Matching genres and dialects is still not sufficient.	beta get ur a very large corpus placement wiv twitter Matching genres and dialects	1.3104903143624869
Is the cross-entropy useful when Chen do not know the actual probability distribution p that generated some data?	The cross-entropy is useful when we do not know the actual probability distribution p that generated some data.		1.3082319787327483
What will not always be able to count entire sentences?	This is because language is creative; new sentences are created all the time, and we will not always be able to count entire sentences.	such AAC devices	1.305798920363919
Do Chen show in Chapter 12?	But natural language is not stationary, since as we show in Chapter 12, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent.		1.305115131778904
Can Chen compute the entropy of some stochastic process by computing a very long sample of the output's average log probability?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.		1.3018464542429373
What do we call the initial test set the development test set or, devset in?	In such cases, we call the initial test set the development test set or, devset.	in such cases	1.3009678821582935
Who print the word whose interval includes this chosen value?	We choose a random value between 0 and 1 and print the word whose interval includes this chosen value.	we	1.2989728836351464
Is the n-gram model like many statistical models?	The n-gram model, like many statistical models, is dependent on the training corpus.		1.2964878808634355
Who do not use raw probability as our metric for evaluating language models, but a variant called perplexity in practice?	In practice we do not use raw probability as our metric for evaluating language models, but a variant called perplexity.	we	1.2961969061520535
Is beta get ur a very large corpus placement wiv twitter Matching genres and dialects still not sufficient?	beta get ur IT placement wiv twitter Matching genres and dialects is still not sufficient.		1.2953300034506396
Who saw above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code?	We saw above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code, so the average was 3.	we	1.2950879978315037
Who can generalize the bigram to the trigram?	We can generalize the bigram (which looks one word into the past) to the trigram (which looks two words into the past) and thus to the n-gram (which looks n 1 words into the past).	we	1.2935901326257073
Do such AAC devices choose a random value between 0 and 1?	We choose a random value between 0 and 1 and print the word whose interval includes this chosen value.		1.290652565610153
Might 3. 3 have a good estimate of words's probability for any n-gram that occurred a sufficient number of times?	For any n-gram that occurred a sufficient number of times, we might have a good estimate of its probability.		1.289006287145078
What call this situation training on the test set?	We call this situation training on the test set.	such AAC devices	1.2884467487761384
What set?	3. 11 Add an option to your program to compute the perplexity of a test set.	3. 11	1.2883836493164567
Was engineering linguistic, and psychological work on modeling word sequences?	Based on Shannon's work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s.		1.2856038800320468
What am I Sam corpus Calculate?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.	the probability of the sentence i want chinese food	1.2834013208324317
Who continue generating words until we randomly generate the sentence-final token </s>?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.	we	1.2822682344305458
What is MLE?	But it is the probability that makes it most likely that Chinese will occur 400 times in a million-word corpus.	the probability that makes it most likely that Chinese will occur 400 times in a million-word corpus	1.2821353273173137
Is 3. 16 the entire sequence of words in some test set?	3. 16 is the entire sequence of words in some test set.		1.2812066032325817
Who 'll have to shave off a bit of probability mass from some more frequent events?	To keep a language model from assigning zero probability to these unseen events, we 'll have to shave off a bit of probability mass from some more frequent events and give it to the events we 've never seen.	we	1.280481961971764
What is one implication of this that the probabilities often encode?	One implication of this is that the probabilities often encode specific facts about a given training corpus.	specific facts about a given training corpus	1.2794783804034804
Who need a training corpus of legal documents?	To build a language model for translating legal documents, we need a training corpus of legal documents.	we	1.2790580818772828
What can we compute by taking a very long sample of the output?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.	the entropy of some stochastic process	1.278554988996969
What is what convenient to describe?	Instead of changing both the numerator and denominator, it is convenient to describe how a smoothing algorithm affects the numerator, by defining an adjusted count c.	how a smoothing algorithm affects the numerator	1.2782501126804178
Is Speech Recognition 450506?	and Lee, K. - F. , Readings in Speech Recognition, 450506.		1.277566312497541
Does p answer the question ``How likely''?	In other words, instead of P, which answers the question ``How likely is w?''		1.2765761030329061
What do our statistical models only give an approximation to?	Thus, our statistical models only give an approximation to the correct distributions and entropies of natural language.	to the correct distributions and entropies of natural language	1.2750011388948055
What are publicly available?	Both are publicly available.	both	1.2749533819855077
What will all the counts that used to be zero now have and so on?	All the counts that used to be zero will now have a count of 1, the counts of 1 will be 2, and so on.	a count of 1	1.2745753845211916
Were punctuation marks treated as words?	All characters were mapped to lower-case and punctuation marks were treated as words.		1.273916968421775
What will we formalize this intuition by introducing models that assign a probability to each possible next word in?	In the following sections we will formalize this intuition by introducing models that assign a probability to each possible next word.	in the following sections	1.2731709672696363
What is the branching factor?	We should expect the perplexity of this test set to be lower since most of the time the next number will be zero, which is very predictable, i. e. Thus, although the branching factor is still 10, the perplexity or weighted branching factor is smaller.	still 10	1.2720809836670492
What do we reach a In, we have to discount the higher-order n-grams to save some probability mass for the lower order n-grams?	We continue backing off until we reach a In order for a backoff model to give a correct probability distribution, we have to discount the higher-order n-grams to save some probability mass for the lower order n-grams.	In order for a backoff model to give a correct probability distribution	1.271044956370492
What allows us to use some m?	It A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allows us to use some m, which is a model of p (i. e. , an approximation to p).	a DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY	1.2702873826461953
What can the test set only contain from this lexicon in such a closed vocabulary system?	In such a closed vocabulary system the test set can only contain words from this lexicon, and there will be no unknown words.	words	1.268541575720485
Can such AAC devices compare the performance of two language models by running the speech recognizer twice for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.		1.2680002904886813
Did Chen show that caches and class-based models provided only minor additional improvement?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.		1.267856693404163
What would we want to pick at the minimum?	At the minimum, we would want to pick N- GRAM L ANGUAGE M ODELS the smallest test set that gives us enough statistical power to measure a statistically significant difference between two potential models.	N- GRAM L ANGUAGE M ODELS the smallest test set that gives us enough statistical power to measure a statistically significant difference between two potential models	1.2676426243488592
Is one step to be sure to use a training corpus that has a similar genre to whatever task 3. 3 are trying to accomplish?	One step is to be sure to use a training corpus that has a similar genre to whatever task we are trying to accomplish.		1.266576939353734
Do such AAC devices continue choosing random numbers?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.		1.2665226575221142
What is the Kneser-Ney intuition to base our estimate of PCONTINUATION on the number of different contexts?	The Kneser-Ney intuition is to base our estimate of PCONTINUATION on the number of different contexts word w has appeared in, that is, the number of bigram types it completes.	word w has appeared in	1.2661979636505825
What do we rely on a discounted probability P if we 've seen this n-gram before (i. e. , if we have non-zero counts in?	In Katz backoff we rely on a discounted probability P if we 've seen this n-gram before (i. e. , if we have non-zero counts).	in Katz backoff	1.2639229237606635
Who might expect some overlap between our n-grams for the two genres?	Shakespeare and the Wall Street Journal are both English, so we might expect some overlap between our n-grams for the two genres.	we	1.2636005392514633
What might we have a good estimate of for any n-gram that occurred a sufficient number of times?	For any n-gram that occurred a sufficient number of times, we might have a good estimate of its probability.	of its probability	1.2632197759303079
Does every word occur following every long string!	As we said above, we cannot just estimate by counting the number of times every word occurs following every long string, because language is creative and any particular context might have never occurred before!		1.2623959400599412
What leave this exact calculation as exercise 12?	We leave this exact calculation as exercise 12.	such AAC devices	1.2623217549283847
Was Mercer R.?	Originally distributed as IBM technical report in Jelinek, F. and Mercer, R. .		1.2618356094471541
Who introduced perplexity in Section 3. 2. 1 as a way to evaluate n-gram models on a test set?	We introduced perplexity in Section 3. 2. 1 as a way to evaluate n-gram models on a test set.	we	1.2604450890523862
If we have no examples of a particular trigram wn2 wn1 wn, what can we instead estimate by using the bigram probability P?	If we are trying to compute P (wn wn2 wn1) but we have no examples of a particular trigram wn2 wn1 wn, we can instead estimate its probability by using the bigram probability P (wn wn1).	its probability	1.2588909937932855
What is it generally represented in as a 64-bit hash number?	Rather than store each word as a string, it is generally represented in memory as a 64-bit hash number, with the words themselves stored on disk.	in memory	1.2558572717371441
What is to be sure to use a training corpus that has a similar genre to whatever task we are trying to accomplish?	One step is to be sure to use a training corpus that has a similar genre to whatever task we are trying to accomplish.	one step	1.255583518889376
Does kenLM: Faster and smaller language model query?	KenLM: Faster and smaller language model queries.		1.253677711937721
What did we introduce perplexity in Section 3. 2. 1 as?	We introduced perplexity in Section 3. 2. 1 as a way to evaluate n-gram models on a test set.	as a way to evaluate n-gram models on a test set	1.2497955042872477
Would the sentence probabilities for all sentences of a given length sum to one without an end-symbol?	Without an end-symbol, the sentence probabilities for all sentences of a given length would sum to one.		1.2497107287383638
Were all characters mapped to lower-case?	All characters were mapped to lower-case and punctuation marks were treated as words.		1.2463936441568357
What do such AAC devices get?	By using log probabilities instead of raw probabilities, we get numbers that are not as small.	numbers that are not as small	1.2463753253192498
What are probabilities to?	Since probabilities are (by definition) less than or equal to 1, the more probabilities we multiply together, the smaller the product becomes.	less than or equal to 1	1.2461660501793845
What 'll we use?	We 'll use data from the now-defunct Berkeley Restaurant Project, a dialogue system from the last century that answered questions about a database of restaurants in Berkeley, California (Jurafsky et al. , 1994).	data from the now-defunct Berkeley Restaurant Project	1.2454260341793497
Can n-gram probabilities be estimated by normalizing?	n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).		1.2448518087976708
What do such AAC devices leave this exact calculation as?	We leave this exact calculation as exercise 12.	as exercise 12	1.2429926219998273
Who call this situation training on the test set?	We call this situation training on the test set.	we	1.242978432611268
Who did they show the advantages of?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.	of Modified Interpolated KneserNey	1.2414898213180505
What did we introduce perplexity in as a way to evaluate n-gram models on a test set?	We introduced perplexity in Section 3. 2. 1 as a way to evaluate n-gram models on a test set.	in Section 3. 2. 1	1.2409107261320713
What rely on a discounted probability P if we 've seen this n-gram before (i. e. , if we have non-zero counts in Katz backoff?	In Katz backoff we rely on a discounted probability P if we 've seen this n-gram before (i. e. , if we have non-zero counts).	these zeros-- things that do not ever occur in the training set but do occur in the test set--	1.2383506996567721
What is limited?	But because any corpus is limited, some perfectly acceptable English word sequences are bound to be missing from it.	any corpus	1.2362281426327926
Who can we use for the first trigram We always represent and compute language model probabilities in log format as log probabilities for example?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.	two pseudo-words	1.2325477681023318
Who can compute the entropy of some stochastic process by taking a very long sample of the output?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.	we	1.2263327460510354
Was add-one smoothing first applied as an engineering solution to the zero-frequency problem by Jeffreys based on an earlier Add-K suggestion by Johnson?	Add-one smoothing derives from Laplace's 1812 law of succession and was first applied as an engineering solution to the zero-frequency problem by Jeffreys based on an earlier Add-K suggestion by Johnson.		1.225673242626272
What gaze or other specific movements to select words from a menu to be spoken by the system?	People often use such AAC devices if they are physically unable or sign but can instead using eye gaze or other specific movements to select words from a menu to be spoken by the system.	people often use such AAC devices if they are physically unable or sign but can instead using eye	1.2239699925657914
What can the test set only contain words from this lexicon in?	In such a closed vocabulary system the test set can only contain words from this lexicon, and there will be no unknown words.	in such a closed vocabulary system	1.2224338228433949
Do 3. 3 need a training corpus of questions?	To build a language model for a question-answering system, we need a training corpus of questions.		1.2202500697183067
What 'll we have to shave off from some more frequent events?	To keep a language model from assigning zero probability to these unseen events, we 'll have to shave off a bit of probability mass from some more frequent events and give it to the events we 've never seen.	a bit of probability mass	1.2192779245951133
Is perplexity commonly used as a quick check on an algorithm?	Nonetheless, because perplexity often correlates with such improvements, it is commonly used as a quick check on an algorithm.		1.21880853647637
Who are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq?	We are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq.	we	1.2184605918028693
What do we need a test set for?	For an intrinsic evaluation of a language model we need a test set.	for an intrinsic evaluation of a language model	1.2184311560510075
Who leave this exact calculation as exercise 12?	We leave this exact calculation as exercise 12.	we	1.2162656649453578
Can (The cross-entropy never be lower than the true entropy? )	(The cross-entropy can never be lower than the true entropy, so a model cannot err by underestimating the true entropy. )		1.2130732682146772
What did a bigram that occurred 4 times in the first 22 million words occur 3. 23 times in on average?	On average, a bigram that occurred 4 times in the first 22 million words occurred 3. 23 times in the next 22 million words.	in the next 22 million words	1.2116737809636804
Do 3. 3 need a training corpus of legal documents?	To build a language model for translating legal documents, we need a training corpus of legal documents.		1.2108995207087179
Was Computer Speech & Language 21?	Computer Speech & Language, 21,492518.		1.2086787836206576
What 'll we have to shave off a bit of from some more frequent events?	To keep a language model from assigning zero probability to these unseen events, we 'll have to shave off a bit of probability mass from some more frequent events and give it to the events we 've never seen.	of probability mass	1.2070373068564684
What can we compare the performance of by seeing which gives the more accurate transcription for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	of two language models	1.207003042079434
What do such AAC devices use?	Sometimes we use a particular test set so often that we implicitly tune to its characteristics.	a particular test set so often that we implicitly tune to a bias that makes the probabilities all look too high, and causes huge inaccuracies in perplexity, the probability-based metric's characteristics	1.2068318969759988
Does modified Kneser-Ney use three different discounts d1, d2, and d3?	Rather than use a single fixed discount d, modified Kneser-Ney uses three different discounts d1, d2, and d3 + for n-grams with counts of 1,2 and three or more, respectively.		1.2054317421781513
What did convert in the training set any word that is not in this set to the unknown word token <UNK> in?	Convert in the training set any word that is not in this set (any OOV word) to the unknown word token <UNK> in a text normalization step.	in a text normalization step	1.2025007533083305
Who choose a random value between 0 and 1?	We choose a random value between 0 and 1 and print the word whose interval includes this chosen value.	we	1.2012102378757716
What does 3. 8 suggest that in practice this discount is actually?	3. 8 suggests that in practice this discount is actually a good one for bigrams with counts 2 through 9.	a good one for bigrams with counts 2 through 9	1.1998428355357795
What are stationary?	Markov models, and hence n-grams, are stationary.	markov models, and hence n-grams,	1.1992664735519336
What did we then compute the perplexity of on a test set of 1. 5 million words with Eq?	We then computed the perplexity of each of these models on a test set of 1. 5 million words with Eq.	of each of these models	1.1961697856318607
Who do not know any way to compute the exact probability of a word given a long sequence of preceding words)?	We do not know any way to compute the exact probability of a word given a long sequence of preceding words, 1).	we	1.1959391231254775
Is one alternative to add-one smoothing to move a bit less of the probability mass from the seen to the unseen events?	One alternative to add-one smoothing is to move a bit less of the probability mass from the seen to the unseen events.		1.1954453450173665
What can we then measure by its performance on some unseen data called the test set or test corpus?	We can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus.	the quality of an n-gram model	1.1938196754714454
What do we need a training corpus of?	To build a language model for translating legal documents, we need a training corpus of legal documents.	of legal documents	1.1924217932940695
What are one?	Give two probabilities, one using Fig.	two probabilities	1.1897416951175943
Who use the trigram if the evidence is sufficient?	In backoff, we use the trigram if the evidence is sufficient, otherwise we use the bigram, otherwise the unigram.	we	1.183275918003487
What do we leave this exact calculation as?	We leave this exact calculation as exercise 12.	as exercise 12	1.182936954485461
What was drawn according to its To give an intuition for the increasing power of higher-order n-grams, Fig?	We next chose a random bigram starting with w (again, drawn according to its To give an intuition for the increasing power of higher-order n-grams, Fig.	again	1.181101686079351
May even simple extensions of the example sentence have counts of zero on the web?	Even simple extensions of the example sentence may have counts of zero on the web (such as ``Walden Pond's water is so transparent that the''; well, used to have counts of zero).		1.1791829979016772
Is one of the most commonly used and best performing n-gram smoothing methods the interpolated Kneser-Ney algorithm?	One of the most commonly used and best performing n-gram smoothing methods is the interpolated Kneser-Ney algorithm (Kneser and Ney 1995, Chen and Goodman 1998).		1.1766293381276207
Is 844 000 possible bigrams?	There are V 2 = 844,000,000 possible bigrams alone, and the number of possible 4-grams is V 4 = 7 1017.		1.1750063579846293
What see above?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.	such AAC devices	1.1740581645155397
Are Shakespeare and the Wall Street Journal both English?	Shakespeare and the Wall Street Journal are both English, so we might expect some overlap between our n-grams for the two genres.		1.1722310444418103
Does such end-to-end evaluation's important not to let the test sentences into the training set?	Since our evaluation metric is based on test set probability, it's important not to let the test sentences into the training set.		1.17106911961629
What is?	That is, we 'll have many cases of putative ``zero probability n-grams'' that should really have some non-zero probability.	that	1.1695804729248622
What can we compare the performance of two language models by seeing which gives the more accurate transcription for?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	for speech recognition	1.16908825899044
Who see in Section 3. 7 that perplexity is also closely related to the informationtheoretic notion of entropy?	We see in Section 3. 7 that perplexity is also closely related to the informationtheoretic notion of entropy.	we	1.1666528038947357
Does the length of the observed word sequence go to infinity?	Cross-entropy is defined in the limit, as the length of the observed word sequence goes to infinity.		1.1656319574100285
What does (What happen to if we do not increase the denominator? )	(What happens to our P values if we do not increase the denominator? )	to these zeros-- things that do not ever occur in the training set but do occur in the test set--'s P values	1.163679090433296
Who need a training corpus of questions?	To build a language model for a question-answering system, we need a training corpus of questions.	we	1.162609176762348
What are Markov models the class of that assume we can predict the probability of some future unit without looking too far into the past?	Markov models are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past.	of probabilistic models	1.1614760024210256
Who will formalize this intuition by introducing models that assign a probability to each possible next word in the following sections?	In the following sections we will formalize this intuition by introducing models that assign a probability to each possible next word.	we	1.1608106948371741
Is instead of P w in other words?	In other words, instead of P, which answers the question ``How likely is w?''		1.1605926280981964
Was Bell System Technical Journal 27?	Bell System Technical Journal, 27,379423.		1.1600462501923285
What is it commonly used as a quick check on?	Nonetheless, because perplexity often correlates with such improvements, it is commonly used as a quick check on an algorithm.	on an algorithm	1.1594091686681107
Do such AAC devices want our test set to be as large as possible?	We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible.		1.1586641176512447
What is it generally represented in memory as?	Rather than store each word as a string, it is generally represented in memory as a 64-bit hash number, with the words themselves stored on disk.	as a 64-bit hash number	1.1580510939157849
Can the probability of upcoming words be dependent on events that were arbitrarily distant and time dependent?	But natural language is not stationary, since as we show in Chapter 12, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent.		1.1569016101268454
What do these zeros-- things that do not ever occur in the training set but do occur in the test set-- add?	Instead of adding 1 to each count, we add a fractional count k (. 05?	a fractional count k (. 05	1.1550473063193234
Are these papers recommended for any reader with further Two commonly used toolkits for building language models are SRILM and KenLM (Heafield 2011?	These papers are recommended for any reader with further Two commonly used toolkits for building language models are SRILM (Stolcke, 2002) and KenLM (Heafield 2011, Heafield et al.		1.1545983643696407
Is cross-entropy defined in the limit?	Cross-entropy is defined in the limit, as the length of the observed word sequence goes to infinity.		1.1512611896967693
What did convert in the training set any word that is not in this set to in a text normalization step?	Convert in the training set any word that is not in this set (any OOV word) to the unknown word token <UNK> in a text normalization step.	to the unknown word token <UNK>	1.1501324478659103
Was again drawn according to its To give an intuition for the increasing power of higher-order n-grams, Fig?	We next chose a random bigram starting with w (again, drawn according to its To give an intuition for the increasing power of higher-order n-grams, Fig.		1.149895816654019
Does model assign a higher probability to the test set?	The answer is simple: whichever model assigns a higher probability to the test set-- meaning it more accurately predicts the test set-- is a better model.		1.1491597030233733
What can we compare the performance of by running the speech recognizer twice for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	of two language models	1.148345269405275
Was Bell System Technical Journal 30?	Bell System Technical Journal, 30,5064.		1.146510597312385
Who would have to get the count of its water is so transparent and divide by the sum of the counts of all possible five word sequences?	We would have to get the count of its water is so transparent and divide by the sum of the counts of all possible five word sequences.	we	1.1453022466935479
What can we compute the entropy of by taking a very long sample of the output?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.	of some stochastic process	1.1452079722293576
Do such AAC devices combine log probabilities by adding them?	E VALUATING L ANGUAGE M ODELS Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them.		1.1447156753250645
Who can compare the performance of two language models by running the speech recognizer twice for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	we	1.1444577107654696
Who does their presence mean are underestimating the probability of all sorts of words that might occur?	First, their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data.	we	1.1385965415083703
What can we compute the entropy of by computing its average log probability?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.	of some stochastic process	1.1358594956596457
What will we formalize by introducing models that assign a probability to each possible next word in the following sections?	In the following sections we will formalize this intuition by introducing models that assign a probability to each possible next word.	this intuition	1.134138988189243
What said above?	As we said above, we cannot just estimate by counting the number of times every word occurs following every long string, because language is creative and any particular context might have never occurred before!	such AAC devices	1.1337114197269946
What can we compute by computing its average log probability?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.	the entropy of some stochastic process	1.1328466116307752
What can we generalize the bigram to?	We can generalize the bigram (which looks one word into the past) to the trigram (which looks two words into the past) and thus to the n-gram (which looks n 1 words into the past).	to the trigram	1.1322990194393947
Was Information Theory 37?	IEEE Transactions on Information Theory, 37,10851094.		1.1319915761453845
Do such AAC devices not use raw probability as our metric for evaluating language models, but a variant called perplexity in practice?	In practice we do not use raw probability as our metric for evaluating language models, but a variant called perplexity.		1.1309604868293577
What can we generalize to the trigram?	We can generalize the bigram (which looks one word into the past) to the trigram (which looks two words into the past) and thus to the n-gram (which looks n 1 words into the past).	the bigram	1.1303541770827659
Who saw it in Eq?	We are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq.	we	1.1294922624896038
What did they compute a bigram grammar from?	They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words.	from 22 million words of AP newswire	1.1294437039981085
What do we use if the evidence is sufficient?	In backoff, we use the trigram if the evidence is sufficient, otherwise we use the bigram, otherwise the unigram.	the trigram	1.1292966962938484
Does the corpus contain V word types?	The corpus contains V word types.		1.1291300447122126
Is the EM algorithm an iterative?	There are various ways to find this optimal set of s. One way is to use the EM algorithm, an iterative learning algorithm that converges on locally optimal s (Jelinek and Mercer, 1980).		1.129100349193891
What do such AAC devices see in?	We see in Section 3. 7 that perplexity is also closely related to the informationtheoretic notion of entropy.	in Section 3. 7 that perplexity is also closely related to the informationtheoretic notion of entropy	1.128864191378639
Who proceed to train the language model as before in either case?	In either case we then proceed to train the language model as before, treating <UNK> like a regular word.	we	1.1246732238538555
Does the word glasses have a much wider distribution?	The word glasses has a much wider distribution.		1.124301662594998
What can we use two pseudo-words for the first trigram We always represent and compute language model probabilities in log format as log probabilities for?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.	for example	1.1237127819962018
Are probabilities generally quantized using only 4-8 bits?	Probabilities are generally quantized using only 4-8 bits (instead of 8-byte floats), and n-grams are stored in reverse tries.		1.1211959181068702
What means we are underestimating the probability of all sorts of words that might occur?	First, their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data.	these zeros-- things that do not ever occur in the training set but do occur in the test set--'s presence	1.1191664545729934
What did they perform?	They performed a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters.	a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters	1.1187788892192274
Would a matrix selected from a random set of seven words be even more sparse?	In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse.		1.1173794037103795
What is it commonly used as on an algorithm?	Nonetheless, because perplexity often correlates with such improvements, it is commonly used as a quick check on an algorithm.	as a quick check	1.1173023763820296
Is such AAC devices's evaluation metric based on test set probability?	Since our evaluation metric is based on test set probability, it's important not to let the test sentences into the training set.		1.1145270398362372
What did they perform a number of comparing different discounting algorithms, cache models, class-based models, and other language model parameters?	They performed a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters.	of carefully controlled experiments	1.1129479438146967
What can we then measure the quality of an n-gram model by on some unseen data called the test set or test corpus?	We can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus.	by its performance	1.1081066175883265
What can we compare the performance of two language models by running the speech recognizer twice for?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	for speech recognition	1.1071298396572988
Can a model not err by underestimating the true entropy?	(The cross-entropy can never be lower than the true entropy, so a model cannot err by underestimating the true entropy. )		1.1055303162305332
If Chen use log base 2, will the resulting value of entropy be measured in bits?	If we use log base 2, the resulting value of entropy will be measured in bits.		1.102871800219471
Is Entropy a measure of information?	Entropy is a measure of information.		1.102720656478715
What come from the corpus with many of the statistical models in our field?	As with many of the statistical models in our field, the probabilities of an n-gram model come from the corpus it is trained on, the training set or training corpus.	the probabilities of an n-gram model	1.1002348942767761
Do no overlap in generated sentences?	While they both model ``English-like sentences'', there is clearly no overlap in generated sentences, and little overlap even in small phrases.		1.100215501368809
Are Chen ready to introduce cross-entropy?	Now we are ready to introduce cross-entropy.		1.0975573380941013
What did they then check in another 22 million words?	They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words.	the counts of each of these bigrams	1.096344644872632
What do we proceed to train the language model as before in?	In either case we then proceed to train the language model as before, treating <UNK> like a regular word.	in either case	1.0949412638909108
Is the m a model of p?	It A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allows us to use some m, which is a model of p (i. e. , an approximation to p).		1.0930841371171036
Was w drawn according to again's To give an intuition for the increasing power of higher-order n-grams, Fig?	We next chose a random bigram starting with w (again, drawn according to its To give an intuition for the increasing power of higher-order n-grams, Fig.		1.0916425607736104
Is such end-to-end evaluation called extrinsic evaluation?	Such end-to-end evaluation is called extrinsic evaluation.		1.0900343335261278
What do such AAC devices introduce in this chapter?	In this chapter we introduce the simplest model that assigns probabilities to sentences and sequences of words, the n-gram.	the simplest model that assigns probabilities to sentences and sequences of words	1.0876499912865354
What do we proceed to train as before in either case?	In either case we then proceed to train the language model as before, treating <UNK> like a regular word.	the language model	1.0874903341467652
Might sentences and sequences of words, the n-gram expect some overlap between our n-grams for the two genres?	Shakespeare and the Wall Street Journal are both English, so we might expect some overlap between our n-grams for the two genres.		1.0861713537724844
What do we not use raw probability as for evaluating language models, but a variant called perplexity in practice?	In practice we do not use raw probability as our metric for evaluating language models, but a variant called perplexity.	as our metric	1.0851177068663715
Are markov models, and hence n-grams, stationary?	Markov models, and hence n-grams, are stationary.		1.0835146658985386
What are these papers recommended for any reader with further Two commonly used toolkits for building?	These papers are recommended for any reader with further Two commonly used toolkits for building language models are SRILM (Stolcke, 2002) and KenLM (Heafield 2011, Heafield et al.	language models are SRILM and KenLM (Heafield 2011	1.0833924852268586
Is the MLE of its probability 1000000 or?	The MLE of its probability is 1000000 or.		1.0813267269748286
If we do not have counts to compute P what can we look to?	Similarly, if we do not have counts to compute P (wn wn1), we can look to the unigram P (wn).	to the unigram P	1.0809372985953223
Who can compare the performance of two language models by seeing which gives the more accurate transcription for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	we	1.07895529328641
Do such AAC devices just divide our data into 80 % training, 10 % development, and 10 % test in practice?	In practice, we often just divide our data into 80 % training, 10 % development, and 10 % test.		1.0784002870900498
What do such AAC devices not use raw probability as for evaluating language models, but a variant called perplexity in practice?	In practice we do not use raw probability as our metric for evaluating language models, but a variant called perplexity.	as our metric	1.0774183489438398
Is perplexity the probability-based metric?	Training on the test set introduces a bias that makes the probabilities all look too high, and causes huge inaccuracies in perplexity, the probability-based metric we introduce below.		1.0757258197839419
Who showed that caches and class-based models provided only minor additional improvement?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.	they	1.0755083214900036
What suggests that in practice this discount is actually a good one for bigrams with counts 2 through 9?	3. 8 suggests that in practice this discount is actually a good one for bigrams with counts 2 through 9.	3. 8	1.0720677107815808
Is a very large corpus equally important to get training data in the appropriate dialect?	It is equally important to get training data in the appropriate dialect, especially when processing social media posts or spoken transcripts.		1.070202363610524
Do such AAC devices get numbers that are not as small?	By using log probabilities instead of raw probabilities, we get numbers that are not as small.		1.0701475162656058
What 'll we follow?	This algorithm does not produce a probability distribution, so we 'll follow Brants et al.	Brants et al	1.069078854329017
Do such AAC devices see in Section 3. 7 that perplexity is also closely related to the informationtheoretic notion of entropy?	We see in Section 3. 7 that perplexity is also closely related to the informationtheoretic notion of entropy.		1.0661851144135848
Who add a fractional count k (. 05?	Instead of adding 1 to each count, we add a fractional count k (. 05?	we	1.0646165952089377
What is creative?	This is because language is creative; new sentences are created all the time, and we will not always be able to count entire sentences.	language	1.0638678589252237
Who can see how much a smoothing algorithm has changed the original counts?	It is often convenient to reconstruct the count matrix so we can see how much a smoothing algorithm has changed the original counts.	we	1.0630213501369359
Who want our test set to be as large as possible?	We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible.	we	1.055426027977981
What 'll we have to shave off a bit of probability mass from?	To keep a language model from assigning zero probability to these unseen events, we 'll have to shave off a bit of probability mass from some more frequent events and give it to the events we 've never seen.	from some more frequent events	1.05522771289565
What do we need?	2 We need the end-symbol to make the bigram grammar a true probability distribution.	the end-symbol to make the bigram grammar a true probability distribution	1.0540456585504319
What will we be computing for a grammar for example?	For a grammar, for example, we will be computing the entropy of some sequence of words W = w0, w1, w2, . . . , wn.	the entropy of some sequence of words W = w0 w2, . . .	1.0499071660187203
What occurred 3. 23 times in the next 22 million words on average?	On average, a bigram that occurred 4 times in the first 22 million words occurred 3. 23 times in the next 22 million words.	a bigram that occurred 4 times in the first 22 million words	1.0478791316180225
What did such AAC devices then compute the perplexity of each of these models on with Eq?	We then computed the perplexity of each of these models on a test set of 1. 5 million words with Eq.	on a test set of 1. 5 million words	1.0475672664895455
If we are trying to compute P, who can instead estimate its probability by using the bigram probability P?	If we are trying to compute P (wn wn2 wn1) but we have no examples of a particular trigram wn2 wn1 wn, we can instead estimate its probability by using the bigram probability P (wn wn1).	we	1.0467460534881807
What did they then check the counts of in another 22 million words?	They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words.	of each of these bigrams	1.0463530239790617
What we only ``back off'' to a lower-order n-gram if we have zero evidence for a higher-order n-gram in?	In other words, we only ``back off'' to a lower-order n-gram if we have zero evidence for a higher-order n-gram.	in other words	1.0443298950325057
What would this model define?	This model would define an infinite set of probability distributions, with one distribution per sentence length.	an infinite set of probability distributions	1.0364349646157582
If we use log base 2, what will be measured in bits?	If we use log base 2, the resulting value of entropy will be measured in bits.	the resulting value of entropy	1.0334341995836085
What are bound to be missing from it?	But because any corpus is limited, some perfectly acceptable English word sequences are bound to be missing from it.	some perfectly acceptable English word sequences	1.03238497303383
Do such AAC devices also want as much training data as possible?	We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible.		1.0260318753379307
Is too much probability mass moved to all the zeros?	The sharp change in counts and probabilities occurs because too much probability mass is moved to all the zeros.		1.0245335170365868
Do such AAC devices usually drop the word ``model''?	In a bit of terminological ambiguity, we usually drop the word ``model'', and thus the term ngram is used to mean either the word sequence itself or the predictive model that assigns it a probability.		1.0237812293372106
What did we then compute the perplexity of each of these models on with Eq?	We then computed the perplexity of each of these models on a test set of 1. 5 million words with Eq.	on a test set of 1. 5 million words	1.0177609319708463
What (happens to our P values if we do not increase the denominator? )	(What happens to our P values if we do not increase the denominator? )	What	1.0141196866458104
What is often convenient to reconstruct the count matrix so we can see how much a smoothing algorithm has changed the original counts?	It is often convenient to reconstruct the count matrix so we can see how much a smoothing algorithm has changed the original counts.	it	1.0129932917762998
What see the following test set: 0 0 0 0 0 3 0 0 0 0?	Now we see the following test set: 0 0 0 0 0 3 0 0 0 0.	such AAC devices	1.0065303765473512
What will we be computing the entropy of w0 w2, . . . for a grammar for example?	For a grammar, for example, we will be computing the entropy of some sequence of words W = w0, w1, w2, . . . , wn.	of some sequence of words W =	0.9940625720077276
Is the algorithm called stupid backoff?	The algorithm is called stupid backoff.		0.9914359129228052
What do we leave as exercise 12?	We leave this exact calculation as exercise 12.	this exact calculation	0.9898689063297457
'll we use data from the now-defunct Berkeley Restaurant Project?	We 'll use data from the now-defunct Berkeley Restaurant Project, a dialogue system from the last century that answered questions about a database of restaurants in Berkeley, California (Jurafsky et al. , 1994).		0.989662172028543
Do little overlap even in small phrases?	While they both model ``English-like sentences'', there is clearly no overlap in generated sentences, and little overlap even in small phrases.		0.9880556672448912
What we only ``back off'' to if we have zero evidence for a higher-order n-gram in other words?	In other words, we only ``back off'' to a lower-order n-gram if we have zero evidence for a higher-order n-gram.	to a lower-order n-gram	0.9879082667425718
What can such AAC devices use two pseudo-words for the first trigram in for example?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.	We always represent and compute language model probabilities in log format as log probabilities	0.9877069195598589
Is between 0 and 1 each word covering an interval proportional to log space's frequency?	Imagine all the words of the English language covering the probability space between 0 and 1, each word covering an interval proportional to its frequency.		0.9875153266600483
Will most of the time the next number be zero?	We should expect the perplexity of this test set to be lower since most of the time the next number will be zero, which is very predictable, i. e. Thus, although the branching factor is still 10, the perplexity or weighted branching factor is smaller.		0.9867212310508531
Is noisy ambiguous input?	Probabilities are essential in any task in which we have to identify words in noisy, ambiguous input, like speech recognition.		0.985895465935249
What is the intuition of the Shannon-McMillan-Breiman theorem that each of these shorter sequences will reoccur in according to their probabilities?	The intuition of the Shannon-McMillan-Breiman theorem is that a long-enough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence according to their probabilities.	in the longer sequence	0.9854251873202304
Is the probability distribution for words at time t the same as the probability distribution at time t + 1 in other words?	In other words, the probability distribution for words at time t is the same as the probability distribution at time t + 1.		0.98156170069107
What is each word covering an interval proportional to its frequency?	Imagine all the words of the English language covering the probability space between 0 and 1, each word covering an interval proportional to its frequency.	between 0 and 1	0.9805371764364168
Who combine log probabilities by adding them?	E VALUATING L ANGUAGE M ODELS Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them.	we	0.9783460232597687
Did we say above that this use of relative frequencies as a way to estimate probabilities is an example of maximum likelihood estimation or MLE?	We said above that this use of relative frequencies as a way to estimate probabilities is an example of maximum likelihood estimation or MLE.		0.975912406803984
What do we not use raw probability as our metric for evaluating language models, but a variant called perplexity in?	In practice we do not use raw probability as our metric for evaluating language models, but a variant called perplexity.	in practice	0.9757737639500179
If we are trying to compute P, what can we instead estimate by using the bigram probability P?	If we are trying to compute P (wn wn2 wn1) but we have no examples of a particular trigram wn2 wn1 wn, we can instead estimate its probability by using the bigram probability P (wn wn1).	its probability	0.9741071209187029
Will such AAC devices not always be able to count entire sentences?	This is because language is creative; new sentences are created all the time, and we will not always be able to count entire sentences.		0.9700718411839158
Do such AAC devices randomly generate the sentence-final token </s>?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.		0.9656912671508171
Who show in Chapter 12?	But natural language is not stationary, since as we show in Chapter 12, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent.	we	0.9639970972606859
What then computed the perplexity of each of these models on a test set of 1. 5 million words with Eq?	We then computed the perplexity of each of these models on a test set of 1. 5 million words with Eq.	such AAC devices	0.9626842437565994
What multiply together?	Since probabilities are (by definition) less than or equal to 1, the more probabilities we multiply together, the smaller the product becomes.	such AAC devices	0.9621257993249295
Did Mind 421423?	Mind, 41,421423.		0.9599131527503921
Did such AAC devices say above that this use of relative frequencies as a way to estimate probabilities is an example of maximum likelihood estimation or MLE?	We said above that this use of relative frequencies as a way to estimate probabilities is an example of maximum likelihood estimation or MLE.		0.9588507923106064
Are preceding words 1?	We do not know any way to compute the exact probability of a word given a long sequence of preceding words, 1).		0.9587844494945741
What do we not use as our metric for evaluating language models, but a variant called perplexity in practice?	In practice we do not use raw probability as our metric for evaluating language models, but a variant called perplexity.	raw probability	0.9535977058518681
What can we use two pseudo-words for the first trigram in for example?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.	We always represent and compute language model probabilities in log format as log probabilities	0.9509048843410299
Might some even be cultural rather than linguistic?	And some might even be cultural rather than linguistic, like the higher probability that people are looking for Chinese versus English food.		0.948275952507319
What might we expect overlap between our n-grams for the two genres?	Shakespeare and the Wall Street Journal are both English, so we might expect some overlap between our n-grams for the two genres.	some	0.9473015692441977
Are probabilities less than or equal to 1?	Since probabilities are (by definition) less than or equal to 1, the more probabilities we multiply together, the smaller the product becomes.		0.9464981341715619
What did we see in Eq?	We are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq.	it	0.9463149834007132
Who have a language task in which this cannot happen because we know all the words that can occur?	Sometimes we have a language task in which this cannot happen because we know all the words that can occur.	we	0.9461971078759392
What do we continue generating until we randomly generate the sentence-final token </s>?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.	words	0.946189120718423
Was Language 189198?	In Oostdijk, N. and de Haan, P. , Corpus-Based Research into Language, 189198.		0.9413069719322189
Who usually drop the word ``model''?	In a bit of terminological ambiguity, we usually drop the word ``model'', and thus the term ngram is used to mean either the word sequence itself or the predictive model that assigns it a probability.	we	0.9402916964139023
Are new sentences created all the time?	This is because language is creative; new sentences are created all the time, and we will not always be able to count entire sentences.		0.9400249309019071
Who will be computing the entropy of some sequence of words W = w0 w2, . . . for a grammar for example?	For a grammar, for example, we will be computing the entropy of some sequence of words W = w0, w1, w2, . . . , wn.	we	0.9389179472749429
Who hypothesize that words that have appeared in more contexts in the past are more likely to appear in some new context as well?	We hypothesize that words that have appeared in more contexts in the past are more likely to appear in some new context as well.	we	0.9381898141288036
What did they show?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.	the advantages of Modified Interpolated KneserNey	0.9362238833890506
If we use log base 2, what will the resulting value of entropy be measured in?	If we use log base 2, the resulting value of entropy will be measured in bits.	in bits	0.930112542847852
What can we then measure the quality of by its performance on some unseen data called the test set or test corpus?	We can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus.	of an n-gram model	0.9284391155897869
What does their presence mean we are underestimating?	First, their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data.	the probability of all sorts of words that might occur	0.9273746983878792
Was the idea not written up until later?	A trigram model was used in the IBM TANGORA speech recognition system in the 1970s, but the idea was not written up until later.		0.9272110336409869
Who continue choosing random numbers?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.	we	0.9252865212794714
What will we be computing the entropy of some sequence of words W = w0 w2, . . . for for example?	For a grammar, for example, we will be computing the entropy of some sequence of words W = w0, w1, w2, . . . , wn.	for a grammar	0.9252461537552017
What will we be computing the entropy of some sequence of words W = w0 w2, . . . for a grammar for?	For a grammar, for example, we will be computing the entropy of some sequence of words W = w0, w1, w2, . . . , wn.	for example	0.9244480106057493
Thus if some words have zero probability, who cannot compute perplexity at all!	Thus if some words have zero probability, we cannot compute perplexity at all, since we cannot divide by 0!	we	0.922205544404842
Who know all the words that can occur?	Sometimes we have a language task in which this cannot happen because we know all the words that can occur.	we	0.9200752105395975
Who then computed the perplexity of each of these models on a test set of 1. 5 million words with Eq?	We then computed the perplexity of each of these models on a test set of 1. 5 million words with Eq.	we	0.9199868613058972
What are Statistical models likely to be pretty useless as predictors if the training sets and the test sets are as different as Shakespeare with?	Statistical models are likely to be pretty useless as predictors if the training sets and the test sets are as different as Shakespeare How should we deal with this problem when we build n-gram models?	How should 3. 3 deal with this problem when we build n-gram models	0.918306871627637
What can we use two pseudo-words for We always represent and compute language model probabilities in log format as log probabilities for example?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.	for the first trigram	0.9164820485988376
What are these zeros?	These zeros-- things that do not ever occur in the training set but do occur in the test set-- are a problem for two reasons.	a problem for two reasons	0.912291407655903
May a small test set be accidentally unrepresentative?	We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible.		0.9097619298948174
Are words the n-gram?	In this chapter we introduce the simplest model that assigns probabilities to sentences and sequences of words, the n-gram.		0.9092087877963067
Who get numbers that are not as small?	By using log probabilities instead of raw probabilities, we get numbers that are not as small.	we	0.9087218651352751
What might we have a good estimate of its probability for?	For any n-gram that occurred a sufficient number of times, we might have a good estimate of its probability.	for any n-gram that occurred a sufficient number of times	0.9083172482741917
What does this method of estimating probabilities directly from counts work fine in?	You should pause now, go to the web, and While this method of estimating probabilities directly from counts works fine in many cases, it turns out that even the web is not big enough to give us good estimates in most cases.	in many cases	0.9066543764002475
What might we expect some overlap between?	Shakespeare and the Wall Street Journal are both English, so we might expect some overlap between our n-grams for the two genres.	between our n-grams for the two genres	0.9056324690200663
What will hurt the performance of any application we want to run on this data?	First, their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data.	the sorts of words that might occur	0.9054060484967577
Does figure 3. 7 show the reconstructed counts?	Figure 3. 7 shows the reconstructed counts.		0.9053756309875984
What do we introduce in this chapter?	In this chapter we introduce the simplest model that assigns probabilities to sentences and sequences of words, the n-gram.	the simplest model that assigns probabilities to sentences and sequences of words	0.9040212356132027
Is 844 000?	There are V 2 = 844,000,000 possible bigrams alone, and the number of possible 4-grams is V 4 = 7 1017.		0.8987102803297369
What simply have never seen before?	But what about words we simply have never seen before?	these zeros-- things that do not ever occur in the training set but do occur in the test set--	0.8984727518749493
Are Markov models the class of probabilistic models that assume such AAC devices can predict the probability of some future unit without looking too far into the past?	Markov models are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past.		0.8978234559374427
What do the probabilities of an n-gram model come from with many of the statistical models in our field?	As with many of the statistical models in our field, the probabilities of an n-gram model come from the corpus it is trained on, the training set or training corpus.	from the corpus	0.8976119620014558
Who then checked the counts of each of these bigrams in another 22 million words?	They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words.	they	0.8945994629184584
What will be much more probable than Their are, and has improved than has improve?	The phrase There are will be much more probable than Their are, and has improved than has improve, allowing us to help users by detecting and correcting these errors.	the phrase There are	0.8900937182355906
What do we call in such cases?	In such cases, we call the initial test set the development test set or, devset.	the initial test set the development test set or, devset	0.8897298282761856
Is the bigram the unigram?	In backoff, we use the trigram if the evidence is sufficient, otherwise we use the bigram, otherwise the unigram.		0.8827587967535311
What turns out that even the web is not big enough to give us good estimates in most cases?	You should pause now, go to the web, and While this method of estimating probabilities directly from counts works fine in many cases, it turns out that even the web is not big enough to give us good estimates in most cases.	it	0.8765105385173131
What does their presence mean we are underestimating the probability of?	First, their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data.	of all sorts of words that might occur	0.874565640650663
Was Practice 381397?	In Gelsema, E. S. and Kanal, L. N. , Proceedings, Workshop on Pattern Recognition in Practice, 381397.		0.8736417907523719
What did they show that caches and class-based models provided?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.	only minor additional improvement	0.8687886128376818
What is only comparable if they An improvement in perplexity does not guarantee an improvement in the performance of a language processing task like speech recognition or machine translation?	The perplexity of two language models is only comparable if they An improvement in perplexity does not guarantee an improvement in the performance of a language processing task like speech recognition or machine translation.	the perplexity of two language models	0.868586160013826
What would we have to get is so transparent and divide by the sum of the counts of all possible five word sequences?	We would have to get the count of its water is so transparent and divide by the sum of the counts of all possible five word sequences.	the count of its water	0.866882354489289
Has the generator chosen the first 4-gram?	Thus, once the generator has chosen the first 4-gram (It cannot be but), there are only five possible continuations (that, I, he, thou, and so) ; indeed, for many 4-grams, there is only one continuation.		0.8657333617776093
What do we also need to adjust the denominator to take into account?	Since there are V words in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra V observations.	the extra V observations	0.864358962372533
What then checked the counts of each of these bigrams in another 22 million words?	They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words.	these zeros-- things that do not ever occur in the training set but do occur in the test set--	0.8581463494366484
Who just divide our data into 80 % training, 10 % development, and 10 % test in practice?	In practice, we often just divide our data into 80 % training, 10 % development, and 10 % test.	we	0.8570245498084628
What do we call?	We call this situation training on the test set.	this situation training on the test set	0.8507571622161549
Does absolute discounting formalize this intuition by subtracting a fixed discount d from each count?	Absolute discounting formalizes this intuition by subtracting a fixed discount d from each count.		0.8496492023600886
Who 'll have to give it to the events we 've never seen?	To keep a language model from assigning zero probability to these unseen events, we 'll have to shave off a bit of probability mass from some more frequent events and give it to the events we 've never seen.	we	0.8442854486897031
Is any corpus limited?	But because any corpus is limited, some perfectly acceptable English word sequences are bound to be missing from it.		0.8428703523612051
Will the phrase There are be much more probable than Their are, and has improved than has improve?	The phrase There are will be much more probable than Their are, and has improved than has improve, allowing us to help users by detecting and correcting these errors.		0.842238681699502
Did such AAC devices say above?	As we said above, we cannot just estimate by counting the number of times every word occurs following every long string, because language is creative and any particular context might have never occurred before!		0.8414481811983558
What does this method of estimating probabilities directly from counts turn out?	You should pause now, go to the web, and While this method of estimating probabilities directly from counts works fine in many cases, it turns out that even the web is not big enough to give us good estimates in most cases.	that even the web is not big enough to give such AAC devices good estimates in most cases	0.839699118755699
Is natural language not stationary?	But natural language is not stationary, since as we show in Chapter 12, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent.		0.8383902888668779
What is the entropy of the choice of horses then = log = 3 bits N- Until now we have been computing the entropy of a single variable?	The entropy of the choice of horses is then = log = 3 bits N- GRAM L ANGUAGE M ODELS Until now we have been computing the entropy of a single variable.	GRAM L ANGUAGE M ODELS	0.8200318278428154
Did they show the advantages of Modified Interpolated KneserNey?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.		0.8152193051542511
What do we continue choosing?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.	random numbers	0.8146272523016969
What did such AAC devices then compute the perplexity of each of these models on a test set of 1. 5 million words with?	We then computed the perplexity of each of these models on a test set of 1. 5 million words with Eq.	with Eq	0.8140844245457501
What did we then compute the perplexity of each of these models on a test set of 1. 5 million words with?	We then computed the perplexity of each of these models on a test set of 1. 5 million words with Eq.	with Eq	0.8137963463929119
What does this algorithm not produce?	This algorithm does not produce a probability distribution, so we 'll follow Brants et al.	a probability distribution	0.8135127463302989
Do such AAC devices see above?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.		0.812630027512836
Who is this algorithm called?	This algorithm is called Laplace smoothing.	Laplace smoothing	0.8109387839960716
'll we introduce both feedforward language models (Bengio et al?	We 'll introduce both feedforward language models (Bengio et al.		0.8040158797501209
What can these zeros-- things that do not ever occur in the training set but do occur in the test set-- use from Church and Gale?	To see this, we can use a clever idea from Church and Gale.	a clever idea	0.7934583849463015
Who use a particular test set so often that we implicitly tune to its characteristics?	Sometimes we use a particular test set so often that we implicitly tune to its characteristics.	we	0.7924764669287563
What do we choose?	We choose a random value between 0 and 1 and print the word whose interval includes this chosen value.	a random value between 0 and 1	0.7905415151797242
What do we add?	Instead of adding 1 to each count, we add a fractional count k (. 05?	a fractional count k (. 05	0.7900981299394148
'll we introduce more sophisticated language models like the RNN LMs of Chapter 9 in later chapters?	In later chapters we 'll introduce more sophisticated language models like the RNN LMs of Chapter 9.		0.7875767526605038
Did these arguments lead many linguists and computational linguists to ignore work in statistical modeling for decades?	These arguments led many linguists and computational linguists to ignore work in statistical modeling for decades.		0.7849970718350994
Is the word sequence the lower?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.		0.7845823117181261
Was each one incremented?	Since there are V words in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra V observations.		0.7812997785365621
Was i. e. P?	In MLE, the resulting parameter set maximizes the likelihood of the training set T given the model M (i. e. , P (T M) ).		0.7810373228550853
What can these zeros-- things that do not ever occur in the training set but do occur in the test set-- not divide by?	Thus if some words have zero probability, we cannot compute perplexity at all, since we cannot divide by 0!	by 0	0.7804084830937352
What do these zeros-- things that do not ever occur in the training set but do occur in the test set-- also need to adjust?	Since there are V words in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra V observations.	the denominator to take into account the extra V observations	0.7749875780206203
What do these zeros-- things that do not ever occur in the training set but do occur in the test set-- know?	Sometimes we have a language task in which this cannot happen because we know all the words that can occur.	all the words that can occur	0.7734253732828438
Who also want as much training data as possible?	We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible.	we	0.7696679118580658
Who should pause now, go to the web?	You should pause now, go to the web, and While this method of estimating probabilities directly from counts works fine in many cases, it turns out that even the web is not big enough to give us good estimates in most cases.	you	0.768121919941466
Who are ready to introduce cross-entropy?	Now we are ready to introduce cross-entropy.	we	0.7600461063391237
Did 3. 15 show?	3. 15 showed, perplexity is related inversely to the likelihood of the test sequence according to the model).		0.7599606219049606
What have a language task in which this cannot happen because we know all the words that can occur?	Sometimes we have a language task in which this cannot happen because we know all the words that can occur.	these zeros-- things that do not ever occur in the training set but do occur in the test set--	0.7591236051729195
Did 3. 11 set?	3. 11 Add an option to your program to compute the perplexity of a test set.		0.7585982699929184
What did they then check the counts of each of these bigrams in?	They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words.	in another 22 million words	0.7564262932605199
What should you pause now, go to?	You should pause now, go to the web, and While this method of estimating probabilities directly from counts works fine in many cases, it turns out that even the web is not big enough to give us good estimates in most cases.	to the web	0.7495806740447954
What do we also want as much training as possible?	We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible.	data	0.7483589930297045
What can these zeros-- things that do not ever occur in the training set but do occur in the test set-- use a clever idea from?	To see this, we can use a clever idea from Church and Gale.	from Church and Gale	0.7450530884497342
What do we have in which this cannot happen because we know all the words that can occur?	Sometimes we have a language task in which this cannot happen because we know all the words that can occur.	a language task	0.7413061038864377
Does the smaller the product become?	Since probabilities are (by definition) less than or equal to 1, the more probabilities we multiply together, the smaller the product becomes.		0.7356449516263932
Who will need an approximation to crossentropy?	We will need an approximation to crossentropy, relying on a (sufficiently long) sequence of fixed length.	we	0.7337694484445885
Does a DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allow us to use some m?	It A DVANCED: P ERPLEXITY ' S R ELATION TO E NTROPY allows us to use some m, which is a model of p (i. e. , an approximation to p).		0.7270175343441627
What is our evaluation metric based on?	Since our evaluation metric is based on test set probability, it's important not to let the test sentences into the training set.	on test set probability	0.72200651931055
Did a bigram that occurred 4 times in the first 22 million words occur 3. 23 times in the next 22 million words on average?	On average, a bigram that occurred 4 times in the first 22 million words occurred 3. 23 times in the next 22 million words.		0.7197566870036043
Who will we need an approximation to?	We will need an approximation to crossentropy, relying on a (sufficiently long) sequence of fixed length.	to crossentropy	0.7181663718407332
Does figure 3. 5 show the add-one smoothed counts for the Figure 3. 5 Add-one smoothed bigram counts for eight of the words corpus of 9332 sentences?	Figure 3. 5 shows the add-one smoothed counts for the Figure 3. 5 Add-one smoothed bigram counts for eight of the words (out of V = 1446) in the Berkeley Restaurant Project corpus of 9332 sentences.		0.7153033525979338
Did convert in the training set any word that is not in this set to the unknown word token <UNK> in a text normalization step?	Convert in the training set any word that is not in this set (any OOV word) to the unknown word token <UNK> in a text normalization step.		0.7059309531214626
Will we be computing the entropy of some sequence of words W = w0 w2, . . . for a grammar for example?	For a grammar, for example, we will be computing the entropy of some sequence of words W = w0, w1, w2, . . . , wn.		0.6920222113238395
What does one implication of this's simplest to visualize for?	It's simplest to visualize how this works for the unigram case.	how this works for the unigram case	0.682671235230297
Do we introduce the simplest model that assigns probabilities to sentences and sequences of words in this chapter?	In this chapter we introduce the simplest model that assigns probabilities to sentences and sequences of words, the n-gram.		0.6796036067544193
What do we show in?	But natural language is not stationary, since as we show in Chapter 12, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent.	in Chapter 12	0.6793440992869544
When do we randomly generate?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.	the sentence-final token </s>	0.6786847321123419
Do the probabilities of an n-gram model come from the corpus with many of the statistical models in such AAC devices's field?	As with many of the statistical models in our field, the probabilities of an n-gram model come from the corpus it is trained on, the training set or training corpus.		0.6723822650339641
Does Equation 3. 4 suggest that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities?	Equation 3. 4 suggests that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities.		0.6675755091461317
Was the average 3?	We saw above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code, so the average was 3.		0.6630541468863969
Are two probabilities one?	Give two probabilities, one using Fig.		0.6618998690080184
Who randomly generate the sentence-final token </s>?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.	we	0.6618039681220005
What do we just divide into 80 % training, 10 % development, and 10 % test in practice?	In practice, we often just divide our data into 80 % training, 10 % development, and 10 % test.	our data	0.6617153412249035
What will we need to crossentropy?	We will need an approximation to crossentropy, relying on a (sufficiently long) sequence of fixed length.	an approximation	0.660971045343006
Can these adjusted counts be computed by Eq?	These adjusted counts can be computed by Eq.		0.6576684964133594
Is the term ngram used to mean either the word sequence itself or the predictive model that assigns it a probability?	In a bit of terminological ambiguity, we usually drop the word ``model'', and thus the term ngram is used to mean either the word sequence itself or the predictive model that assigns it a probability.		0.6542294566835067
What 'll we have to give to the events we 've never seen?	To keep a language model from assigning zero probability to these unseen events, we 'll have to shave off a bit of probability mass from some more frequent events and give it to the events we 've never seen.	it	0.64838830501373
What is the entropy of the choice of horses then Until now we have been computing the entropy of a single variable?	The entropy of the choice of horses is then = log = 3 bits N- GRAM L ANGUAGE M ODELS Until now we have been computing the entropy of a single variable.	= log = 3 bits N- GRAM L ANGUAGE M ODELS	0.6476369769195065
What did we see it in?	We are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq.	in Eq	0.645849459897063
Am I Sam corpus Calculate the probability of the sentence i want chinese food?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.		0.6448025383777374
Will we formalize this intuition by introducing models that assign a probability to each possible next word in the following sections?	In the following sections we will formalize this intuition by introducing models that assign a probability to each possible next word.		0.64468229580805
Have we chosen the sample words to cohere with each other in fact?	In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse.		0.6402530982233592
Is the branching factor still 10?	We should expect the perplexity of this test set to be lower since most of the time the next number will be zero, which is very predictable, i. e. Thus, although the branching factor is still 10, the perplexity or weighted branching factor is smaller.		0.6359169028533882
What this is followed by the?	One way to estimate this probability is from relative frequency counts: take a very large corpus, count the number of times we see its water is so transparent that, and count the number of times this is followed by the.	one way to estimate this probability is from relative frequency counts: take a very large corpus, count the number of times we see its water is so transparent that, and count the number of times	0.6267429621697259
Does a Figure 3. 3 Eight sentences randomly generated from four n-grams computed from Shakespeare's works?	A Figure 3. 3 Eight sentences randomly generated from four n-grams computed from Shakespeare's works.		0.622689698921328
What would we have to get by?	We would have to get the count of its water is so transparent and divide by the sum of the counts of all possible five word sequences.	the count of its water is so transparent and divide by the sum of the counts of all possible five word sequences	0.6119014393525197
What would such AAC devices want to pick N- GRAM L ANGUAGE M ODELS the smallest test set at the minimum?	At the minimum, we would want to pick N- GRAM L ANGUAGE M ODELS the smallest test set that gives us enough statistical power to measure a statistically significant difference between two potential models.	that gives us enough statistical power to measure a statistically significant difference between two potential models	0.6108590870014698
Can we use a clever idea from Church and Gale?	To see this, we can use a clever idea from Church and Gale.		0.6097997523870102
If a higher-order n-gram has a zero count, do we simply backoff to a lower order n-gram?	If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed weight.		0.607226880516897
What would we want to pick N- GRAM L ANGUAGE M ODELS the smallest test set at the minimum?	At the minimum, we would want to pick N- GRAM L ANGUAGE M ODELS the smallest test set that gives us enough statistical power to measure a statistically significant difference between two potential models.	that gives us enough statistical power to measure a statistically significant difference between two potential models	0.6056693978016017
What do we present?	We present ways to modify the MLE estimates slightly to get better probability estimates in Section 3. 4.	ways to modify the MLE estimates slightly to get better probability estimates in Section 3. 4	0.6051992115269798
What can we compare the performance of two language models by seeing for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.	which gives the more accurate transcription	0.60361115631859
What is this ratio called?	This ratio is called a relative frequency.	a relative frequency	0.5954385451766255
What does it's simplest to visualize for?	It's simplest to visualize how this works for the unigram case.	how this works for the unigram case	0.5932111420726818
Was Mind 41?	Mind, 41,421423.		0.5890156506407884
What can we then measure the quality of an n-gram model by its performance on?	We can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus.	on some unseen data called the test set or test corpus	0.5797507751236735
Does Equation 3. 28 show the equation for interpolation with context-conditioned weights: P = 1 (wn1 How are these values set?	Equation 3. 28 shows the equation for interpolation with context-conditioned weights: P (wn wn2 wn1) = 1 (wn1 How are these values set?		0.5797000565142993
What do we print?	We choose a random value between 0 and 1 and print the word whose interval includes this chosen value.	the word whose interval includes this chosen value	0.5795187976129388
Do we recursively back off to the Katz probability for the shorter-history- gram?	Otherwise, we recursively back off to the Katz probability for the shorter-history- gram.		0.5765611861421136
What does these zeros-- things that do not ever occur in the training set but do occur in the test set--'s presence mean?	First, their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data.	we are underestimating the probability of all sorts of words that might occur	0.5755875923863356
What do we combine by adding them?	E VALUATING L ANGUAGE M ODELS Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them.	log probabilities	0.5722788066065625
Do such AAC devices introduce the simplest model that assigns probabilities to sentences and sequences of words in this chapter?	In this chapter we introduce the simplest model that assigns probabilities to sentences and sequences of words, the n-gram.		0.5690884857190879
Do we call the initial test set the development test set or, devset in such cases?	In such cases, we call the initial test set the development test set or, devset.		0.5688391719840422
What is this algorithm therefore called?	This algorithm is therefore called add-k smoothing.	add-k smoothing	0.5666616028489426
Do we need the end-symbol to make the bigram grammar a true probability distribution?	2 We need the end-symbol to make the bigram grammar a true probability distribution.		0.5620749145565922
Do such AAC devices multiply together?	Since probabilities are (by definition) less than or equal to 1, the more probabilities we multiply together, the smaller the product becomes.		0.559514407115083
What do these zeros-- things that do not ever occur in the training set but do occur in the test set-- hypothesize in?	We hypothesize that words that have appeared in more contexts in the past are more likely to appear in some new context as well.	that words that have appeared in more contexts in the past are more likely to appear in some new context as well	0.5589476693001583
Are both publicly available?	Both are publicly available.		0.5530821380928359
Is a held-out corpus an additional training corpus that we use to set hyperparameters like these values?	A held-out corpus is an additional training corpus that we use to set hyperparameters like these values, by choosing the values that maximize the likelihood of the held-out corpus.		0.5508534617882446
If our test sentence is part of the training corpus, who will mistakenly assign it an artificially high probability when it occurs in the test set?	If our test sentence is part of the training corpus, we will mistakenly assign it an artificially high probability when it occurs in the test set.	we	0.5484815707958479
Did by eq?	By Eq.		0.5474696822664402
Is the number of possible 4-grams V 4 = 7 1017?	There are V 2 = 844,000,000 possible bigrams alone, and the number of possible 4-grams is V 4 = 7 1017.		0.544854709108856
Would this model define an infinite set of probability distributions?	This model would define an infinite set of probability distributions, with one distribution per sentence length.		0.5402128781117388
What do we just divide our data into 80 % training, 10 % development, and 10 % test in?	In practice, we often just divide our data into 80 % training, 10 % development, and 10 % test.	in practice	0.5373727694082695
What do we see?	Now we see the following test set: 0 0 0 0 0 3 0 0 0 0.	the following test set: 0 0 0 0 0 3 0 0 0 0	0.5352743967050637
What do we just divide our data into in practice?	In practice, we often just divide our data into 80 % training, 10 % development, and 10 % test.	into 80 % training, 10 % development, and 10 % test	0.5339640235257392
Is the Kneser-Ney intuition to base our estimate of PCONTINUATION on the number of different contexts word w has appeared in?	The Kneser-Ney intuition is to base our estimate of PCONTINUATION on the number of different contexts word w has appeared in, that is, the number of bigram types it completes.		0.5334548821703886
Did we introduce perplexity in Section 3. 2. 1 as a way to evaluate n-gram models on a test set?	We introduced perplexity in Section 3. 2. 1 as a way to evaluate n-gram models on a test set.		0.5195145432298545
Do these solve a major problem with n-gram language models: the number of parameters increases exponentially as the n-gram order increases?	These solve a major problem with n-gram language models: the number of parameters increases exponentially as the n-gram order increases, and n-grams have no way to generalize from training to test set.		0.5194710045878912
Are probabilities essential in any task in which we have to identify words in noisy like speech recognition?	Probabilities are essential in any task in which we have to identify words in noisy, ambiguous input, like speech recognition.		0.5183965803129449
Are what kinds of linguistic phenomena captured in these bigram statistics?	What kinds of linguistic phenomena are captured in these bigram statistics?		0.5137720840469773
What will this sequence cross?	Since this sequence will cross many sentence boundaries, we need to include the begin- and end-sentence markers <s> and </s> in the probability computation.	many sentence boundaries	0.5125241670937379
What do we rely on if we have non-zero counts in Katz backoff?	In Katz backoff we rely on a discounted probability P if we 've seen this n-gram before (i. e. , if we have non-zero counts).	on a discounted probability P if we 've seen this n-gram before (i. e. ,	0.5118831139653053
Who cannot divide by 0?	Thus if some words have zero probability, we cannot compute perplexity at all, since we cannot divide by 0!	we	0.5115715377787686
What can we not divide by?	Thus if some words have zero probability, we cannot compute perplexity at all, since we cannot divide by 0!	by 0	0.5094316466729825
Can the test set only contain words from this lexicon in such a closed vocabulary system?	In such a closed vocabulary system the test set can only contain words from this lexicon, and there will be no unknown words.		0.507472677841176
Does the more information the n-gram give us about the word sequence the perplexity (since as Eq?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.		0.49321334932826133
Did they perform a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters?	They performed a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters.		0.4910412580323209
Does KenLM make it possible to build web-scale language models?	SRILM offers a wider range of options and types of discounting, while KenLM is optimized for speed and memory size, making it possible to build web-scale language models.		0.4822995121325322
Does it's simplest to visualize how this works for the unigram case?	It's simplest to visualize how this works for the unigram case.		0.47859813624417846
What does add-k smoothing require that these zeros-- things that do not ever occur in the training set but do occur in the test set-- have?	Add-k smoothing requires that we have a method for choosing k; this can be done, for example, by optimizing on a devset.	a method for choosing k	0.47535545194526097
Did they compute a bigram grammar from 22 million words of AP newswire?	They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words.		0.4748694434484393
Are some perfectly acceptable English word sequences bound to be missing from it?	But because any corpus is limited, some perfectly acceptable English word sequences are bound to be missing from it.		0.4732611425737261
Is output hand-corrected The longer the context on which we train the model?	Output is hand-corrected The longer the context on which we train the model, the more coherent the sentences.		0.4729188329656062
What does that seem rather!	That seems rather a lot to estimate!	a lot to estimate	0.46071715324172624
Are Markov models the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past?	Markov models are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past.		0.45982530365778107
Do we present ways to modify the MLE estimates slightly to get better probability estimates in Section 3. 4?	We present ways to modify the MLE estimates slightly to get better probability estimates in Section 3. 4.		0.4589920573983739
Are we finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq?	We are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq.		0.45854873423693476
Is it convenient to describe how a smoothing algorithm affects the numerator?	Instead of changing both the numerator and denominator, it is convenient to describe how a smoothing algorithm affects the numerator, by defining an adjusted count c.		0.45641983323965496
If we have no examples of a particular trigram wn2 wn1 wn, can we instead estimate its probability by using the bigram probability P?	If we are trying to compute P (wn wn2 wn1) but we have no examples of a particular trigram wn2 wn1 wn, we can instead estimate its probability by using the bigram probability P (wn wn1).		0.4563204848192757
Did we next choose a random bigram starting with w (again?	We next chose a random bigram starting with w (again, drawn according to its To give an intuition for the increasing power of higher-order n-grams, Fig.		0.4556347038510453
Do these solve a major problem with n-gram language models: n-grams have no way to generalize from training to test set?	These solve a major problem with n-gram language models: the number of parameters increases exponentially as the n-gram order increases, and n-grams have no way to generalize from training to test set.		0.4512274844053237
'll we follow Brants et al?	This algorithm does not produce a probability distribution, so we 'll follow Brants et al.		0.4469727147515421
What do we want?	We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible.	our test set to be as large as possible	0.4458345591171713
Do we rely on a discounted probability P if we 've seen this n-gram before (i. e. , if we have non-zero counts in Katz backoff?	In Katz backoff we rely on a discounted probability P if we 've seen this n-gram before (i. e. , if we have non-zero counts).		0.42734115098508063
Does add-k smoothing require that we have a method for choosing k?	Add-k smoothing requires that we have a method for choosing k; this can be done, for example, by optimizing on a devset.		0.4109561986467014
What do we hypothesize in?	We hypothesize that words that have appeared in more contexts in the past are more likely to appear in some new context as well.	that words that have appeared in more contexts in the past are more likely to appear in some new context as well	0.4062773337091572
What does these zeros-- things that do not ever occur in the training set but do occur in the test set--'s presence mean we are underestimating?	First, their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data.	the probability of all sorts of words that might occur	0.406141400935881
Was w drawn according to its To give an intuition for the increasing power of higher-order n-grams, Fig?	We next chose a random bigram starting with w (again, drawn according to its To give an intuition for the increasing power of higher-order n-grams, Fig.		0.40424389992294874
What do these zeros-- things that do not ever occur in the training set but do occur in the test set-- use if the evidence is sufficient?	In backoff, we use the trigram if the evidence is sufficient, otherwise we use the bigram, otherwise the unigram.	the trigram	0.3961112193263776
Is language creative?	This is because language is creative; new sentences are created all the time, and we will not always be able to count entire sentences.		0.3958932548508165
Can we compare the performance of two language models by running the speech recognizer twice for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.		0.3953773041466522
Do such AAC devices print the word whose interval includes this chosen value?	We choose a random value between 0 and 1 and print the word whose interval includes this chosen value.		0.3925880108917159
What is one implication of this?	One implication of this is that the probabilities often encode specific facts about a given training corpus.	that the probabilities often encode specific facts about a given training corpus	0.3819066324831233
Does 3. 12 Given a training set of 100 numbers consist of 91 zeros and 1 each of the other digits 1-9?	3. 12 Given a training set of 100 numbers consists of 91 zeros and 1 each of the other digits 1-9.		0.3813848312292445
Do 3. 3 shows random sentences generated from unigram, bigram, trigram, and 4-gram To him swallowed confess hear both?	3. 3 shows random sentences generated from unigram, bigram, trigram, and 4-gram To him swallowed confess hear both.		0.38059841353817037
Who does a careful investigation of the 4-gram sentences show like?	Indeed, a careful investigation of the 4-gram sentences shows that they look a little too much like Shakespeare.	that they look a little too much like Shakespeare	0.3784601292294134
What will this intuition mainly modify?	It will mainly modify the smaller counts, N- GRAM L ANGUAGE M ODELS for which we do not necessarily trust the estimate anyway, and Fig.	the smaller counts	0.37816340228965806
Does one implication of this's simplest to visualize how this works for the unigram case?	It's simplest to visualize how this works for the unigram case.		0.37725640719437425
Is one implication of this that the probabilities often encode specific facts about a given training corpus?	One implication of this is that the probabilities often encode specific facts about a given training corpus.		0.37494963324526953
What do we know?	Sometimes we have a language task in which this cannot happen because we know all the words that can occur.	all the words that can occur	0.3701966877453631
Is beta get ur IT placement wiv twitter Matching genres and dialects still not sufficient?	beta get ur IT placement wiv twitter Matching genres and dialects is still not sufficient.		0.3694404820277204
What did these zeros-- things that do not ever occur in the training set but do occur in the test set-- compute a bigram grammar from?	They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words.	from 22 million words of AP newswire	0.3669013843952038
Would it be nice to have a metric that can be used to quickly evaluate potential improvements in a language model?	Instead, it would be nice to have a metric that can be used to quickly evaluate potential improvements in a language model.		0.3619184872921668
Do we print the word whose interval includes this chosen value?	We choose a random value between 0 and 1 and print the word whose interval includes this chosen value.		0.35698115457357593
What will it mainly modify?	It will mainly modify the smaller counts, N- GRAM L ANGUAGE M ODELS for which we do not necessarily trust the estimate anyway, and Fig.	the smaller counts	0.35493892388041237
Do we need a test set for an intrinsic evaluation of a language model?	For an intrinsic evaluation of a language model we need a test set.		0.342342377160084
Do our statistical models only give an approximation to the correct distributions and entropies of natural language?	Thus, our statistical models only give an approximation to the correct distributions and entropies of natural language.		0.33793127658414024
Do such AAC devices call this situation training on the test set?	We call this situation training on the test set.		0.3316409333238499
Can we generalize the bigram to the trigram?	We can generalize the bigram (which looks one word into the past) to the trigram (which looks two words into the past) and thus to the n-gram (which looks n 1 words into the past).		0.3304165724235504
Can we compare the performance of two language models by seeing which gives the more accurate transcription for speech recognition?	Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.		0.3292344699964116
Who am I?	Now write out all the non-zero trigram probabilities for the I am Sam corpus Calculate the probability of the sentence i want chinese food.	Sam corpus Calculate the probability of the sentence i want chinese food	0.32815515329100453
Does kneser-Ney have its roots in a method called absolute discounting?	Kneser-Ney has its roots in a method called absolute discounting.		0.3271268339193618
What is it convenient to describe?	Instead of changing both the numerator and denominator, it is convenient to describe how a smoothing algorithm affects the numerator, by defining an adjusted count c.	how a smoothing algorithm affects the numerator	0.32688631639526977
Will this sequence cross many sentence boundaries?	Since this sequence will cross many sentence boundaries, we need to include the begin- and end-sentence markers <s> and </s> in the probability computation.		0.32061771449825116
What do we also need to adjust?	Since there are V words in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra V observations.	the denominator to take into account the extra V observations	0.31812574655769965
Do we also need to adjust the denominator to take into account the extra V observations?	Since there are V words in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra V observations.		0.3171692911716337
Does most of what we will use entropy for involve sequences?	But most of what we will use entropy for involves sequences.		0.30132877482217957
Is it generally represented in memory as a 64-bit hash number?	Rather than store each word as a string, it is generally represented in memory as a 64-bit hash number, with the words themselves stored on disk.		0.30071890142794766
Do we need a training corpus of questions?	To build a language model for a question-answering system, we need a training corpus of questions.		0.2974419629001228
Do we need a training corpus of legal documents?	To build a language model for translating legal documents, we need a training corpus of legal documents.		0.2963715914226741
What can these zeros-- things that do not ever occur in the training set but do occur in the test set-- see?	It is often convenient to reconstruct the count matrix so we can see how much a smoothing algorithm has changed the original counts.	how much a smoothing algorithm has changed the original counts	0.2900851431294913
Do such AAC devices leave this exact calculation as exercise 12?	We leave this exact calculation as exercise 12.		0.2763048898429292
Do 3. 26-- give us the highest probability of the held-out set?	3. 26-- give us the highest probability of the held-out set.		0.27476183892942574
Do we call this situation training on the test set?	We call this situation training on the test set.		0.2741288130661279
Is one step to be sure to use a training corpus that has a similar genre to whatever task we are trying to accomplish?	One step is to be sure to use a training corpus that has a similar genre to whatever task we are trying to accomplish.		0.2626804300082526
'll we have to shave off a bit of probability mass from some more frequent events?	To keep a language model from assigning zero probability to these unseen events, we 'll have to shave off a bit of probability mass from some more frequent events and give it to the events we 've never seen.		0.26213075936859953
Is the perplexity of this test set to be lower since most of the time the next number will be zero i. e. very predictable?	We should expect the perplexity of this test set to be lower since most of the time the next number will be zero, which is very predictable, i. e. Thus, although the branching factor is still 10, the perplexity or weighted branching factor is smaller.		0.2598516284713299
Will we need an approximation to crossentropy?	We will need an approximation to crossentropy, relying on a (sufficiently long) sequence of fixed length.		0.2591504063720138
Who will not always be able to count entire sentences?	This is because language is creative; new sentences are created all the time, and we will not always be able to count entire sentences.	we	0.25668521959229595
Do Chen see the following test set: 0 0 0 0 0 3 0 0 0 0?	Now we see the following test set: 0 0 0 0 0 3 0 0 0 0.		0.24974740471339008
Can we compute the entropy of some stochastic process by taking a very long sample of the output?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.		0.24965058288254394
Is the intuition of the Shannon-McMillan-Breiman theorem that each of these shorter sequences will reoccur in the longer sequence according to their probabilities?	The intuition of the Shannon-McMillan-Breiman theorem is that a long-enough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence according to their probabilities.		0.2488008987261714
Do we not know any way to compute the exact probability of a word given a long sequence of preceding words)?	We do not know any way to compute the exact probability of a word given a long sequence of preceding words, 1).		0.23732889732777052
What does their presence mean?	First, their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data.	we are underestimating the probability of all sorts of words that might occur	0.23402800338195084
Are these zeros a problem for two reasons?	These zeros-- things that do not ever occur in the training set but do occur in the test set-- are a problem for two reasons.		0.23352206528339803
Do we use the trigram if the evidence is sufficient?	In backoff, we use the trigram if the evidence is sufficient, otherwise we use the bigram, otherwise the unigram.		0.2329096255722174
Do we add a fractional count k (. 05?	Instead of adding 1 to each count, we add a fractional count k (. 05?		0.23243153791146565
Do we leave this exact calculation as exercise 12?	We leave this exact calculation as exercise 12.		0.23184992404334448
Do we choose a random value between 0 and 1?	We choose a random value between 0 and 1 and print the word whose interval includes this chosen value.		0.2279395418981709
What is trained on?	As with many of the statistical models in our field, the probabilities of an n-gram model come from the corpus it is trained on, the training set or training corpus.	it	0.22329251038636388
Is it the probability that makes it most likely that Chinese will occur 400 times in a million-word corpus?	But it is the probability that makes it most likely that Chinese will occur 400 times in a million-word corpus.		0.2190190609743814
Do we hypothesize that words that have appeared in more contexts in the past are more likely to appear in some new context as well?	We hypothesize that words that have appeared in more contexts in the past are more likely to appear in some new context as well.		0.2180657463012885
Will this intuition mainly modify the smaller counts?	It will mainly modify the smaller counts, N- GRAM L ANGUAGE M ODELS for which we do not necessarily trust the estimate anyway, and Fig.		0.21071287057607213
Is it equally important to get training data in the appropriate dialect?	It is equally important to get training data in the appropriate dialect, especially when processing social media posts or spoken transcripts.		0.21055824657983413
If we use log base 2, will the resulting value of entropy be measured in bits?	If we use log base 2, the resulting value of entropy will be measured in bits.		0.21048655718083076
Is one way to do this to have a variable that ranges over sequences of words?	One way to do this is to have a variable that ranges over sequences of words.		0.20508122438270338
Might we have a good estimate of its probability for any n-gram that occurred a sufficient number of times?	For any n-gram that occurred a sufficient number of times, we might have a good estimate of its probability.		0.20241569888419741
Is it commonly used as a quick check on an algorithm?	Nonetheless, because perplexity often correlates with such improvements, it is commonly used as a quick check on an algorithm.		0.20183017000183678
Can we compute the entropy of some stochastic process by computing its average log probability?	To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.		0.19880170817419662
What might we expect between?	Shakespeare and the Wall Street Journal are both English, so we might expect some overlap between our n-grams for the two genres.	some overlap between our n-grams for the two genres	0.1942848779973685
Is the cross-entropy useful when we do not know the actual probability distribution p that generated some data?	The cross-entropy is useful when we do not know the actual probability distribution p that generated some data.		0.19036603773994853
What is it often convenient to reconstruct the count matrix so we can see?	It is often convenient to reconstruct the count matrix so we can see how much a smoothing algorithm has changed the original counts.	how much a smoothing algorithm has changed the original counts	0.19017878759174422
Is this algorithm called Laplace smoothing?	This algorithm is called Laplace smoothing.		0.18781841732403448
Can we then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus?	We can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus.		0.18701428260369646
Do we continue generating words until we randomly generate the sentence-final token </s>?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.		0.1842950168637385
Did we see above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code?	We saw above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code, so the average was 3.		0.17715529416664477
Does this algorithm not produce a probability distribution?	This algorithm does not produce a probability distribution, so we 'll follow Brants et al.		0.1755425103570043
Do we see in Section 3. 7 that perplexity is also closely related to the informationtheoretic notion of entropy?	We see in Section 3. 7 that perplexity is also closely related to the informationtheoretic notion of entropy.		0.17421878812528657
Will the sorts of words that might occur hurt the performance of any application we want to run on this data?	First, their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data.		0.16101038880383856
Do we proceed to train the language model as before in either case?	In either case we then proceed to train the language model as before, treating <UNK> like a regular word.		0.15389051631286366
Does the table below show the perplexity of a 1. 5 million word WSJ test set according to each of these grammars?	The table below shows the perplexity of a 1. 5 million word WSJ test set according to each of these grammars.		0.14464240711132836
Did they show that caches and class-based models provided only minor additional improvement?	They showed the advantages of Modified Interpolated KneserNey, which has since become the standard baseline for language modeling, especially because they showed that caches and class-based models provided only minor additional improvement.		0.14089433970939602
Can we use two pseudo-words for the first trigram We always represent and compute language model probabilities in log format as log probabilities for example?	For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i. e. , P. We always represent and compute language model probabilities in log format as log probabilities.		0.13914002547416926
If we do not have counts to compute P can we look to the unigram P?	Similarly, if we do not have counts to compute P (wn wn1), we can look to the unigram P (wn).		0.13543462375094073
Who see above?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.	we	0.12065490180555338
If we spend the whole day betting and each horse is coded with 3 bits would we be sending 3 bits per race?	If we spend the whole day betting and each horse is coded with 3 bits, on average we would be sending 3 bits per race.		0.11943540444424272
Does (What happen to our P values if we do not increase the denominator? )	(What happens to our P values if we do not increase the denominator? )		0.11389262612671436
What do we get?	By using log probabilities instead of raw probabilities, we get numbers that are not as small.	numbers that are not as small	0.10321387988769404
What does 3. 8 suggest that in this discount is actually a good one for bigrams with counts 2 through 9?	3. 8 suggests that in practice this discount is actually a good one for bigrams with counts 2 through 9.	in practice	0.09807213353432842
Can we see how much a smoothing algorithm has changed the original counts?	It is often convenient to reconstruct the count matrix so we can see how much a smoothing algorithm has changed the original counts.		0.09394935145987704
Will it mainly modify the smaller counts?	It will mainly modify the smaller counts, N- GRAM L ANGUAGE M ODELS for which we do not necessarily trust the estimate anyway, and Fig.		0.0933149279961083
Does it's important not to let the test sentences into the training set?	Since our evaluation metric is based on test set probability, it's important not to let the test sentences into the training set.		0.08892411294459635
Do we continue choosing random numbers?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.		0.08553993096330048
Who simply have never seen before?	But what about words we simply have never seen before?	we	0.08341299926952273
Is our evaluation metric based on test set probability?	Since our evaluation metric is based on test set probability, it's important not to let the test sentences into the training set.		0.08149379176303828
Who said above?	As we said above, we cannot just estimate by counting the number of times every word occurs following every long string, because language is creative and any particular context might have never occurred before!	we	0.07137254640797086
If we are trying to compute P, can we instead estimate its probability by using the bigram probability P?	If we are trying to compute P (wn wn2 wn1) but we have no examples of a particular trigram wn2 wn1 wn, we can instead estimate its probability by using the bigram probability P (wn wn1).		0.06910669451316309
What 'll we have to give it to?	To keep a language model from assigning zero probability to these unseen events, we 'll have to shave off a bit of probability mass from some more frequent events and give it to the events we 've never seen.	to the events we 've never seen	0.04230441824254427
Do such AAC devices see the following test set: 0 0 0 0 0 3 0 0 0 0?	Now we see the following test set: 0 0 0 0 0 3 0 0 0 0.		0.01821167824503478
Do we show in Chapter 12?	But natural language is not stationary, since as we show in Chapter 12, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent.		0.015857262416590245
Is between 0 and 1 each word covering an interval proportional to its frequency?	Imagine all the words of the English language covering the probability space between 0 and 1, each word covering an interval proportional to its frequency.		0.005299171924785018
Have these zeros-- things that do not ever occur in the training set but do occur in the test set-- simply never seen before?	But what about words we simply have never seen before?		0.0016228698252334084
Does their presence mean we are underestimating the probability of all sorts of words that might occur?	First, their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data.		0.0013412540976611709
What do we see in?	We see in Section 3. 7 that perplexity is also closely related to the informationtheoretic notion of entropy.	in Section 3. 7 that perplexity is also closely related to the informationtheoretic notion of entropy	-0.0023183704548594974
Is this because language is creative?	This is because language is creative; new sentences are created all the time, and we will not always be able to count entire sentences.		-0.004250187753975121
Do we get numbers that are not as small?	By using log probabilities instead of raw probabilities, we get numbers that are not as small.		-0.010654098877626472
Do we not use raw probability as our metric for evaluating language models, but a variant called perplexity in practice?	In practice we do not use raw probability as our metric for evaluating language models, but a variant called perplexity.		-0.016597420506468064
Did they then check the counts of each of these bigrams in another 22 million words?	They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words.		-0.02303051458433525
Do we know all the words that can occur?	Sometimes we have a language task in which this cannot happen because we know all the words that can occur.		-0.02869039858503486
Do we usually drop the word ``model''?	In a bit of terminological ambiguity, we usually drop the word ``model'', and thus the term ngram is used to mean either the word sequence itself or the predictive model that assigns it a probability.		-0.029942897476137542
Who see the following test set: 0 0 0 0 0 3 0 0 0 0?	Now we see the following test set: 0 0 0 0 0 3 0 0 0 0.	we	-0.036586844344892344
Thus if some words have zero probability, can we not compute perplexity at all!	Thus if some words have zero probability, we cannot compute perplexity at all, since we cannot divide by 0!		-0.03884525569812203
Did we then compute the perplexity of each of these models on a test set of 1. 5 million words with Eq?	We then computed the perplexity of each of these models on a test set of 1. 5 million words with Eq.		-0.03943475655238471
Did such AAC devices then compute the perplexity of each of these models on a test set of 1. 5 million words with Eq?	We then computed the perplexity of each of these models on a test set of 1. 5 million words with Eq.		-0.04180609841252192
Do we also want as much training data as possible?	We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible.		-0.04462680588593648
Might we expect some overlap between our n-grams for the two genres?	Shakespeare and the Wall Street Journal are both English, so we might expect some overlap between our n-grams for the two genres.		-0.045494876847522514
Should you pause now, go to the web?	You should pause now, go to the web, and While this method of estimating probabilities directly from counts works fine in many cases, it turns out that even the web is not big enough to give us good estimates in most cases.		-0.04621291258182403
Do we randomly generate the sentence-final token </s>?	We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.		-0.05750086932299259
Will we not always be able to count entire sentences?	This is because language is creative; new sentences are created all the time, and we will not always be able to count entire sentences.		-0.05857910340269945
Does this method of estimating probabilities directly from counts work fine in many cases?	You should pause now, go to the web, and While this method of estimating probabilities directly from counts works fine in many cases, it turns out that even the web is not big enough to give us good estimates in most cases.		-0.060888686541411774
Are we ready to introduce cross-entropy?	Now we are ready to introduce cross-entropy.		-0.07025347374265123
Who multiply together?	Since probabilities are (by definition) less than or equal to 1, the more probabilities we multiply together, the smaller the product becomes.	we	-0.09127746338505671
Is this algorithm therefore called add-k smoothing?	This algorithm is therefore called add-k smoothing.		-0.09177257352150159
Would we have to get the count of its water is so transparent and divide by the sum of the counts of all possible five word sequences?	We would have to get the count of its water is so transparent and divide by the sum of the counts of all possible five word sequences.		-0.09323100600507939
Do we have a language task in which this cannot happen because we know all the words that can occur?	Sometimes we have a language task in which this cannot happen because we know all the words that can occur.		-0.10144760525993513
What can we see?	It is often convenient to reconstruct the count matrix so we can see how much a smoothing algorithm has changed the original counts.	how much a smoothing algorithm has changed the original counts	-0.12152847265767852
Did we see it in Eq?	We are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq.		-0.14123674122963426
Is this ratio called a relative frequency?	This ratio is called a relative frequency.		-0.1539380298825641
What is it?	But it is the probability that makes it most likely that Chinese will occur 400 times in a million-word corpus.	the probability that makes it most likely that Chinese will occur 400 times in a million-word corpus	-0.15525814411023908
'll we have to give it to the events we 've never seen?	To keep a language model from assigning zero probability to these unseen events, we 'll have to shave off a bit of probability mass from some more frequent events and give it to the events we 've never seen.		-0.1677924746300341
Do we want our test set to be as large as possible?	We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible.		-0.17503425854384558
Do the probabilities of an n-gram model come from the corpus with many of the statistical models in our field?	As with many of the statistical models in our field, the probabilities of an n-gram model come from the corpus it is trained on, the training set or training corpus.		-0.17624270679657084
Does it turn out that even the web is not big enough to give us good estimates in most cases?	You should pause now, go to the web, and While this method of estimating probabilities directly from counts works fine in many cases, it turns out that even the web is not big enough to give us good estimates in most cases.		-0.18139025664826613
Do we combine log probabilities by adding them?	E VALUATING L ANGUAGE M ODELS Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them.		-0.19284708385206217
Does that seem rather a lot to estimate!	That seems rather a lot to estimate!		-0.19423823107150118
Do we use a particular test set so often that we implicitly tune to its characteristics?	Sometimes we use a particular test set so often that we implicitly tune to its characteristics.		-0.20226778007414992
Do we just divide our data into 80 % training, 10 % development, and 10 % test in practice?	In practice, we often just divide our data into 80 % training, 10 % development, and 10 % test.		-0.2269336760918501
Can this be done?	Add-k smoothing requires that we have a method for choosing k; this can be done, for example, by optimizing on a devset.		-0.2298123718147651
Is it often convenient to reconstruct the count matrix so we can see how much a smoothing algorithm has changed the original counts?	It is often convenient to reconstruct the count matrix so we can see how much a smoothing algorithm has changed the original counts.		-0.24071805468929153
Can these zeros-- things that do not ever occur in the training set but do occur in the test set-- not divide by 0?	Thus if some words have zero probability, we cannot compute perplexity at all, since we cannot divide by 0!		-0.2463089838412329
Is this modification called smoothing or discounting?	This modification is called smoothing or discounting.		-0.2647008006672631
Did we say above?	As we said above, we cannot just estimate by counting the number of times every word occurs following every long string, because language is creative and any particular context might have never occurred before!		-0.2661096537124761
Can we not divide by 0?	Thus if some words have zero probability, we cannot compute perplexity at all, since we cannot divide by 0!		-0.27821793059332056
What does it turn out?	You should pause now, go to the web, and While this method of estimating probabilities directly from counts works fine in many cases, it turns out that even the web is not big enough to give us good estimates in most cases.	that even the web is not big enough to give us good estimates in most cases	-0.29153164682430754
Do we see above?	As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq.		-0.3205348867944109
Is it trained on?	As with many of the statistical models in our field, the probabilities of an n-gram model come from the corpus it is trained on, the training set or training corpus.		-0.33529987662740734
Do these zeros-- things that do not ever occur in the training set but do occur in the test set-- add a fractional count k (. 05?	Instead of adding 1 to each count, we add a fractional count k (. 05?		-0.34519614657066167
Does this method of estimating probabilities directly from counts turn out that even the web is not big enough to give such AAC devices good estimates in most cases?	You should pause now, go to the web, and While this method of estimating probabilities directly from counts works fine in many cases, it turns out that even the web is not big enough to give us good estimates in most cases.		-0.3528822068233122
If our test sentence is part of the training corpus, will we mistakenly assign it an artificially high probability when it occurs in the test set?	If our test sentence is part of the training corpus, we will mistakenly assign it an artificially high probability when it occurs in the test set.		-0.4209697483565009
Have we simply never seen before?	But what about words we simply have never seen before?		-0.5057138743506355
Do we multiply together?	Since probabilities are (by definition) less than or equal to 1, the more probabilities we multiply together, the smaller the product becomes.		-0.5332549303739957
Does 3. 8 suggest that in practice this discount is actually a good one for bigrams with counts 2 through 9?	3. 8 suggests that in practice this discount is actually a good one for bigrams with counts 2 through 9.		-0.5878066675855063
Is that?	That is, we 'll have many cases of putative ``zero probability n-grams'' that should really have some non-zero probability.		-0.617362317392671
Do these zeros-- things that do not ever occur in the training set but do occur in the test set-- know all the words that can occur?	Sometimes we have a language task in which this cannot happen because we know all the words that can occur.		-0.7449615374164802
Do we see the following test set: 0 0 0 0 0 3 0 0 0 0?	Now we see the following test set: 0 0 0 0 0 3 0 0 0 0.		-0.8658466153858411
What do we use?	Sometimes we use a particular test set so often that we implicitly tune to its characteristics.	a particular test set so often that we implicitly tune to its characteristics	-1.227146646125949
